[
  {
    "objectID": "files/Lecture_6_class/index.html#learning-outcomes",
    "href": "files/Lecture_6_class/index.html#learning-outcomes",
    "title": "Sampling Distributions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nCovariance Properties\nSampling Distributions\nCentral Limit Theorem"
  },
  {
    "objectID": "files/Lecture_6_class/index.html#covariance-1",
    "href": "files/Lecture_6_class/index.html#covariance-1",
    "title": "Sampling Distributions",
    "section": "Covariance",
    "text": "Covariance"
  },
  {
    "objectID": "files/Lecture_6_class/index.html#observing-data-1",
    "href": "files/Lecture_6_class/index.html#observing-data-1",
    "title": "Sampling Distributions",
    "section": "Observing Data",
    "text": "Observing Data"
  },
  {
    "objectID": "files/Lecture_6_class/index.html#descriptive-statistics-1",
    "href": "files/Lecture_6_class/index.html#descriptive-statistics-1",
    "title": "Sampling Distributions",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics"
  },
  {
    "objectID": "files/Lecture_6_class/index.html#sample-mean",
    "href": "files/Lecture_6_class/index.html#sample-mean",
    "title": "Sampling Distributions",
    "section": "Sample Mean",
    "text": "Sample Mean"
  },
  {
    "objectID": "files/Lecture_6_class/index.html#sample-variance",
    "href": "files/Lecture_6_class/index.html#sample-variance",
    "title": "Sampling Distributions",
    "section": "Sample Variance",
    "text": "Sample Variance"
  },
  {
    "objectID": "files/Lecture_6_class/index.html#expected-value",
    "href": "files/Lecture_6_class/index.html#expected-value",
    "title": "Sampling Distributions",
    "section": "Expected Value",
    "text": "Expected Value"
  },
  {
    "objectID": "files/Lecture_6_class/index.html#variance",
    "href": "files/Lecture_6_class/index.html#variance",
    "title": "Sampling Distributions",
    "section": "Variance",
    "text": "Variance"
  },
  {
    "objectID": "files/Lecture_6_class/index.html#sampling-mean",
    "href": "files/Lecture_6_class/index.html#sampling-mean",
    "title": "Sampling Distributions",
    "section": "Sampling Mean",
    "text": "Sampling Mean"
  },
  {
    "objectID": "files/Lecture_6_class/index.html#sampling-mean-1",
    "href": "files/Lecture_6_class/index.html#sampling-mean-1",
    "title": "Sampling Distributions",
    "section": "Sampling Mean",
    "text": "Sampling Mean"
  },
  {
    "objectID": "files/Lecture_6_class/index.html#central-limit-theorem-1",
    "href": "files/Lecture_6_class/index.html#central-limit-theorem-1",
    "title": "Sampling Distributions",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nLet \\(X_1, X_2, \\ldots, X_n\\) be identical and independent distributed random variables with \\(E(X_i)=\\mu\\) and \\(Var(X_i) = \\sigma²\\). We define \\[\nY_n = \\sqrt n \\left(\\frac{\\bar X-\\mu}{\\sigma}\\right) \\mathrm{ where }\\ \\bar X = \\frac{1}{n}\\sum^n_{i=1}X_i.\n\\]\nThen, the distribution of the function \\(Y_n\\) converges to a standard normal distribution function as \\(n\\rightarrow \\infty\\)."
  },
  {
    "objectID": "files/Lecture_11_class/index.html#learning-outcomes",
    "href": "files/Lecture_11_class/index.html#learning-outcomes",
    "title": "Goodness of Estimators",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nConsistency\nSufficiency\nInformation\nEfficiency"
  },
  {
    "objectID": "files/Lecture_11_class/index.html#consistency-1",
    "href": "files/Lecture_11_class/index.html#consistency-1",
    "title": "Goodness of Estimators",
    "section": "Consistency",
    "text": "Consistency\nAn estimator is considered a consistent estimator of \\(\\theta\\) if the estimator, on average, converges to \\(\\theta\\) as \\(n\\rightarrow\\infty\\)."
  },
  {
    "objectID": "files/Lecture_11_class/index.html#consistency-2",
    "href": "files/Lecture_11_class/index.html#consistency-2",
    "title": "Goodness of Estimators",
    "section": "Consistency",
    "text": "Consistency\nLet \\(X_1,\\ldots,X_n\\) be a random sample from a distribution with parameter \\(\\theta\\). The estimator \\(\\hat \\theta\\) is a consistent estimator of the \\(\\theta\\) if\n\n\\(E\\{(\\hat\\theta-\\theta)^2\\}\\rightarrow0\\) as \\(n\\rightarrow \\infty\\)\n\\(P(|\\hat\\theta-\\theta|\\ge \\epsilon)\\rightarrow0\\) as \\(n\\rightarrow \\infty\\) for every \\(\\epsilon&gt;0\\)"
  },
  {
    "objectID": "files/Lecture_11_class/index.html#sufficiency-1",
    "href": "files/Lecture_11_class/index.html#sufficiency-1",
    "title": "Goodness of Estimators",
    "section": "Sufficiency",
    "text": "Sufficiency\nSufficiency evaluates whether a statistic (or estimator) contains enough information of a parameter \\(\\theta\\). In essence a statistic is considered sufficient to infer \\(\\theta\\) if it provides enough information about \\(\\theta\\)."
  },
  {
    "objectID": "files/Lecture_11_class/index.html#sufficiency-2",
    "href": "files/Lecture_11_class/index.html#sufficiency-2",
    "title": "Goodness of Estimators",
    "section": "Sufficiency",
    "text": "Sufficiency\nLet \\(X_1,\\ldots,X_n\\) be a random sample from a distribution with parameter \\(\\theta\\). A statistic \\(T=t(X_1,\\ldots,X_n)\\) is said to be sufficient for making inferences of a parameter \\(\\theta\\) if condition joint distribution of \\(X_1,\\ldots,X_n\\) given \\(T=t\\) does not depend on \\(\\theta\\)."
  },
  {
    "objectID": "files/Lecture_11_class/index.html#example",
    "href": "files/Lecture_11_class/index.html#example",
    "title": "Goodness of Estimators",
    "section": "Example",
    "text": "Example\nLet \\(X_1,\\ldots, X_n\\overset{iid}{\\sim}Bernoulli(p)\\) and \\(Y_n=\\sum^n_{i=1}X_i\\). Show that \\(Y_n\\) is a sufficient statistic for \\(p\\)."
  },
  {
    "objectID": "files/Lecture_11_class/index.html#information-1",
    "href": "files/Lecture_11_class/index.html#information-1",
    "title": "Goodness of Estimators",
    "section": "Information",
    "text": "Information\nIn Statistics, information is thought of as how much does the data tell you about a parameter \\(\\theta\\). In general, the more data is provided, the more information is provided to estimate \\(\\theta\\)."
  },
  {
    "objectID": "files/Lecture_11_class/index.html#information-2",
    "href": "files/Lecture_11_class/index.html#information-2",
    "title": "Goodness of Estimators",
    "section": "Information",
    "text": "Information\nInformation can be quantified using Fisher’s Information \\(I(\\theta)\\). For a single observation, Fisher’s Information is defined as\n\\[\nI(\\theta)=E\\left[-\\frac{\\partial^2}{\\partial\\theta^2}\\log\\{f(X;\\theta)\\}\\right],\n\\]\nwhere \\(f(X;\\theta)\\) is either the PMF or PDF of the random variable \\(X\\)."
  },
  {
    "objectID": "files/Lecture_11_class/index.html#information-3",
    "href": "files/Lecture_11_class/index.html#information-3",
    "title": "Goodness of Estimators",
    "section": "Information",
    "text": "Information\nFurthermore, \\(I(\\theta)\\) can be defined as\n\\[\nI(\\theta)=Var\\left\\{\\frac{\\partial}{\\partial\\theta}\\log f(X;\\theta)\\right\\}.\n\\]"
  },
  {
    "objectID": "files/Lecture_11_class/index.html#proof",
    "href": "files/Lecture_11_class/index.html#proof",
    "title": "Goodness of Estimators",
    "section": "Proof",
    "text": "Proof\nShow the following property:\n\\[\nE\\left[-\\frac{\\partial^2}{\\partial\\theta^2}\\log\\{f(X;\\theta)\\}\\right] = Var\\left\\{\\frac{\\partial}{\\partial\\theta}\\log f(X;\\theta)\\right\\}\n\\]"
  },
  {
    "objectID": "files/Lecture_11_class/index.html#efficiency-1",
    "href": "files/Lecture_11_class/index.html#efficiency-1",
    "title": "Goodness of Estimators",
    "section": "Efficiency",
    "text": "Efficiency\nEfficiency of an estimator \\(T\\) is the ratio of variation compared to the lowest possible variance."
  },
  {
    "objectID": "files/Lecture_11_class/index.html#efficiency-2",
    "href": "files/Lecture_11_class/index.html#efficiency-2",
    "title": "Goodness of Estimators",
    "section": "Efficiency",
    "text": "Efficiency\nThe efficiency of an estimator \\(T\\), where \\(T\\) is an unbiased estimator of \\(\\theta\\), is defined as\n\\[\nefficiency\\ of\\ T = \\frac{1}{Var(T)nI(\\theta)}\n\\]"
  },
  {
    "objectID": "files/Lecture_11_class/index.html#example-1",
    "href": "files/Lecture_11_class/index.html#example-1",
    "title": "Goodness of Estimators",
    "section": "Example",
    "text": "Example\nLet \\(X_1,\\ldots, X_n\\overset{iid}{\\sim}Unif(0,\\theta)\\) and \\(\\hat\\theta=2\\bar X\\). Find the efficiency of \\(\\hat \\theta\\)."
  },
  {
    "objectID": "files/Lecture_9_class/index.html#learning-outcomes",
    "href": "files/Lecture_9_class/index.html#learning-outcomes",
    "title": "Maximum Likelihood Estimator",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nLikelihood Function\nMaximum Likelihood Estimator\nLog-Likelihood Function"
  },
  {
    "objectID": "files/Lecture_9_class/index.html#estimators",
    "href": "files/Lecture_9_class/index.html#estimators",
    "title": "Maximum Likelihood Estimator",
    "section": "Estimators",
    "text": "Estimators\nAn estimator is an operation computing the value of an estimate, that targets the parameter, using measurements from a sample."
  },
  {
    "objectID": "files/Lecture_9_class/index.html#data",
    "href": "files/Lecture_9_class/index.html#data",
    "title": "Maximum Likelihood Estimator",
    "section": "Data",
    "text": "Data\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}F(\\boldsymbol \\theta)\\) where \\(F(\\cdot)\\) is a known distribution function and \\(\\boldsymbol\\theta\\) is a vector of parameters. Let \\(\\boldsymbol X = (X_1,\\ldots, X_n)^\\mathrm{T}\\), be the sample collected."
  },
  {
    "objectID": "files/Lecture_9_class/index.html#likelihood-function-1",
    "href": "files/Lecture_9_class/index.html#likelihood-function-1",
    "title": "Maximum Likelihood Estimator",
    "section": "Likelihood Function",
    "text": "Likelihood Function\nUsing the joint pdf or pmf of the sample \\(\\boldsymbol X\\), the likelihood function is a function of \\(\\boldsymbol \\theta\\), given the observed data \\(\\boldsymbol X =\\boldsymbol x\\), defined as\n\\[\nL(\\boldsymbol \\theta|\\boldsymbol x)=f(\\boldsymbol x|\\boldsymbol \\theta)\n\\]\nIf the data is iid, then\n\\[\nf(\\boldsymbol x|\\boldsymbol \\theta) = \\prod^n_{i=1}f(x_i|\\boldsymbol\\theta)\n\\]"
  },
  {
    "objectID": "files/Lecture_9_class/index.html#log-likelihood-function-1",
    "href": "files/Lecture_9_class/index.html#log-likelihood-function-1",
    "title": "Maximum Likelihood Estimator",
    "section": "Log-Likelihood Function",
    "text": "Log-Likelihood Function\nIf \\(\\ln\\{L(\\boldsymbol \\theta)\\}\\) is monotone of \\(\\boldsymbol \\theta\\), then maximizing \\(\\ln\\{L(\\boldsymbol \\theta)\\}\\) will yield the maximum likelihood estimators."
  },
  {
    "objectID": "files/Lecture_9_class/index.html#bernoulli-distribution",
    "href": "files/Lecture_9_class/index.html#bernoulli-distribution",
    "title": "Maximum Likelihood Estimator",
    "section": "Bernoulli Distribution",
    "text": "Bernoulli Distribution\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}\\mathrm{Bernoulli}(p)\\), show that the MLE of \\(p\\) is \\(\\bar x\\)."
  },
  {
    "objectID": "files/Lecture_9_class/index.html#normal-distribution",
    "href": "files/Lecture_9_class/index.html#normal-distribution",
    "title": "Maximum Likelihood Estimator",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\). Show that the MLE’s of \\(\\mu\\) and \\(\\sigma^2\\) are \\(\\bar x\\) and \\(\\frac{n-1}{n}s^2\\), respectively."
  },
  {
    "objectID": "files/Lecture_8_class/index.html#learning-outcomes",
    "href": "files/Lecture_8_class/index.html#learning-outcomes",
    "title": "Estimators",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nEstimators\nUnbiased Estimators\nBias\nMean Square Error"
  },
  {
    "objectID": "files/Lecture_8_class/index.html#estimators",
    "href": "files/Lecture_8_class/index.html#estimators",
    "title": "Estimators",
    "section": "Estimators",
    "text": "Estimators\nAn estimator is an operation computing the value of an estimate, that targets the parameter, using measurements from a sample."
  },
  {
    "objectID": "files/Lecture_8_class/index.html#unbiased-estimator",
    "href": "files/Lecture_8_class/index.html#unbiased-estimator",
    "title": "Estimators",
    "section": "Unbiased Estimator",
    "text": "Unbiased Estimator\nAn unbiased estimator \\(\\hat\\theta\\) is an estimator that satisfies the following condition:\n\\[\nE(\\hat\\theta) = \\theta\n\\]"
  },
  {
    "objectID": "files/Lecture_8_class/index.html#bias",
    "href": "files/Lecture_8_class/index.html#bias",
    "title": "Estimators",
    "section": "Bias",
    "text": "Bias\nThe bias of an estimator is defined as\n\\[\nB(\\hat\\theta) = E(\\hat\\theta)-\\theta\n\\]"
  },
  {
    "objectID": "files/Lecture_8_class/index.html#mean-square-error",
    "href": "files/Lecture_8_class/index.html#mean-square-error",
    "title": "Estimators",
    "section": "Mean Square Error",
    "text": "Mean Square Error\nThe mean square of an estimator is \\(\\hat\\theta\\) is given as\n\\[\n\\begin{eqnarray}\nMSE(\\hat\\theta) & = & E\\{(\\hat\\theta-\\theta)^2\\} \\\\\n& = & Var(\\hat\\theta) + B(\\hat\\theta)^2\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "files/Lecture_8_class/index.html#is-bar-x-an-unbiased-estimator-of-mu",
    "href": "files/Lecture_8_class/index.html#is-bar-x-an-unbiased-estimator-of-mu",
    "title": "Estimators",
    "section": "Is \\(\\bar X\\) an unbiased estimator of \\(\\mu\\)?",
    "text": "Is \\(\\bar X\\) an unbiased estimator of \\(\\mu\\)?\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\), find the bias of \\(\\bar X\\)."
  },
  {
    "objectID": "files/Lecture_8_class/index.html#why-is-s²-divided-by-n-1-instead-of-n",
    "href": "files/Lecture_8_class/index.html#why-is-s²-divided-by-n-1-instead-of-n",
    "title": "Estimators",
    "section": "Why is \\(S²\\) divided by \\(n-1\\) instead of \\(n\\)?",
    "text": "Why is \\(S²\\) divided by \\(n-1\\) instead of \\(n\\)?\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\), find the bias of \\(S²\\)."
  },
  {
    "objectID": "files/Lecture_8_class/index.html#problem",
    "href": "files/Lecture_8_class/index.html#problem",
    "title": "Estimators",
    "section": "Problem",
    "text": "Problem\nLet \\(X_1,X_2,X_3\\) follow and exponential distribution with mean and variance \\(\\lambda\\) and \\(\\lambda²\\), respectively. Using the following estimators:\n\n\\(\\hat\\theta_1 = X_1\\)\n\\(\\hat\\theta_2 = \\frac{X_1+X_2}{2}\\)\n\\(\\hat\\theta_3 = \\frac{X_1+2X_2}{3}\\)\n\\(\\hat\\theta_4 = \\frac{X_1+X_2+X_3}{3}\\)\n\nIdentify which estimator\n\nIs unbiased?\nHas the lowest variance?"
  },
  {
    "objectID": "files/Lecture_5_class/index.html#learning-outcomes",
    "href": "files/Lecture_5_class/index.html#learning-outcomes",
    "title": "Functions of Random Variables",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nFunctions of Random Variables\nFinding PDFs using the distribution function\nFinding the PDF of a function of random variables\nUsing Moment Generating Functions"
  },
  {
    "objectID": "files/Lecture_5_class/index.html#function-of-random-variables-1",
    "href": "files/Lecture_5_class/index.html#function-of-random-variables-1",
    "title": "Functions of Random Variables",
    "section": "Function of Random Variables",
    "text": "Function of Random Variables"
  },
  {
    "objectID": "files/Lecture_5_class/index.html#using-the-distribution-function",
    "href": "files/Lecture_5_class/index.html#using-the-distribution-function",
    "title": "Functions of Random Variables",
    "section": "Using the Distribution Function",
    "text": "Using the Distribution Function\nLet there be a random variable \\(X\\) with a known distribution function \\(F_X(x)\\), the density function for the random variable \\(Y=g(X)\\) can be found with the following steps\n\n\n\nFind the region of \\(Y\\) in the space of \\(X\\), find \\(g^{-1}(y)\\)\nFind the region of \\(Y\\le y\\)\nFind \\(F_Y(y)=P(Y\\le y)\\) using the probability density function of \\(X\\) over region \\(Y\\le y\\)\nFind \\(f_Y(y)\\) by differentiating \\(F_Y(y)\\)"
  },
  {
    "objectID": "files/Lecture_5_class/index.html#example-1",
    "href": "files/Lecture_5_class/index.html#example-1",
    "title": "Functions of Random Variables",
    "section": "Example 1",
    "text": "Example 1\nLet \\(X\\) have the following probability density function:\n\\[\nf_X(x)=\\left\\{\\begin{array}{cc}\n2x & 0\\le x \\le 1 \\\\\n0 & \\mathrm{otherwise}\n\\end{array}\n\\right.\n\\]\nFind the probability density function of \\(Y=3X-1\\)?"
  },
  {
    "objectID": "files/Lecture_5_class/index.html#using-the-pdf",
    "href": "files/Lecture_5_class/index.html#using-the-pdf",
    "title": "Functions of Random Variables",
    "section": "Using the PDF",
    "text": "Using the PDF\nLet there be a random variable \\(X\\) with a known distribution function \\(F_X(x)\\), if the random variable \\(Y=g(X)\\) is either increasing or decreasing, than the probability density function can be found as\n\\[\nf_Y(y) = f_X\\{g^{-1}(y)\\}\\left|\\frac{dg^{-1}(y)}{dy}\\right|\n\\]"
  },
  {
    "objectID": "files/Lecture_5_class/index.html#example-2",
    "href": "files/Lecture_5_class/index.html#example-2",
    "title": "Functions of Random Variables",
    "section": "Example 2",
    "text": "Example 2\nLet \\(X\\) have the following probability density function:\n\\[\nf_X(x)=\\left\\{\\begin{array}{cc}\n\\frac{3}{2}x^2 + x & 0\\le y \\le 1 \\\\\n0 & \\mathrm{otherwise}\n\\end{array}\n\\right.\n\\]\nFind the probability density function of \\(Y=5-(X/2)\\)?"
  },
  {
    "objectID": "files/Lecture_5_class/index.html#using-the-mgf",
    "href": "files/Lecture_5_class/index.html#using-the-mgf",
    "title": "Functions of Random Variables",
    "section": "Using the MGF",
    "text": "Using the MGF\nUsing the uniqueness property of Moment Generating Functions, for a random variable \\(X\\) with a known distribution function \\(F_X(x)\\) and random variable \\(Y=g(X)\\), the distribution of \\(Y\\) can be found by:\n\n\nFind the moment generating function of \\(Y\\), \\(M_Y(t)\\).\nCompare \\(M_Y(t)\\), with known moment generating functions. If \\(M_Y(t)=M_V(t)\\), for all values \\(t\\), them \\(Y\\) and \\(V\\) have identical distributions."
  },
  {
    "objectID": "files/Lecture_5_class/index.html#example-3",
    "href": "files/Lecture_5_class/index.html#example-3",
    "title": "Functions of Random Variables",
    "section": "Example 3",
    "text": "Example 3\nLet \\(X\\) follow a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Find the distribution of \\(Z=\\frac{X-\\mu}{\\sigma}\\)."
  },
  {
    "objectID": "files/Lecture_5_class/index.html#example-4",
    "href": "files/Lecture_5_class/index.html#example-4",
    "title": "Functions of Random Variables",
    "section": "Example 4",
    "text": "Example 4\nLet \\(Z\\) follow a standard normal distribution with mean \\(0\\) and variance \\(1\\). Find the distribution of \\(Y=Z^2\\)"
  },
  {
    "objectID": "files/Lecture_3_class/index.html#learning-outcomes",
    "href": "files/Lecture_3_class/index.html#learning-outcomes",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nReview Random Variables\nReview Discrete Random Variables\nReview Continuous Random Variables"
  },
  {
    "objectID": "files/Lecture_3_class/index.html#random-variables",
    "href": "files/Lecture_3_class/index.html#random-variables",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Random Variables",
    "text": "Random Variables\nA random variable is function that maps the sample space to real value."
  },
  {
    "objectID": "files/Lecture_3_class/index.html#discrete-random-variables-1",
    "href": "files/Lecture_3_class/index.html#discrete-random-variables-1",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\nA random variable is considered to be discrete if it can only map to a finite or countably infinite number of distinct values."
  },
  {
    "objectID": "files/Lecture_3_class/index.html#pmf",
    "href": "files/Lecture_3_class/index.html#pmf",
    "title": "Review Random Variables and Distribution Functions",
    "section": "PMF",
    "text": "PMF\nThe probability mass function of discrete variable can be represented by a formula, table, or a graph. The Probability of a random variable Y can be expressed as \\(P(Y=y)\\) for all values of \\(y\\)."
  },
  {
    "objectID": "files/Lecture_3_class/index.html#cdf",
    "href": "files/Lecture_3_class/index.html#cdf",
    "title": "Review Random Variables and Distribution Functions",
    "section": "CDF",
    "text": "CDF\nThe cumulative distribution function provides the \\(P(Y\\leq y)\\) for a random variable \\(Y\\)."
  },
  {
    "objectID": "files/Lecture_3_class/index.html#expected-value",
    "href": "files/Lecture_3_class/index.html#expected-value",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Expected Value",
    "text": "Expected Value\nThe expected value is the value we expect when we randomly sample from population that follows a specific distribution. The expected value of Y is\n\\[\nE(Y)=\\sum_y yP(y)\n\\]"
  },
  {
    "objectID": "files/Lecture_3_class/index.html#variance",
    "href": "files/Lecture_3_class/index.html#variance",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Variance",
    "text": "Variance\nThe variance is the expected squared difference between the random variable and expected value.\n\\[\nVar(Y)=\\sum_y\\{y-E(Y)\\}^2P(y)\n\\]\n\\[\nVar(Y) = E(X^2) - E(X)^2\n\\]"
  },
  {
    "objectID": "files/Lecture_3_class/index.html#known-distributions",
    "href": "files/Lecture_3_class/index.html#known-distributions",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Known Distributions",
    "text": "Known Distributions\n\n\n\n\n\n\n\n\nDistribution\nParameter(s)\nPMF \\(P(Y=y)\\)\n\n\n\n\nBernoulli\n\\(p\\)\n\\(p\\)\n\n\nBinomial\n\\(n\\) and \\(p\\)\n\\((^n_y)p^y(1-p)^{n-p}\\)\n\n\nGeometric\n\\(p\\)\n\\((1-p)^{y-1}p\\)\n\n\nNegative Binomial\n\\(r\\) and \\(p\\)\n\\((^{y-1}_{r-1})p^{r-1}(1-p)^{y-r}\\)\n\n\nHypergeometric\n\\(N\\), \\(n\\), and \\(r\\)\n\\(\\frac{(^r_y)(^{N-r}_{n-y})}{(^N_n)}\\)\n\n\nPoisson\n\\(\\lambda\\)\n\\(\\frac{\\lambda^y}{y!} e^{-\\lambda}\\)"
  },
  {
    "objectID": "files/Lecture_3_class/index.html#binomial-distribution-1",
    "href": "files/Lecture_3_class/index.html#binomial-distribution-1",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\nAn experiment is said to follow a binomial distribution if\n\nFixed \\(n\\)\nEach trial has 2 outcomes\nThe probability of success is a constant \\(p\\)\nThe trials are independent of each\n\n\n\\(P(X=x)=(^n_x)p^x(1-p)^{n-x}\\)"
  },
  {
    "objectID": "files/Lecture_3_class/index.html#expected-value-of-a-binomial-distribution",
    "href": "files/Lecture_3_class/index.html#expected-value-of-a-binomial-distribution",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Expected Value of a Binomial Distribution",
    "text": "Expected Value of a Binomial Distribution"
  },
  {
    "objectID": "files/Lecture_3_class/index.html#continued",
    "href": "files/Lecture_3_class/index.html#continued",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Continued",
    "text": "Continued"
  },
  {
    "objectID": "files/Lecture_3_class/index.html#poisson-distribution-1",
    "href": "files/Lecture_3_class/index.html#poisson-distribution-1",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nThe poisson distribution describes an experiment that measures that occurrence of an event at specific point and/or time period.\n\n\\(P(X=x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}\\)"
  },
  {
    "objectID": "files/Lecture_3_class/index.html#expected-value-of-a-poisson-distribution",
    "href": "files/Lecture_3_class/index.html#expected-value-of-a-poisson-distribution",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Expected Value of a Poisson Distribution",
    "text": "Expected Value of a Poisson Distribution"
  },
  {
    "objectID": "files/Lecture_3_class/index.html#continuous-random-variables-1",
    "href": "files/Lecture_3_class/index.html#continuous-random-variables-1",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\nA random variable \\(X\\) is considered continuous if the \\(P(X=x)\\) does not exist."
  },
  {
    "objectID": "files/Lecture_3_class/index.html#cdf-1",
    "href": "files/Lecture_3_class/index.html#cdf-1",
    "title": "Review Random Variables and Distribution Functions",
    "section": "CDF",
    "text": "CDF\nThe cumulative distribution function of \\(X\\) provides the \\(P(X\\leq x)\\), denoted by \\(F(x)\\), for the domain of \\(X\\).\nProperties of the CDF of \\(X\\):\n\n\\(F(-\\infty)\\equiv \\lim_{y\\rightarrow -\\infty}F(y)=0\\)\n\\(F(\\infty)\\equiv \\lim_{y\\rightarrow \\infty}F(y)=1\\)\n\\(F(x)\\) is a nondecreaseing function"
  },
  {
    "objectID": "files/Lecture_3_class/index.html#pdf",
    "href": "files/Lecture_3_class/index.html#pdf",
    "title": "Review Random Variables and Distribution Functions",
    "section": "PDF",
    "text": "PDF\nThe probability density function of the random variable \\(X\\) is given by\n\\[\nf(x)=\\frac{dF(x)}{d(x)}=F^\\prime(x)\n\\]\nwherever the derivative exists.\nProperties of pdfs:\n\n\\(f(x)\\geq 0\\)\n\\(\\int^\\infty_{-\\infty}f(x)dx=1\\)\n\\(P(a\\leq X\\leq b) = P(a&lt;X&lt;b)=\\int^b_af(x)dx\\)"
  },
  {
    "objectID": "files/Lecture_3_class/index.html#expected-value-1",
    "href": "files/Lecture_3_class/index.html#expected-value-1",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Expected Value",
    "text": "Expected Value\nThe expected value for a continuous distribution is defined as\n\\[\nE(X)=\\int x f(x)dx\n\\]\nThe expectation of a function \\(g(X)\\) is defined as\n\\[\nE\\{g(X)\\}=\\int g(x)f(x)dx\n\\]"
  },
  {
    "objectID": "files/Lecture_3_class/index.html#expected-value-properties",
    "href": "files/Lecture_3_class/index.html#expected-value-properties",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Expected Value Properties",
    "text": "Expected Value Properties\n\n\\(E(c)=c\\), where \\(c\\) is constant\n\\(E\\{cg(X)\\}=cE\\{g(X)\\}\\)\n\\(E\\{g_1(X)+g_2(X)+\\cdots+g_n(X)\\}=E\\{g_1(X)\\}+E\\{g_2(X)\\}+\\cdots+E\\{g_n(X)\\}\\)"
  },
  {
    "objectID": "files/Lecture_3_class/index.html#variance-1",
    "href": "files/Lecture_3_class/index.html#variance-1",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Variance",
    "text": "Variance\nThe variance of continuous variable is defined as\n\\[\nVar(X) =  E[\\{X-E(X)\\}^2] = \\int \\{X-E(X)\\}^2 f(x)dx\n\\]"
  },
  {
    "objectID": "files/Lecture_3_class/index.html#uniform-distribution-1",
    "href": "files/Lecture_3_class/index.html#uniform-distribution-1",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nA random variable is said to follow uniform distribution if the density function is constant between two parameters.\n\n\\[\nf(x) = \\left\\{\\begin{array}{cc}\n\\frac{1}{b-a} & a \\leq x \\leq b\\\\\n0 & \\mathrm{elsewhere}\n\\end{array}\\right.\n\\]"
  },
  {
    "objectID": "files/Lecture_3_class/index.html#expected-value-2",
    "href": "files/Lecture_3_class/index.html#expected-value-2",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Expected Value",
    "text": "Expected Value"
  },
  {
    "objectID": "files/Lecture_3_class/index.html#normal-distribution-1",
    "href": "files/Lecture_3_class/index.html#normal-distribution-1",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nA random variable is said to follow a normal distribution if the the frequency of occurrence follow a Gaussian function.\n\n\\[\nf(x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}\n\\]"
  },
  {
    "objectID": "files/Lecture_3_class/index.html#expected-value-3",
    "href": "files/Lecture_3_class/index.html#expected-value-3",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Expected Value",
    "text": "Expected Value"
  },
  {
    "objectID": "files/Lecture_3_class/index.html#continued-1",
    "href": "files/Lecture_3_class/index.html#continued-1",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Continued",
    "text": "Continued"
  },
  {
    "objectID": "files/Lecture_3/index.html#learning-outcomes",
    "href": "files/Lecture_3/index.html#learning-outcomes",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nReview Random Variables\nReview Discrete Random Variables\nReview Continuous Random Variables"
  },
  {
    "objectID": "files/Lecture_3/index.html#random-variables",
    "href": "files/Lecture_3/index.html#random-variables",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Random Variables",
    "text": "Random Variables\nA random variable is function that maps the sample space to real value."
  },
  {
    "objectID": "files/Lecture_3/index.html#discrete-random-variables-1",
    "href": "files/Lecture_3/index.html#discrete-random-variables-1",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\nA random variable is considered to be discrete if it can only map to a finite or countably infinite number of distinct values."
  },
  {
    "objectID": "files/Lecture_3/index.html#pmf",
    "href": "files/Lecture_3/index.html#pmf",
    "title": "Review Random Variables and Distribution Functions",
    "section": "PMF",
    "text": "PMF\nThe probability mass function of discrete variable can be represented by a formula, table, or a graph. The Probability of a random variable Y can be expressed as \\(P(Y=y)\\) for all values of \\(y\\)."
  },
  {
    "objectID": "files/Lecture_3/index.html#cdf",
    "href": "files/Lecture_3/index.html#cdf",
    "title": "Review Random Variables and Distribution Functions",
    "section": "CDF",
    "text": "CDF\nThe cumulative distribution function provides the \\(P(Y\\leq y)\\) for a random variable \\(Y\\)."
  },
  {
    "objectID": "files/Lecture_3/index.html#expected-value",
    "href": "files/Lecture_3/index.html#expected-value",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Expected Value",
    "text": "Expected Value\nThe expected value is the value we expect when we randomly sample from population that follows a specific distribution. The expected value of Y is\n\\[\nE(Y)=\\sum_y yP(y)\n\\]"
  },
  {
    "objectID": "files/Lecture_3/index.html#variance",
    "href": "files/Lecture_3/index.html#variance",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Variance",
    "text": "Variance\nThe variance is the expected squared difference between the random variable and expected value.\n\\[\nVar(Y)=\\sum_y\\{y-E(Y)\\}^2P(y)\n\\]\n\\[\nVar(Y) = E(X^2) - E(X)^2\n\\]"
  },
  {
    "objectID": "files/Lecture_3/index.html#known-distributions",
    "href": "files/Lecture_3/index.html#known-distributions",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Known Distributions",
    "text": "Known Distributions\n\n\n\n\n\n\n\n\nDistribution\nParameter(s)\nPMF \\(P(Y=y)\\)\n\n\n\n\nBernoulli\n\\(p\\)\n\\(p\\)\n\n\nBinomial\n\\(n\\) and \\(p\\)\n\\((^n_y)p^y(1-p)^{n-p}\\)\n\n\nGeometric\n\\(p\\)\n\\((1-p)^{y-1}p\\)\n\n\nNegative Binomial\n\\(r\\) and \\(p\\)\n\\((^{y-1}_{r-1})p^{r-1}(1-p)^{y-r}\\)\n\n\nHypergeometric\n\\(N\\), \\(n\\), and \\(r\\)\n\\(\\frac{(^r_y)(^{N-r}_{n-y})}{(^N_n)}\\)\n\n\nPoisson\n\\(\\lambda\\)\n\\(\\frac{\\lambda^y}{y!} e^{-\\lambda}\\)"
  },
  {
    "objectID": "files/Lecture_3/index.html#binomial-distribution-1",
    "href": "files/Lecture_3/index.html#binomial-distribution-1",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\nAn experiment is said to follow a binomial distribution if\n\nFixed \\(n\\)\nEach trial has 2 outcomes\nThe probability of success is a constant \\(p\\)\nThe trials are independent of each\n\n\n\\(P(X=x)=(^n_x)p^x(1-p)^{n-x}\\)"
  },
  {
    "objectID": "files/Lecture_3/index.html#expected-value-of-a-binomial-distribution",
    "href": "files/Lecture_3/index.html#expected-value-of-a-binomial-distribution",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Expected Value of a Binomial Distribution",
    "text": "Expected Value of a Binomial Distribution"
  },
  {
    "objectID": "files/Lecture_3/index.html#continued",
    "href": "files/Lecture_3/index.html#continued",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Continued",
    "text": "Continued"
  },
  {
    "objectID": "files/Lecture_3/index.html#poisson-distribution-1",
    "href": "files/Lecture_3/index.html#poisson-distribution-1",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nThe poisson distribution describes an experiment that measures that occurrence of an event at specific point and/or time period.\n\n\\(P(X=x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}\\)"
  },
  {
    "objectID": "files/Lecture_3/index.html#expected-value-of-a-poisson-distribution",
    "href": "files/Lecture_3/index.html#expected-value-of-a-poisson-distribution",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Expected Value of a Poisson Distribution",
    "text": "Expected Value of a Poisson Distribution"
  },
  {
    "objectID": "files/Lecture_3/index.html#continuous-random-variables-1",
    "href": "files/Lecture_3/index.html#continuous-random-variables-1",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\nA random variable \\(X\\) is considered continuous if the \\(P(X=x)\\) does not exist."
  },
  {
    "objectID": "files/Lecture_3/index.html#cdf-1",
    "href": "files/Lecture_3/index.html#cdf-1",
    "title": "Review Random Variables and Distribution Functions",
    "section": "CDF",
    "text": "CDF\nThe cumulative distribution function of \\(X\\) provides the \\(P(X\\leq x)\\), denoted by \\(F(x)\\), for the domain of \\(X\\).\nProperties of the CDF of \\(X\\):\n\n\\(F(-\\infty)\\equiv \\lim_{y\\rightarrow -\\infty}F(y)=0\\)\n\\(F(\\infty)\\equiv \\lim_{y\\rightarrow \\infty}F(y)=1\\)\n\\(F(x)\\) is a nondecreaseing function"
  },
  {
    "objectID": "files/Lecture_3/index.html#pdf",
    "href": "files/Lecture_3/index.html#pdf",
    "title": "Review Random Variables and Distribution Functions",
    "section": "PDF",
    "text": "PDF\nThe probability density function of the random variable \\(X\\) is given by\n\\[\nf(x)=\\frac{dF(x)}{d(x)}=F^\\prime(x)\n\\]\nwherever the derivative exists.\nProperties of pdfs:\n\n\\(f(x)\\geq 0\\)\n\\(\\int^\\infty_{-\\infty}f(x)dx=1\\)\n\\(P(a\\leq X\\leq b) = P(a&lt;X&lt;b)=\\int^b_af(x)dx\\)"
  },
  {
    "objectID": "files/Lecture_3/index.html#expected-value-1",
    "href": "files/Lecture_3/index.html#expected-value-1",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Expected Value",
    "text": "Expected Value\nThe expected value for a continuous distribution is defined as\n\\[\nE(X)=\\int x f(x)dx\n\\]\nThe expectation of a function \\(g(X)\\) is defined as\n\\[\nE\\{g(X)\\}=\\int g(x)f(x)dx\n\\]"
  },
  {
    "objectID": "files/Lecture_3/index.html#expected-value-properties",
    "href": "files/Lecture_3/index.html#expected-value-properties",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Expected Value Properties",
    "text": "Expected Value Properties\n\n\\(E(c)=c\\), where \\(c\\) is constant\n\\(E\\{cg(X)\\}=cE\\{g(X)\\}\\)\n\\(E\\{g_1(X)+g_2(X)+\\cdots+g_n(X)\\}=E\\{g_1(X)\\}+E\\{g_2(X)\\}+\\cdots+E\\{g_n(X)\\}\\)"
  },
  {
    "objectID": "files/Lecture_3/index.html#variance-1",
    "href": "files/Lecture_3/index.html#variance-1",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Variance",
    "text": "Variance\nThe variance of continuous variable is defined as\n\\[\nVar(X) =  E[\\{X-E(X)\\}^2] = \\int \\{X-E(X)\\}^2 f(x)dx\n\\]"
  },
  {
    "objectID": "files/Lecture_3/index.html#uniform-distribution-1",
    "href": "files/Lecture_3/index.html#uniform-distribution-1",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nA random variable is said to follow uniform distribution if the density function is constant between two parameters.\n\n\\[\nf(x) = \\left\\{\\begin{array}{cc}\n\\frac{1}{b-a} & a \\leq x \\leq b\\\\\n0 & \\mathrm{elsewhere}\n\\end{array}\\right.\n\\]"
  },
  {
    "objectID": "files/Lecture_3/index.html#expected-value-2",
    "href": "files/Lecture_3/index.html#expected-value-2",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Expected Value",
    "text": "Expected Value"
  },
  {
    "objectID": "files/Lecture_3/index.html#normal-distribution-1",
    "href": "files/Lecture_3/index.html#normal-distribution-1",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nA random variable is said to follow a normal distribution if the the frequency of occurrence follow a Gaussian function.\n\n\\[\nf(x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}\n\\]"
  },
  {
    "objectID": "files/Lecture_3/index.html#expected-value-3",
    "href": "files/Lecture_3/index.html#expected-value-3",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Expected Value",
    "text": "Expected Value"
  },
  {
    "objectID": "files/Lecture_3/index.html#continued-1",
    "href": "files/Lecture_3/index.html#continued-1",
    "title": "Review Random Variables and Distribution Functions",
    "section": "Continued",
    "text": "Continued"
  },
  {
    "objectID": "files/Lecture_16_class/index.html#learning-outcomes",
    "href": "files/Lecture_16_class/index.html#learning-outcomes",
    "title": "Likelihood Ratio Test",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nLikelihood Ratio Test"
  },
  {
    "objectID": "files/Lecture_16_class/index.html#likelihood-ratio-test-1",
    "href": "files/Lecture_16_class/index.html#likelihood-ratio-test-1",
    "title": "Likelihood Ratio Test",
    "section": "Likelihood Ratio Test",
    "text": "Likelihood Ratio Test\nThe likelihood ratio test is used when you cannot find a uniformly most powerful test for a given set of hypothesis. This will yield a very good test that is decently powerful."
  },
  {
    "objectID": "files/Lecture_16_class/index.html#hypothesis",
    "href": "files/Lecture_16_class/index.html#hypothesis",
    "title": "Likelihood Ratio Test",
    "section": "Hypothesis",
    "text": "Hypothesis\n\n\n\\(H_0:\\ \\theta\\in\\Theta_0\\)\n\n\\(H_a:\\ \\theta\\in\\Theta_a\\)\n\n\n\\(\\Theta = \\Theta_0\\cup\\Theta_a\\)\n\\(\\Theta\\) is the parameters space"
  },
  {
    "objectID": "files/Lecture_16_class/index.html#test-statistic",
    "href": "files/Lecture_16_class/index.html#test-statistic",
    "title": "Likelihood Ratio Test",
    "section": "Test Statistic",
    "text": "Test Statistic\n\\[\n\\Lambda = \\frac{L(\\hat\\theta_0)}{L(\\hat\\theta)}=\\frac{f(x_1,\\ldots,x_n;\\hat\\theta_0)}{f(x_1,\\ldots,x_n;\\hat\\theta)}\n\\]\n\n\\(\\hat\\theta_0=\\underset{\\theta\\in\\Theta_0}{\\arg\\max}\\ L(\\theta)\\)\n\\(\\hat\\theta=\\underset{\\theta\\in\\Theta}{\\arg\\max}\\ L(\\theta)\\): MLE"
  },
  {
    "objectID": "files/Lecture_16_class/index.html#decision",
    "href": "files/Lecture_16_class/index.html#decision",
    "title": "Likelihood Ratio Test",
    "section": "Decision",
    "text": "Decision\nReject \\(H_0\\) if \\(\\Lambda\\le k\\)"
  },
  {
    "objectID": "files/Lecture_16_class/index.html#one-sample-t-test",
    "href": "files/Lecture_16_class/index.html#one-sample-t-test",
    "title": "Likelihood Ratio Test",
    "section": "One-Sample t-test",
    "text": "One-Sample t-test\n\n\n\\(H_0:\\ \\mu=\\mu_0\\)\n\n\\(H_a:\\ \\mu\\ne\\mu_0\\)\n\n\\[\nX_1,\\ldots,X_n\\sim N(\\mu,\\sigma^2)\n\\]"
  },
  {
    "objectID": "files/Lecture_14_class/index.html#learning-outcomes",
    "href": "files/Lecture_14_class/index.html#learning-outcomes",
    "title": "Rao-Blackwell Theorem and MVUE",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nRao-Blackwell Theorem\nMVUE\nMLE Properties"
  },
  {
    "objectID": "files/Lecture_14_class/index.html#rao-blackwell-theorem-1",
    "href": "files/Lecture_14_class/index.html#rao-blackwell-theorem-1",
    "title": "Rao-Blackwell Theorem and MVUE",
    "section": "Rao-Blackwell Theorem",
    "text": "Rao-Blackwell Theorem\nLet the joint distribution of \\(X_1,\\ldots,X_n\\) depend on parameter \\(\\theta\\) and let \\(T\\) be a sufficient statistic for \\(\\theta\\). If we consider estimating \\(h(\\theta)\\), a function of \\(\\theta\\). If \\(U\\) is an unbiased estimator of \\(h(\\theta)\\), then the estimator \\(U^*=E(U|T)\\) is also an unbiased estimator of \\(h(\\theta)\\) and has a variance no larger than \\(U\\)."
  },
  {
    "objectID": "files/Lecture_14_class/index.html#minimum-variance-unbiased-estimator-1",
    "href": "files/Lecture_14_class/index.html#minimum-variance-unbiased-estimator-1",
    "title": "Rao-Blackwell Theorem and MVUE",
    "section": "Minimum Variance Unbiased Estimator",
    "text": "Minimum Variance Unbiased Estimator\nLet \\(\\mathcal U\\) be a set of all unbiased estimators \\(T\\) of \\(\\theta\\in\\Theta\\) and \\(E(T^2)&lt;\\infty\\). An estimator \\(T_0\\) is said to be the minimum variance unbiased estimator for \\(\\theta\\) if\n\\[\nE\\{(T_0-\\theta)^2\\}\\le E\\{(T-\\theta)^2\\}\n\\]\nfor all \\(\\theta\\in\\Theta\\) and every \\(T\\in \\mathcal U\\)."
  },
  {
    "objectID": "files/Lecture_14_class/index.html#invariance-property",
    "href": "files/Lecture_14_class/index.html#invariance-property",
    "title": "Rao-Blackwell Theorem and MVUE",
    "section": "Invariance Property",
    "text": "Invariance Property\nIf \\(\\hat \\theta\\) is an ML estimator of \\(\\theta\\), then for any one-to-one function \\(g\\), the ML estimator for \\(g(\\theta)\\) is \\(g(\\hat\\theta)\\)."
  },
  {
    "objectID": "files/Lecture_14_class/index.html#large-sample-theory",
    "href": "files/Lecture_14_class/index.html#large-sample-theory",
    "title": "Rao-Blackwell Theorem and MVUE",
    "section": "Large Sample Theory",
    "text": "Large Sample Theory\nLet \\(X_1,\\ldots,X_n\\) be a random sample from a distribution with parameter \\(\\theta\\). Let \\(\\hat \\theta\\) be the MLE estimator for a parameter \\(\\theta\\). As \\(n\\rightarrow\\infty\\), then \\(\\hat \\theta\\) has a normal distribution with mean \\(\\theta\\) and variance \\(1/nI(\\theta)\\), where\n\\[\nI(\\theta)=E\\left[-\\frac{\\partial^2}{\\partial\\theta^2}\\log\\{f(X;\\theta)\\}\\right]\n\\]"
  },
  {
    "objectID": "files/Lecture_14_class/index.html#weibull-distribution",
    "href": "files/Lecture_14_class/index.html#weibull-distribution",
    "title": "Rao-Blackwell Theorem and MVUE",
    "section": "Weibull Distribution",
    "text": "Weibull Distribution\nLet \\(X_1,\\ldots,X_n\\) be a random sample with the following distribution:\n\\[\nf(x)=\\frac{2x}{\\theta}e^{-x^2/\\theta}\\ \\mathrm{for}\\ x&gt;0\n\\]\nFind the MVUE of \\(\\theta\\)."
  },
  {
    "objectID": "files/Lecture_14_class/index.html#exponential-distribution",
    "href": "files/Lecture_14_class/index.html#exponential-distribution",
    "title": "Rao-Blackwell Theorem and MVUE",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\nLet \\(X_1,\\ldots,X_n\\) be a random sample with the following distribution:\n\\[\nf(x)=\\frac{1}{\\theta}e^{-x/\\theta}\\ \\mathrm{for}\\ x&gt;0\n\\]\nFind the MVUE of \\(\\theta\\)."
  },
  {
    "objectID": "files/Lecture_14_class/index.html#normal-distribution",
    "href": "files/Lecture_14_class/index.html#normal-distribution",
    "title": "Rao-Blackwell Theorem and MVUE",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nLet \\(X_1,\\ldots,X_n\\) be a random sample with the following distribution:\n\\[\nf(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-(x-\\mu)^2/2\\sigma^2}\n\\]\nFind the MVUE of \\(\\mu\\) and \\(\\sigma^2\\)."
  },
  {
    "objectID": "files/Lecture_14_class/index.html#bernoulli-distribution",
    "href": "files/Lecture_14_class/index.html#bernoulli-distribution",
    "title": "Rao-Blackwell Theorem and MVUE",
    "section": "Bernoulli Distribution",
    "text": "Bernoulli Distribution\nLet \\(X_1,\\ldots,X_n\\) be a random sample with the following distribution:\n\\[\nf(x)=p^x(1-p)^{1-x}\n\\]\nFind the MVUE of \\(p\\)."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Term: Fall 2024\nInstructor: Isaac Quintanilla Salinas\nContact: isaac.qs@csuci.edu\nOffice Location: BTE 2840\nOffice Hours: MW 5-6 PM T 10AM-12PM\nLecture: Monday and Wednesday 12-1:15 PM in BT 2688\nPre-Requisites: MATH 201 or MATH 202/PSY 202 or MATH 300 or MATH 352\n\n\n\nThis course is an introduction to mathematical statistics with an emphasis on statistical estimation and hypothesis testing. The course will be comprised of both theory and applications. We begin with a condensed review of fundamental concepts from Math 352; particularly, we briefly review important discrete and continuous probability distributions. We will then begin our discussion on the main topic of this course, statistical inference, through the study of distributions of functions of random variables using the method of moment-generating functions and order statistics. We then discuss ideas of convergence with sampling distributions and the central limit theorem. Next, we consider the topics of estimation, properties of point estimators, and methods of estimation. Finally, we study the theory of statistical tests and likelihood ratio tests. Depending on time, other topics may be added or removed.\n\n\n\n\nDemonstrate statistical knowledge and apply it to various data sets.\nUse basic principles of statistical inference (both Bayesian and frequentist).\nBuild a starter statistical toolbox and discuss the utility and limitations of these techniques.\nUse software and simulation to do statistics.\nDemonstrate ability to discuss statistical information in oral and written form.\n\n\n\n\n\nModern Mathematical Statistics (MMS) with Applications, by Jay L. Devore and Kenneth N. Berk , Second Edition, Springer, 2021. (available online for free through the Broome Library).\n\n\n\n\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n25%\n\n\nExam 1\n25%\n\n\nExam 2\n25%\n\n\nExam 3\n25%\n\n\n\nAt the end of the quarter, course grades will be assigned according to the following scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA+\n98 – 100\nB+\n87 – &lt;90\nC+\n77 – &lt;80\nD+\n67 – &lt;70\n\n\n\n\nA\n93 – &lt;98\nB\n83 – &lt;87\nC\n73 – &lt;77\nD\n63 – &lt;67\nF\n&lt; 60\n\n\nA–\n90 - &lt;93\nB-\n80 – &lt;83\nC–\n70 – &lt;73\nD–\n60 – &lt;63\n\n\n\n\n\n\n\nHomework will be assigned on a regular basis and posted on https://m453.inqs.info/hw.html and CANVAS. All homework assignments are due at the beginning of class. The homework is to help you practice the concepts learned in lecture and to help you study. You must turn in your own individual homework and show your understanding of the material. At the end of the semester, the three lowest homework grades will be dropped.\n\n\n\nThere will be three exams and Final.\nExam #1 will most likely be during the 6th week of the semester. Exam #2 will most likely be during the 11th week of the semester. Exam #3 will be on December 9, 2024 10:30 AM - 12:30 PM.\nWhile the exams are not considered cumulative, the material builds on each other. Developing a strong understanding of the material through out the course is important for your success.\nAt the end of the semester, your lowest exam grade will be replaced by your median average exam grade.\nThis course will operate under a zero-tolerance policy. Talking during the time of the exam, sharing materials, looking at another students’ exam, or not following directions given will be subject to the\nUniversity’s academic integrity policy.\n\n\n\nThere will be 4 extra credit opportunities worth a total of 10% of your overall grade. (There are no make-ups for missed extra credit assignments!) More information will be provided on the extra credit assignments on a later date.\n\n\n\n\nThe following outline may be subject to change. Any changes will be announced in class.\n\n\n\n\n\n\n\n\n\nWeek\nLecture\nTopic\nReading\n\n\n\n\n1 8/26-8/30\n1 Monday\nIntroduction to Statistics\n1.1-1.4\n\n\n\n2 Wednesday\nReview: RV and Distribution Function\n3.1, 3.5-3.7, 4.1, 4.3-4.5\n\n\n2 9/2-9/6\n3 Labor Day\n\n\n\n\n\n4 Wednesday\nReview: Moment Generating Functions\n3.4, 4.2\n\n\n3 9/9-9/13\n5 Monday\nTransonformation of Random Variables\n4.7\n\n\n\n6 Wednesday\nJoint Distributions\n5.1, 5.2\n\n\n4 9/16- 9/20\n7 Monday\nLinear Combinations and Conditional Distributions\n5.3, 5.4\n\n\n\n8 Wednesday\nFunctions of RV’s and Order Statistics\n5.6, 5.7\n\n\n5 9/23-9/27\n9 Monday\nSampling Distributions\n6.1, 6.2\n\n\n\n10 Wednesday\nCommon Sampling Distributions\n6.3, 6.4\n\n\n6 9/30-10/4\n11 Monday\nExam #1\n\n\n\n\n12 Wednesday\nPoint Estimation\n7.1\n\n\n7 10/7-10/11\n13 Monday\nMaximum Likelihood Estimator\n7.2\n\n\n\n14 Wednesday\nMethod of Moments Estimator\n7.2\n\n\n8 10/14-10/18\n15 Monday\nSufficiency\n7.3\n\n\n\n16 Wednesday\nInformation and Efficiency\n7.4\n\n\n9 10/21-10/25\n17 Monday\nConfidence Intervals\n8.1\n\n\n\n18 Wednesday\nSingle Sample Intervals\n8.2\n\n\n10 10/28-11/1\n19 Monday\nProportion Intervals\n8.3\n\n\n\n20 Wednesday\nIntervals for Variances\n8.4\n\n\n11 11/4-11/15\n21 Monday\nBootstrap-based Intervals\n8.5\n\n\n\n22 Wednesday\nExam #2\n\n\n\n12 11/11-11/15\n23 Monday\nVeteran’s Day\n\n\n\n\n24 Wednesday\nHypothesis Testing\n9.1\n\n\n13 11/18-11/22\n25 Monday\nTests for Population Mean\n9.2\n\n\n\n26 Wednesday\nTests for Population Porportion\n9.3\n\n\n14 11/25-11/29\n27 Monday\nP-Value\n9.4\n\n\n\n28 Wednesday\nNeyman-Pearson Lemma and Likelihood Ratio Test (Virtual Class)\n9.5\n\n\n15 12/2-12/6\n29 Monday\nSimple Linear Regression\n12.1\n\n\n\n30 Wednesday\nEstimation and Inference\n12.2-12.4\n\n\n16 12/9\n\nExam #3\n\n\n\n\n\n\n\n\nAcademic Honesty:\nPlease conduct yourself with honesty and integrity. Do not submit others’ work as your own. For assignments and quizzes that allow you to work with a group, only put your name on what the group submits if you genuinely contributed to the work. Work completely independently on exams, using only the materials that are indicated as allowed. Failure to observe academic honesty results in substantial penalties that can include failing the course.\nDisabilities:\nIf you are a student with a disability requesting reasonable accommodations in this course, you need to contact Disability Accommodations and Support Services (DASS) located on the second floor of Arroyo Hall, via email accommodations@csuci.edu or call 805-437-3331. All requests for reasonable accommodations require registration with DASS in advance of need: https://www.csuci.edu/dass/students/apply-for-services.htm. Faculty, students and DASS will work together regarding classroom accommodations. You are encouraged to discuss approved.\nEmergency Procedure Notice to Students:\nCSUCI is following guidelines and public orders from the California Department of Public Health and Ventura County Public Health for the COVID-19 pandemic as it pertains to CSUCI students, employees and visitors on the campus. Students are expected to adhere to all health and safety requirements as noted on the University’s Fall 2022 Semester website or they may be subject to removal from the classroom."
  },
  {
    "objectID": "syllabus.html#math-453-mathematical-statistics",
    "href": "syllabus.html#math-453-mathematical-statistics",
    "title": "Syllabus",
    "section": "",
    "text": "Term: Fall 2024\nInstructor: Isaac Quintanilla Salinas\nContact: isaac.qs@csuci.edu\nOffice Location: BTE 2840\nOffice Hours: MW 5-6 PM T 10AM-12PM\nLecture: Monday and Wednesday 12-1:15 PM in BT 2688\nPre-Requisites: MATH 201 or MATH 202/PSY 202 or MATH 300 or MATH 352\n\n\n\nThis course is an introduction to mathematical statistics with an emphasis on statistical estimation and hypothesis testing. The course will be comprised of both theory and applications. We begin with a condensed review of fundamental concepts from Math 352; particularly, we briefly review important discrete and continuous probability distributions. We will then begin our discussion on the main topic of this course, statistical inference, through the study of distributions of functions of random variables using the method of moment-generating functions and order statistics. We then discuss ideas of convergence with sampling distributions and the central limit theorem. Next, we consider the topics of estimation, properties of point estimators, and methods of estimation. Finally, we study the theory of statistical tests and likelihood ratio tests. Depending on time, other topics may be added or removed.\n\n\n\n\nDemonstrate statistical knowledge and apply it to various data sets.\nUse basic principles of statistical inference (both Bayesian and frequentist).\nBuild a starter statistical toolbox and discuss the utility and limitations of these techniques.\nUse software and simulation to do statistics.\nDemonstrate ability to discuss statistical information in oral and written form.\n\n\n\n\n\nModern Mathematical Statistics (MMS) with Applications, by Jay L. Devore and Kenneth N. Berk , Second Edition, Springer, 2021. (available online for free through the Broome Library).\n\n\n\n\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n25%\n\n\nExam 1\n25%\n\n\nExam 2\n25%\n\n\nExam 3\n25%\n\n\n\nAt the end of the quarter, course grades will be assigned according to the following scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA+\n98 – 100\nB+\n87 – &lt;90\nC+\n77 – &lt;80\nD+\n67 – &lt;70\n\n\n\n\nA\n93 – &lt;98\nB\n83 – &lt;87\nC\n73 – &lt;77\nD\n63 – &lt;67\nF\n&lt; 60\n\n\nA–\n90 - &lt;93\nB-\n80 – &lt;83\nC–\n70 – &lt;73\nD–\n60 – &lt;63\n\n\n\n\n\n\n\nHomework will be assigned on a regular basis and posted on https://m453.inqs.info/hw.html and CANVAS. All homework assignments are due at the beginning of class. The homework is to help you practice the concepts learned in lecture and to help you study. You must turn in your own individual homework and show your understanding of the material. At the end of the semester, the three lowest homework grades will be dropped.\n\n\n\nThere will be three exams and Final.\nExam #1 will most likely be during the 6th week of the semester. Exam #2 will most likely be during the 11th week of the semester. Exam #3 will be on December 9, 2024 10:30 AM - 12:30 PM.\nWhile the exams are not considered cumulative, the material builds on each other. Developing a strong understanding of the material through out the course is important for your success.\nAt the end of the semester, your lowest exam grade will be replaced by your median average exam grade.\nThis course will operate under a zero-tolerance policy. Talking during the time of the exam, sharing materials, looking at another students’ exam, or not following directions given will be subject to the\nUniversity’s academic integrity policy.\n\n\n\nThere will be 4 extra credit opportunities worth a total of 10% of your overall grade. (There are no make-ups for missed extra credit assignments!) More information will be provided on the extra credit assignments on a later date.\n\n\n\n\nThe following outline may be subject to change. Any changes will be announced in class.\n\n\n\n\n\n\n\n\n\nWeek\nLecture\nTopic\nReading\n\n\n\n\n1 8/26-8/30\n1 Monday\nIntroduction to Statistics\n1.1-1.4\n\n\n\n2 Wednesday\nReview: RV and Distribution Function\n3.1, 3.5-3.7, 4.1, 4.3-4.5\n\n\n2 9/2-9/6\n3 Labor Day\n\n\n\n\n\n4 Wednesday\nReview: Moment Generating Functions\n3.4, 4.2\n\n\n3 9/9-9/13\n5 Monday\nTransonformation of Random Variables\n4.7\n\n\n\n6 Wednesday\nJoint Distributions\n5.1, 5.2\n\n\n4 9/16- 9/20\n7 Monday\nLinear Combinations and Conditional Distributions\n5.3, 5.4\n\n\n\n8 Wednesday\nFunctions of RV’s and Order Statistics\n5.6, 5.7\n\n\n5 9/23-9/27\n9 Monday\nSampling Distributions\n6.1, 6.2\n\n\n\n10 Wednesday\nCommon Sampling Distributions\n6.3, 6.4\n\n\n6 9/30-10/4\n11 Monday\nExam #1\n\n\n\n\n12 Wednesday\nPoint Estimation\n7.1\n\n\n7 10/7-10/11\n13 Monday\nMaximum Likelihood Estimator\n7.2\n\n\n\n14 Wednesday\nMethod of Moments Estimator\n7.2\n\n\n8 10/14-10/18\n15 Monday\nSufficiency\n7.3\n\n\n\n16 Wednesday\nInformation and Efficiency\n7.4\n\n\n9 10/21-10/25\n17 Monday\nConfidence Intervals\n8.1\n\n\n\n18 Wednesday\nSingle Sample Intervals\n8.2\n\n\n10 10/28-11/1\n19 Monday\nProportion Intervals\n8.3\n\n\n\n20 Wednesday\nIntervals for Variances\n8.4\n\n\n11 11/4-11/15\n21 Monday\nBootstrap-based Intervals\n8.5\n\n\n\n22 Wednesday\nExam #2\n\n\n\n12 11/11-11/15\n23 Monday\nVeteran’s Day\n\n\n\n\n24 Wednesday\nHypothesis Testing\n9.1\n\n\n13 11/18-11/22\n25 Monday\nTests for Population Mean\n9.2\n\n\n\n26 Wednesday\nTests for Population Porportion\n9.3\n\n\n14 11/25-11/29\n27 Monday\nP-Value\n9.4\n\n\n\n28 Wednesday\nNeyman-Pearson Lemma and Likelihood Ratio Test (Virtual Class)\n9.5\n\n\n15 12/2-12/6\n29 Monday\nSimple Linear Regression\n12.1\n\n\n\n30 Wednesday\nEstimation and Inference\n12.2-12.4\n\n\n16 12/9\n\nExam #3\n\n\n\n\n\n\n\n\nAcademic Honesty:\nPlease conduct yourself with honesty and integrity. Do not submit others’ work as your own. For assignments and quizzes that allow you to work with a group, only put your name on what the group submits if you genuinely contributed to the work. Work completely independently on exams, using only the materials that are indicated as allowed. Failure to observe academic honesty results in substantial penalties that can include failing the course.\nDisabilities:\nIf you are a student with a disability requesting reasonable accommodations in this course, you need to contact Disability Accommodations and Support Services (DASS) located on the second floor of Arroyo Hall, via email accommodations@csuci.edu or call 805-437-3331. All requests for reasonable accommodations require registration with DASS in advance of need: https://www.csuci.edu/dass/students/apply-for-services.htm. Faculty, students and DASS will work together regarding classroom accommodations. You are encouraged to discuss approved.\nEmergency Procedure Notice to Students:\nCSUCI is following guidelines and public orders from the California Department of Public Health and Ventura County Public Health for the COVID-19 pandemic as it pertains to CSUCI students, employees and visitors on the campus. Students are expected to adhere to all health and safety requirements as noted on the University’s Fall 2022 Semester website or they may be subject to removal from the classroom."
  },
  {
    "objectID": "lectures/3b.html#partial-derivatives",
    "href": "lectures/3b.html#partial-derivatives",
    "title": "Joint Distribution Functions",
    "section": "Partial Derivatives",
    "text": "Partial Derivatives\nFor a function \\(f(x,y)\\), the partial derivative with respect to \\(x\\) is taken by differentiating \\(f(x,y)\\) with respect to \\(x\\) while treating \\(y\\) as a constant. For example:\n\\(f(x,y) = x^2 + \\ln(y)\\)"
  },
  {
    "objectID": "lectures/3b.html#multiple-integration",
    "href": "lectures/3b.html#multiple-integration",
    "title": "Joint Distribution Functions",
    "section": "Multiple Integration",
    "text": "Multiple Integration\nMultiple integration is when you integrate a multivariate function by multiple variables. This is done by integrating the function by an individual variable at a time. For example:\n\\(f(x,y)=x^2 + y^2\\) which can be integrated as:"
  },
  {
    "objectID": "lectures/3b.html#joint-distributions-1",
    "href": "lectures/3b.html#joint-distributions-1",
    "title": "Joint Distribution Functions",
    "section": "Joint Distributions",
    "text": "Joint Distributions\nA joint distribution is a process where more than one random variable is generated; for example, collecting biomedical data, such as multiple biomarkers, are considered to follow a joint distribution. In mathematical terms, instead of dealing with a random variable, we are dealing with a random vector. Observing a particular random vector will have a probability attached to it."
  },
  {
    "objectID": "lectures/3b.html#bivariate-discrete-distributions",
    "href": "lectures/3b.html#bivariate-discrete-distributions",
    "title": "Joint Distribution Functions",
    "section": "Bivariate Discrete Distributions",
    "text": "Bivariate Discrete Distributions\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, the joint distribution function of \\((X_1, X_2)\\) is defined as\n\\[\np_{X_1,X_2}(x_1, x_2) = P(X_1=x_1, X_2 = x_2).\n\\]\nThe properties of a bivariate discrete distribution are\n\n\\(p_{X_1,X_2}(x_1,x_2)\\ge 0\\) for all \\(x_1,\\ x_2\\)\n\\(\\sum_{x_1}\\sum_{x2}p(x_1,x_2)=1\\)"
  },
  {
    "objectID": "lectures/3b.html#bivariate-continuous-distribution",
    "href": "lectures/3b.html#bivariate-continuous-distribution",
    "title": "Joint Distribution Functions",
    "section": "Bivariate Continuous Distribution",
    "text": "Bivariate Continuous Distribution\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, the joint distribution function of \\((X_1, X_2)\\) is defined as\n\\[\nF_{X_1,X_2}(x_1, x_2) = P(X_1\\le x_1, X_2 \\le x_2).\n\\]\nThe properties of a bivariate continuous distribution are\n\n\\(f_{X_1,X_2}(x_1,x_2)=\\frac{\\partial^2F(x_1,x_2)}{\\partial x_1\\partial x_2}\\)\n\\(f_{X_1,X_2}(x_1, x_2)\\ge 0\\)\n\\(\\int_{x_1}\\int_{x_2}f_{X_1,X_2}(x_1,x_2)dx_2dx_1=1\\)"
  },
  {
    "objectID": "lectures/3b.html#example",
    "href": "lectures/3b.html#example",
    "title": "Joint Distribution Functions",
    "section": "Example",
    "text": "Example\n\\[\nf(x,y) \\left\\{\\begin{array}{cc}\n3x & 0\\le y\\le x\\le 1\\\\\n0 & \\mathrm{otherwise}\n\\end{array}\\right.\n\\]\nFind \\(P(0\\le X\\le 0.5,0.25\\le Y)\\)"
  },
  {
    "objectID": "lectures/3b.html#marginal-density-functions",
    "href": "lectures/3b.html#marginal-density-functions",
    "title": "Joint Distribution Functions",
    "section": "Marginal Density Functions",
    "text": "Marginal Density Functions\nA Marginal Density Function is density function of one random variable from a random vector."
  },
  {
    "objectID": "lectures/3b.html#marginal-discrete-probability-mass-function",
    "href": "lectures/3b.html#marginal-discrete-probability-mass-function",
    "title": "Joint Distribution Functions",
    "section": "Marginal Discrete Probability Mass Function",
    "text": "Marginal Discrete Probability Mass Function\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint distribution function of\n\\[\np_{X_1,X_2}(x_1, x_2) = P(X_1=x_1, X_2 = x_2).\n\\]\nThe marginal distribution of \\(X_1\\) is defined as\n\\[\np_{X_1}(x_1) = \\sum_{x_2}p_{X_1,X_2}(x_1,x_2)\n\\]"
  },
  {
    "objectID": "lectures/3b.html#marginal-continuous-density-function",
    "href": "lectures/3b.html#marginal-continuous-density-function",
    "title": "Joint Distribution Functions",
    "section": "Marginal Continuous Density Function",
    "text": "Marginal Continuous Density Function\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). The marginal distribution of \\(X_1\\) is defined as\n\\[\nf_{X_1}(x_1) = \\int_{x_2}f_{X_1,X_2}(x_1,x_2)dx_2\n\\]"
  },
  {
    "objectID": "lectures/3b.html#example-1",
    "href": "lectures/3b.html#example-1",
    "title": "Joint Distribution Functions",
    "section": "Example",
    "text": "Example\n\\[\nf_{X,Y}(x,y) \\left\\{\\begin{array}{cc}\n2x & 0\\le y \\le 1;\\ 0 \\le x\\le 1\\\\\n0 & \\mathrm{otherwise}\n\\end{array}\\right.\n\\]\nFind \\(f_X(x)\\)"
  },
  {
    "objectID": "lectures/3b.html#conditional-distributions-1",
    "href": "lectures/3b.html#conditional-distributions-1",
    "title": "Joint Distribution Functions",
    "section": "Conditional Distributions",
    "text": "Conditional Distributions\nA conditional distribution provides the probability of a random variable, given that it was conditioned on the value of a second random variable."
  },
  {
    "objectID": "lectures/3b.html#discrete-conditional-distributions",
    "href": "lectures/3b.html#discrete-conditional-distributions",
    "title": "Joint Distribution Functions",
    "section": "Discrete Conditional Distributions",
    "text": "Discrete Conditional Distributions\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint distribution function of\n\\[\np_{X_1,X_2}(x_1, x_2) = P(X_1=x_1, X_2 = x_2).\n\\]\nThe conditional distribution of \\(X_1|X_2=x_2\\) is defined as\n\\[\np_{X_1|X_2}(x_1) = \\frac{p_{X_1,X_2}(x_1,x_2)}{p_{X_2}(x_2)}\n\\]"
  },
  {
    "objectID": "lectures/3b.html#continuous-conditional-distributions",
    "href": "lectures/3b.html#continuous-conditional-distributions",
    "title": "Joint Distribution Functions",
    "section": "Continuous Conditional Distributions",
    "text": "Continuous Conditional Distributions\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). The conditional distribution of \\(X_1|X_2=_2\\) is defined as\n\\[\nf_{X_1|X_2}(x_1) = \\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)}\n\\]"
  },
  {
    "objectID": "lectures/3b.html#example-2",
    "href": "lectures/3b.html#example-2",
    "title": "Joint Distribution Functions",
    "section": "Example",
    "text": "Example\nLet the joint density function of \\(X_1\\) and \\(X_2\\) be defined as\n\\[\nf_{X_1,X_2}(x_1,x_2)=\\left\\{\\begin{array}{cc}\n30x_1x_2² & x_1 -1 \\le x_2 \\le 1-x_1; 0\\le x_1\\le 1\\\\\n0 & \\mathrm{elsewhere}\n\\end{array}\\right.\n\\]\nFind the conditional density function of \\(X_2|X_1=x_1\\)."
  },
  {
    "objectID": "lectures/3b.html#independent-random-variables",
    "href": "lectures/3b.html#independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Independent Random Variables",
    "text": "Independent Random Variables\nRandom variables are considered independent of each other if the probability of one variable does not affect the probability of another variable."
  },
  {
    "objectID": "lectures/3b.html#discrete-independent-random-variables",
    "href": "lectures/3b.html#discrete-independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Discrete Independent Random Variables",
    "text": "Discrete Independent Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint density function of \\(p_{X_1,X_2}(x_1,x_2)\\). \\(X_1\\) is independent of \\(X_2\\) if and only if\n\\[\np_{X_1,X_2}(x_1,x_2) = p_{X_1}(x_1)p_{X_2}(x_2)\n\\]"
  },
  {
    "objectID": "lectures/3b.html#continuous-independent-random-variables",
    "href": "lectures/3b.html#continuous-independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Continuous Independent Random Variables",
    "text": "Continuous Independent Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). \\(X_1\\) is independent of \\(X_2\\) if and only if\n\\[\nf_{X_1,X_2}(x_1,x_2) = f_{X_1}(x_1)f_{X_2}(x_2)\n\\]"
  },
  {
    "objectID": "lectures/3b.html#matrix-algebra",
    "href": "lectures/3b.html#matrix-algebra",
    "title": "Joint Distribution Functions",
    "section": "Matrix Algebra",
    "text": "Matrix Algebra\n\\[\nA = \\left(\\begin{array}{cc}\na_1 & 0\\\\\n0 & a_2\n\\end{array}\\right)\n\\]\n\\[\n\\det(A) = a_1a_2\n\\]\n\\[\nA^{-1}=\\left(\\begin{array}{cc}\n1/a_1 & 0 \\\\\n0 & 1/a_2\n\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "lectures/3b.html#example-3",
    "href": "lectures/3b.html#example-3",
    "title": "Joint Distribution Functions",
    "section": "Example",
    "text": "Example\n\\[\n\\left(\\begin{array}{c}\nX\\\\\nY\n\\end{array}\\right)\\sim N \\left\\{\n\\left(\\begin{array}{c}\n\\mu_x\\\\\n\\mu_y\n\\end{array}\\right),\\left(\\begin{array}{cc}\n\\sigma_x^2 & 0\\\\\n0 & \\sigma_y^2\n\\end{array}\\right)\n\\right\\}\n\\]\nShow that \\(X\\perp Y\\).\n\\[\nf_{X,Y}(x,y)=\\det(2\\pi\\Sigma)^{-1/2}\\exp\\left\\{-\\frac{1}{2}(\\boldsymbol{w}-\\boldsymbol\\mu)^T\\Sigma^{-1}(\\boldsymbol w-\\boldsymbol\\mu)\\right\\}\n\\]\nwhere \\(\\Sigma=\\left(\\begin{array}{cc}\\sigma_y^2 & 0\\\\0 & \\sigma_y^2\\end{array}\\right)\\), \\(\\boldsymbol \\mu = \\left(\\begin{array}{cc}\\mu_x\\\\ \\mu_y \\end{array}\\right)\\), and \\(\\boldsymbol w = \\left(\\begin{array}{cc} x\\\\ y \\end{array}\\right)\\)"
  },
  {
    "objectID": "lectures/3b.html#expectations-1",
    "href": "lectures/3b.html#expectations-1",
    "title": "Joint Distribution Functions",
    "section": "Expectations",
    "text": "Expectations\nLet \\(X_1, X_2, \\ldots,X_n\\) be a set of random variables, the expectation of a function \\(g(X_1,\\ldots, X_n)\\) is defined as\n\\[\nE\\{g(X_1,\\ldots, X_n)\\} = \\sum_{x_1\\in X_1}\\cdots\\sum_{x_n\\in X_n}g(X_1,\\ldots, X_n)p(x_1,\\ldots,x_n)\n\\]\nor\n\\[\nE\\{g(\\boldsymbol X)\\} = \\int_{x_1\\in X_1}\\cdots\\int_{x_n\\in X_n}g(\\boldsymbol X)f(\\boldsymbol X)dx_n \\cdots dx_1\n\\]\n\n\\(\\boldsymbol X = (X_1,\\cdots, X_n)\\)"
  },
  {
    "objectID": "lectures/3b.html#expected-value-and-variance-of-linear-functions",
    "href": "lectures/3b.html#expected-value-and-variance-of-linear-functions",
    "title": "Joint Distribution Functions",
    "section": "Expected Value and Variance of Linear Functions",
    "text": "Expected Value and Variance of Linear Functions\nLet \\(X_1,\\ldots,X_n\\) and \\(Y_1,\\ldots,Y_m\\) be random variables with \\(E(X_i)=\\mu_i\\) and \\(E(Y_j)=\\tau_j\\). Furthermore, let \\(U = \\sum^n_{i=1}a_iX_i\\) and \\(V=\\sum^m_{j=1}b_jY_j\\) where \\(\\{a_i\\}^n_{i=1}\\) and \\(\\{b_j\\}_{j=1}^m\\) are constants. We have the following properties:\n\n\\(E(U)=\\sum_{i=1}^na_i\\mu_i\\)\n\\(Var(U)=\\sum^n_{i=1}a_i^2Var(X_i)+2\\underset{i&lt;j}{\\sum\\sum}a_ia_jCov(X_i,X_j)\\)\n\\(Cov(U,V)=\\sum^n_{i=1}\\sum^m_{j=1}Cov(X_i,Y_j)\\)"
  },
  {
    "objectID": "lectures/3b.html#conditional-expectations",
    "href": "lectures/3b.html#conditional-expectations",
    "title": "Joint Distribution Functions",
    "section": "Conditional Expectations",
    "text": "Conditional Expectations\nLet \\(X_1\\) and \\(X_2\\) be two random variables, the conditional expectation of \\(g(X_1)\\), given \\(X_2=x_2\\), is defined as\n\\[\nE\\{g(X_1)|X_2=x_2\\}=\\sum_{x_1}g(x_1)p(x_1|x_2)\n\\]\nor\n\\[\nE\\{g(X_1)|X_2=x_2\\}=\\int_{x_1}g(x_1)f(x_1|x_2)dx_1.\n\\]"
  },
  {
    "objectID": "lectures/3b.html#conditional-expectations-1",
    "href": "lectures/3b.html#conditional-expectations-1",
    "title": "Joint Distribution Functions",
    "section": "Conditional Expectations",
    "text": "Conditional Expectations\nFurthermore,\n\\[\nE(X_1)=E_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n\\]\nand\n\\[\nVar(X_1) = E_{X_2}\\{Var_{X_1|X_2}(X_1|X_2)\\} + Var_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n\\]"
  },
  {
    "objectID": "lectures/3b.html#covariance-1",
    "href": "lectures/3b.html#covariance-1",
    "title": "Joint Distribution Functions",
    "section": "Covariance",
    "text": "Covariance\nLet \\(X_1\\) and \\(X_2\\) be 2 random variables with mean \\(E(X_1)=\\mu_1\\) and \\(E(X_2)=\\mu_2\\), respectively. The covariance of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\begin{eqnarray*}\nCov(X_1,X_2) & = & E\\{(X_1-\\mu_1)(X_2-\\mu_2)\\}\\\\\n& =& E(X_1X_2)-\\mu_1\\mu_2\n\\end{eqnarray*}\n\\]\nIf \\(X_1\\) and \\(X_2\\) are independent random variables, then\n\\[\nCov(X_1,X_2)=0\n\\]"
  },
  {
    "objectID": "lectures/3b.html#correlation",
    "href": "lectures/3b.html#correlation",
    "title": "Joint Distribution Functions",
    "section": "Correlation",
    "text": "Correlation\nThe correlation of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\rho = Cor(X_1,X_2) = \\frac{Cov(X_1,X_2)}{\\sqrt{Var(X_1)Var(X_2)}}\n\\]"
  },
  {
    "objectID": "lectures/2b.html#learning-outcomes",
    "href": "lectures/2b.html#learning-outcomes",
    "title": "Review:",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nDefine Moment Generating Functions\nDiscuss Properties"
  },
  {
    "objectID": "lectures/2b.html#moments",
    "href": "lectures/2b.html#moments",
    "title": "Review:",
    "section": "Moments",
    "text": "Moments\nThe \\(k\\)th moment is defined as the expectation of the random variable, raised to the \\(k\\)th power, defined as \\(E(X^k)\\)."
  },
  {
    "objectID": "lectures/2b.html#moment-generating-functions-1",
    "href": "lectures/2b.html#moment-generating-functions-1",
    "title": "Review:",
    "section": "Moment Generating Functions",
    "text": "Moment Generating Functions\nThe moment generating functions is used to obtain the \\(k\\)th moment. The mgf is defined as\n\\[\nm(t) = E(e^{tX})\n\\]\nThe \\(k\\)th moment can be obtained by taking the \\(k\\)th derivative of the mgf, with respect to \\(t\\), and setting \\(t\\) equal to 0:\n\\[\nE(X^k)=\\frac{d^km(t)}{dt}\\Bigg|_{t=0}\n\\]"
  },
  {
    "objectID": "lectures/2b.html#mgf",
    "href": "lectures/2b.html#mgf",
    "title": "Review:",
    "section": "MGF",
    "text": "MGF"
  },
  {
    "objectID": "lectures/2b.html#expected-value",
    "href": "lectures/2b.html#expected-value",
    "title": "Review:",
    "section": "Expected Value",
    "text": "Expected Value"
  },
  {
    "objectID": "lectures/2b.html#variance",
    "href": "lectures/2b.html#variance",
    "title": "Review:",
    "section": "Variance",
    "text": "Variance"
  },
  {
    "objectID": "lectures/2b.html#variance-1",
    "href": "lectures/2b.html#variance-1",
    "title": "Review:",
    "section": "Variance",
    "text": "Variance"
  },
  {
    "objectID": "lectures/2b.html#mgf-1",
    "href": "lectures/2b.html#mgf-1",
    "title": "Review:",
    "section": "MGF",
    "text": "MGF"
  },
  {
    "objectID": "lectures/2b.html#mgf-2",
    "href": "lectures/2b.html#mgf-2",
    "title": "Review:",
    "section": "MGF",
    "text": "MGF"
  },
  {
    "objectID": "lectures/2b.html#mgf-3",
    "href": "lectures/2b.html#mgf-3",
    "title": "Review:",
    "section": "MGF",
    "text": "MGF"
  },
  {
    "objectID": "lectures/2b.html#linearity",
    "href": "lectures/2b.html#linearity",
    "title": "Review:",
    "section": "Linearity",
    "text": "Linearity\nLet \\(X\\) follow a distribution \\(f\\), with the an MGF \\(M_X(t)\\), the MGF of \\(Y=aX+b\\) is given as\n\\[\nM_Y(t) = e^{tb}M_X(at)\n\\]"
  },
  {
    "objectID": "lectures/2b.html#derivation",
    "href": "lectures/2b.html#derivation",
    "title": "Review:",
    "section": "Derivation",
    "text": "Derivation"
  },
  {
    "objectID": "lectures/2b.html#linearity-1",
    "href": "lectures/2b.html#linearity-1",
    "title": "Review:",
    "section": "Linearity",
    "text": "Linearity\nLet \\(X\\) and \\(Y\\) be two random variables with MGFs \\(M_X(t)\\) and \\(M_Y(t)\\), respectively, and are independent. The MGF of \\(U=X-Y\\)\n\\[\nM_U(t) = M_X(t)M_Y(-t)\n\\]"
  },
  {
    "objectID": "lectures/2b.html#derivation-1",
    "href": "lectures/2b.html#derivation-1",
    "title": "Review:",
    "section": "Derivation",
    "text": "Derivation"
  },
  {
    "objectID": "lectures/2b.html#uniqueness",
    "href": "lectures/2b.html#uniqueness",
    "title": "Review:",
    "section": "Uniqueness",
    "text": "Uniqueness\nLet \\(X\\) and \\(Y\\) have the following distributions \\(F_X(x)\\) and \\(F_Y(y)\\) and MGFs \\(M_X(t)\\) and \\(M_Y(t)\\), respectively. \\(X\\) and \\(Y\\) have the same distribution \\(F_X(x)=F_Y(y)\\) if and only if \\(M_X(t)=M_Y(t)\\)."
  },
  {
    "objectID": "lectures/2b.html#uniqueness-1",
    "href": "lectures/2b.html#uniqueness-1",
    "title": "Review:",
    "section": "Uniqueness",
    "text": "Uniqueness\nLet \\(X_1,\\cdots, X_n\\) be independent random variables, where \\(X_i\\sim N(\\mu_i, \\sigma^2_i)\\), with \\(M_{X_i}(t)=\\exp\\{\\mu_i t+\\sigma^2_it^2/2\\}\\) for \\(i=1,\\cdots, n\\). Find the MGF of \\(Y=a_1X_1+\\cdots+a_nX_n\\), where \\(a_1, \\cdots, a_n\\) are constants."
  },
  {
    "objectID": "lectures/1b.html#random-variables",
    "href": "lectures/1b.html#random-variables",
    "title": "Review:",
    "section": "Random Variables",
    "text": "Random Variables\nA random variable is function that maps the sample space to real value."
  },
  {
    "objectID": "lectures/1b.html#discrete-random-variables-1",
    "href": "lectures/1b.html#discrete-random-variables-1",
    "title": "Review:",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\nA random variable is considered to be discrete if it can only map to a finite or countably infinite number of distinct values."
  },
  {
    "objectID": "lectures/1b.html#pmf",
    "href": "lectures/1b.html#pmf",
    "title": "Review:",
    "section": "PMF",
    "text": "PMF\nThe probability mass function of discrete variable can be represented by a formula, table, or a graph. The Probability of a random variable Y can be expressed as \\(P(Y=y)\\) for all values of \\(y\\)."
  },
  {
    "objectID": "lectures/1b.html#cdf",
    "href": "lectures/1b.html#cdf",
    "title": "Review:",
    "section": "CDF",
    "text": "CDF\nThe cumulative distribution function provides the \\(P(Y\\leq y)\\) for a random variable \\(Y\\)."
  },
  {
    "objectID": "lectures/1b.html#expected-value",
    "href": "lectures/1b.html#expected-value",
    "title": "Review:",
    "section": "Expected Value",
    "text": "Expected Value\nThe expected value is the value we expect when we randomly sample from population that follows a specific distribution. The expected value of Y is\n\\[\nE(Y)=\\sum_y yP(y)\n\\]"
  },
  {
    "objectID": "lectures/1b.html#variance",
    "href": "lectures/1b.html#variance",
    "title": "Review:",
    "section": "Variance",
    "text": "Variance\nThe variance is the expected squared difference between the random variable and expected value.\n\\[\nVar(Y)=\\sum_y\\{y-E(Y)\\}^2P(y)\n\\]\n\\[\nVar(Y) = E(X^2) - E(X)^2\n\\]"
  },
  {
    "objectID": "lectures/1b.html#known-distributions",
    "href": "lectures/1b.html#known-distributions",
    "title": "Review:",
    "section": "Known Distributions",
    "text": "Known Distributions\n\n\n\n\n\n\n\n\nDistribution\nParameter(s)\nPMF \\(P(Y=y)\\)\n\n\n\n\nBernoulli\n\\(p\\)\n\\(p\\)\n\n\nBinomial\n\\(n\\) and \\(p\\)\n\\((^n_y)p^y(1-p)^{n-p}\\)\n\n\nGeometric\n\\(p\\)\n\\((1-p)^{y-1}p\\)\n\n\nNegative Binomial\n\\(r\\) and \\(p\\)\n\\((^{y-1}_{r-1})p^{r-1}(1-p)^{y-r}\\)\n\n\nHypergeometric\n\\(N\\), \\(n\\), and \\(r\\)\n\\(\\frac{(^r_y)(^{N-r}_{n-y})}{(^N_n)}\\)\n\n\nPoisson\n\\(\\lambda\\)\n\\(\\frac{\\lambda^y}{y!} e^{-\\lambda}\\)"
  },
  {
    "objectID": "lectures/1b.html#binomial-distribution-1",
    "href": "lectures/1b.html#binomial-distribution-1",
    "title": "Review:",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\nAn experiment is said to follow a binomial distribution if\n\nFixed \\(n\\)\nEach trial has 2 outcomes\nThe probability of success is a constant \\(p\\)\nThe trials are independent of each\n\n\n\\(P(X=x)=(^n_x)p^x(1-p)^{n-x}\\)"
  },
  {
    "objectID": "lectures/1b.html#expected-value-of-a-binomial-distribution",
    "href": "lectures/1b.html#expected-value-of-a-binomial-distribution",
    "title": "Review:",
    "section": "Expected Value of a Binomial Distribution",
    "text": "Expected Value of a Binomial Distribution"
  },
  {
    "objectID": "lectures/1b.html#continued",
    "href": "lectures/1b.html#continued",
    "title": "Review:",
    "section": "Continued",
    "text": "Continued"
  },
  {
    "objectID": "lectures/1b.html#poisson-distribution-1",
    "href": "lectures/1b.html#poisson-distribution-1",
    "title": "Review:",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nThe poisson distribution describes an experiment that measures that occurrence of an event at specific point and/or time period.\n\n\\(P(X=x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}\\)"
  },
  {
    "objectID": "lectures/1b.html#expected-value-of-a-poisson-distribution",
    "href": "lectures/1b.html#expected-value-of-a-poisson-distribution",
    "title": "Review:",
    "section": "Expected Value of a Poisson Distribution",
    "text": "Expected Value of a Poisson Distribution"
  },
  {
    "objectID": "lectures/1b.html#continuous-random-variables-1",
    "href": "lectures/1b.html#continuous-random-variables-1",
    "title": "Review:",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\nA random variable \\(X\\) is considered continuous if the \\(P(X=x)\\) does not exist."
  },
  {
    "objectID": "lectures/1b.html#cdf-1",
    "href": "lectures/1b.html#cdf-1",
    "title": "Review:",
    "section": "CDF",
    "text": "CDF\nThe cumulative distribution function of \\(X\\) provides the \\(P(X\\leq x)\\), denoted by \\(F(x)\\), for the domain of \\(X\\).\nProperties of the CDF of \\(X\\):\n\n\\(F(-\\infty)\\equiv \\lim_{y\\rightarrow -\\infty}F(y)=0\\)\n\\(F(\\infty)\\equiv \\lim_{y\\rightarrow \\infty}F(y)=1\\)\n\\(F(x)\\) is a nondecreaseing function"
  },
  {
    "objectID": "lectures/1b.html#pdf",
    "href": "lectures/1b.html#pdf",
    "title": "Review:",
    "section": "PDF",
    "text": "PDF\nThe probability density function of the random variable \\(X\\) is given by\n\\[\nf(x)=\\frac{dF(x)}{d(x)}=F^\\prime(x)\n\\]\nwherever the derivative exists.\nProperties of pdfs:\n\n\\(f(x)\\geq 0\\)\n\\(\\int^\\infty_{-\\infty}f(x)dx=1\\)\n\\(P(a\\leq X\\leq b) = P(a&lt;X&lt;b)=\\int^b_af(x)dx\\)"
  },
  {
    "objectID": "lectures/1b.html#expected-value-1",
    "href": "lectures/1b.html#expected-value-1",
    "title": "Review:",
    "section": "Expected Value",
    "text": "Expected Value\nThe expected value for a continuous distribution is defined as\n\\[\nE(X)=\\int x f(x)dx\n\\]\nThe expectation of a function \\(g(X)\\) is defined as\n\\[\nE\\{g(X)\\}=\\int g(x)f(x)dx\n\\]"
  },
  {
    "objectID": "lectures/1b.html#expected-value-properties",
    "href": "lectures/1b.html#expected-value-properties",
    "title": "Review:",
    "section": "Expected Value Properties",
    "text": "Expected Value Properties\n\n\\(E(c)=c\\), where \\(c\\) is constant\n\\(E\\{cg(X)\\}=cE\\{g(X)\\}\\)\n\\(E\\{g_1(X)+g_2(X)+\\cdots+g_n(X)\\}=E\\{g_1(X)\\}+E\\{g_2(X)\\}+\\cdots+E\\{g_n(X)\\}\\)"
  },
  {
    "objectID": "lectures/1b.html#variance-1",
    "href": "lectures/1b.html#variance-1",
    "title": "Review:",
    "section": "Variance",
    "text": "Variance\nThe variance of continuous variable is defined as\n\\[\nVar(X) =  E[\\{X-E(X)\\}^2] = \\int \\{X-E(X)\\}^2 f(x)dx\n\\]"
  },
  {
    "objectID": "lectures/1b.html#uniform-distribution-1",
    "href": "lectures/1b.html#uniform-distribution-1",
    "title": "Review:",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nA random variable is said to follow uniform distribution if the density function is constant between two parameters.\n\n\\[\nf(x) = \\left\\{\\begin{array}{cc}\n\\frac{1}{b-a} & a \\leq x \\leq b\\\\\n0 & \\mathrm{elsewhere}\n\\end{array}\\right.\n\\]"
  },
  {
    "objectID": "lectures/1b.html#expected-value-2",
    "href": "lectures/1b.html#expected-value-2",
    "title": "Review:",
    "section": "Expected Value",
    "text": "Expected Value"
  },
  {
    "objectID": "lectures/1b.html#normal-distribution-1",
    "href": "lectures/1b.html#normal-distribution-1",
    "title": "Review:",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nA random variable is said to follow a normal distribution if the the frequency of occurrence follow a Gaussian function.\n\n\\[\nf(x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}\n\\]"
  },
  {
    "objectID": "lectures/1b.html#expected-value-3",
    "href": "lectures/1b.html#expected-value-3",
    "title": "Review:",
    "section": "Expected Value",
    "text": "Expected Value"
  },
  {
    "objectID": "lectures/1b.html#continued-1",
    "href": "lectures/1b.html#continued-1",
    "title": "Review:",
    "section": "Continued",
    "text": "Continued"
  },
  {
    "objectID": "hws/hw2.html",
    "href": "hws/hw2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Homework 2 is due 9/22/2024 at 11:59 PM. Submit your homework on Canvas as one PDF document.\n\nShow that the moment generating function for a random variable \\(X\\sim Gamma(\\alpha,\\beta)\\) is \\(\\left(1-\\beta t\\right)^{-\\alpha}\\).\nLet \\(X\\) be a discrete random variable with PMF\n\\[\nP(X=x)=\\left\\{\\begin{array}{cc}\n0.25+0.5e^{-\\theta} & x =0 \\\\\n0.25+0.5\\theta e^{-\\theta} & x =1 \\\\\n0.5\\frac{\\theta^x}{x!}e^{-\\theta} & x = 2,3,\\ldots\\\\\n0 & \\textrm{otherwise}\n\\end{array}\\right.\n\\] Show that the PMF is valid.\nLet \\(X\\) and \\(Y\\) be two random variables that have the following joint distribution function:\n\n\\[\nf(x,y)=\\left\\{\\begin{array}{cc}\ne^{-x/2}ye^{-y²} & x&gt;0;y&gt;0\\\\\n0 & \\mathrm{otherwise}\n\\end{array}\n\\right.\n\\]\nWhat is the marginal distribution of \\(Y\\)?\n\nFind the moment-generating function of \\(X\\sim\\chi^2_k\\) and \\(f_X(x)=\\frac{x^{k/2-1}\\exp\\{-x/2\\}}{2^{k/2}\\Gamma(k/2)}\\) with \\(x&gt;0\\)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Mathematical Statistics!",
    "section": "",
    "text": "Brief Introduction\n\n\n\n\n\nWelcome to the course! This is the home page of the course where I will provide a recap on what was covered in the week. Here I will post any documents or videos for your reference. If you have any questions, please email me at isaac.qs@csuci.edu.\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 4\n\n\n\n\n\nThis week, we will review functions of random variables and introduce the idea of sampling distributions and Central Limit Theorem. \n\n\n\n\n\nSep 12, 2024\n\n\n\n\n\n\n\nWeek 3\n\n\n\n\n\nThis week, we will cover Transformations of Random Variables \n\n\n\n\n\nSep 5, 2024\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\n\nThis week we will review basic concepts related to distribution functions and random variables \n\n\n\n\n\nAug 29, 2024\n\n\n\n\n\n\n\nWeek 1\n\n\n\n\n\nThis week is designed to be an introduction week. We will briefly discuss topics related statistics and inference. Then we will look at installing R and RStudio as well as the basics of using R. \n\n\n\n\n\nAug 22, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/week_3.html#resources",
    "href": "posts/week_3.html#resources",
    "title": "Week 3",
    "section": "Resources",
    "text": "Resources\nYou must log on to your CI Google account to access the video.\n\n\n\nLecture\nSlides\nVideos\n\n\n\n\nMonday\nSlides\nVideo\n\n\nWednesday\nSlides\nAudio Only"
  },
  {
    "objectID": "hw.html",
    "href": "hw.html",
    "title": "Homework",
    "section": "",
    "text": "Below are the different homework assignments for the course. Scan your homework as a PDF. You can use CamScanner, Dropbox, Google Drive, or icloud. Any assignments as images or not pdfs will not be graded.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 2\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 1\n\n\n\n\n\n\n\n\n\n\n\nAug 29, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "exc/exc3.html",
    "href": "exc/exc3.html",
    "title": "Extra Credit 3",
    "section": "",
    "text": "Create a website using either Quarto, RMarkdown, Blogdown, Jekyll, or any other website building tools that are not point-and-click based. Once you are finished creating your website, you must host it on GitHub Pages1. Below are the guidelines for the website:"
  },
  {
    "objectID": "exc/exc3.html#example-website",
    "href": "exc/exc3.html#example-website",
    "title": "Extra Credit 3",
    "section": "Example Website",
    "text": "Example Website\nMy Website Repo"
  },
  {
    "objectID": "exc/exc3.html#videos",
    "href": "exc/exc3.html#videos",
    "title": "Extra Credit 3",
    "section": "Videos",
    "text": "Videos\nRStudio Cloud"
  },
  {
    "objectID": "exc/exc3.html#GitHub",
    "href": "exc/exc3.html#GitHub",
    "title": "Extra Credit 3",
    "section": "Creating a GitHub Account (Required)",
    "text": "Creating a GitHub Account (Required)\nGitHub can be considered as the google drive of code. It has great features to track code, implement changes, and host websites. You can create a Github account here: https://github.com/.\nOnce you have your account create a new repository with the following naming scheme: USERNAME.github.io. Then, make sure that repository is public, and click to create a README document."
  },
  {
    "objectID": "exc/exc3.html#building-a-website-with-quarto",
    "href": "exc/exc3.html#building-a-website-with-quarto",
    "title": "Extra Credit 3",
    "section": "Building a Website with Quarto",
    "text": "Building a Website with Quarto\nI highly recommend building your website using quarto, it is very simple platform where your text documents become webpages.\n\nUsing RStudio Cloud\nRStudio Cloud (rstudio.cloud/) is an excellent resource to create website. It has all the software that you will need to create a website. Before we begin, make sure to create repository as stated by in the Creating a GitHub Account (Required) section.\n\nCreating an Account\nWhen you load the website, create a free account using either your email or your CSUCI gmail account. Once you create an account, the dashboard should load with your workspace.\n\n\nCreating a Project\nOn the top-right corner, click on the “New Project” blue button, and select “New RStudio Project”. This will create a new project called “Untitled Project”. Now change the name “Untitled Project” to “Website”. This project will create the material you need to develop a website.\n\n\nCreating and Connecting an SSH Key\nIn you website project, you will need a way for the project to connect with GitHub. You can do this by generating an SSH key in RStudio Cloud and transferring it to GitHub.\nThe SSH key can be thought of as a key that will unlock your computer to transfer data. You will generate the lock and key in RStudio, and then give the key to GitHub. Afterwards, transferring files will occur securely.\nTo generate an SSH key, go to the menu bar and select “Tools”, then “Global Options”. A menu will pop-up, select “Git/SVN” from the side menu. Click on the button “Create SSH key …”. A window will pop-up, then enter a password. Then click “Create”. Another window will pop-up showing your lock and key, you can close the window immediately.\nIn a different tab, go to your github account. On the top-right corner, click on your profile and select “Settings”. This will redirect you to a different page. In the left menu, click on “SSH and GPG Keys”. In the “SSH keys” section, click on the “New SSH key” green button. A new page will pop-up that will allow you to add a key. In the “Title” section, add any title2.\nBack at RStudio Cloud, click on the “View public key” link. Copy the highlighted text. Go back to Github and paste it in the “Key” section. Lastly, click on the “Add SSH Key” green button.\nNow you can transfer file easily between RStudio and GitHub.\n\n\nCloning a Repository\nCloning a repository is the process of downloading a repository from a remote server, in this case GitHub account. This will allow you to re-download your repository if it is ever deleted.\nIn GitHub, navigate to your repository. On the top-right hand corner, click on the green button labeled “Code”. Make sure the “SSH” tab is selected and copy, or click on the double squares button, the text they provide. You should copy something that look like this:\ngit@github.com:USERNAME/USERNAME.github.io.git \nIn your RStudio Cloud Project, select the terminal tab in your console pane, usually on the left-side of the IDE. Paste the following text in the terminal tab3:\ngit clone git@github.com:USERNAME/USERNAME.github.io.git\nIt will prompt you to accept the SSH connection, type “yes”. Afterwards, it will ask you for your SSH password. Type your password. Then, it will download your repository as a folder. In the “Files” tab, you should see the newly created folder.\n\n\nMaking a Quarto Website\nFor this section, we will primarily be working in the “Terminal” Tab in RStudio. Click on the “Terminal” Tab in the “Console” Pane. The you will type the following:\ncd USERNAME.github.io\nMake sure to replace USERNAME with your user name for GitHub. The above command will change the working folder to “USERNAME.github.io”. Afterwards, you want to fill the folder with the necessary contents for making a Quarto Website. Type the following command in the terminal:\nquarto create-project . --type website\nThis will fill generate new files for your website. Next, you will need to create a “nojekyll” file that tells GitHub, not to use Jekyll. Type the following in the terminal:\ntouch .nojekyll\nNow, go to the “Files” Tab and open “_quarto.yml”. At the top of the document, you will need to add output-dir: docs under the project:, you should have something like this:\nproject:\n  type: website\n  output-dir: docs\nNow you are set to Render your website!\n\n\nRendering and Publishing a Website\nWhenever you are finished updating your website files, you will need render your website so it will update all the new content. To begin, make sure you are working in the directory (folder) containing your website files:\ncd USERNAME.github.io\nNow, you can render your website using the following command:\nquarto render\nYour website will be rendered in the docs folder. Then you will need to commit the changes using git. First type\ngit add .\nThe commit your files using the following commands:\ngit commit -m \"Updates\"\nLastly, push your updates to GitHub:\ngit push\nIt will prompt your for your password related towards you SSH key. Then it will push all the updates. GitHub will then publish your website in a few minutes. Lastly, type “USERNAME.github.io” in any browser and your website should work.\n\n\n\nUsing RStudio on Mac/Linux\nComing Soon …\n\n\nUsing RStudio on Windows\nComing Soon …"
  },
  {
    "objectID": "exc/exc3.html#creating-webpages",
    "href": "exc/exc3.html#creating-webpages",
    "title": "Extra Credit 3",
    "section": "Creating Webpages",
    "text": "Creating Webpages\nTo create a webpage, you will simply open a new quarto document, edit the page, and save it in your website’s folder. To link the webpage in your menu bar, add the qmd file to the “_quarto.yml” file:\nwebsite:\n  title: \"inqs909.github.io\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - about.qmd\n      - NEWFILE.qmd\nThe render your website and push it to GitHub."
  },
  {
    "objectID": "exc/exc3.html#footnotes",
    "href": "exc/exc3.html#footnotes",
    "title": "Extra Credit 3",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGitHub Pages allow you to host websites for free if it is a Public Repository.↩︎\nI usually give different names of galaxies for a title.↩︎\nYou may need to use a special keyboard shortcut to paste text. For me, it is ctrl-shift-v. Right-clicking should work as well.↩︎"
  },
  {
    "objectID": "exc/exc2.html",
    "href": "exc/exc2.html",
    "title": "Extra Credit 2",
    "section": "",
    "text": "Provide a summary of one book below, an write a brief analysis in supporting or opposing the book, and connect key elements of the book to your every day life. Some of these books are available through the Broome Library. The professor my have the books that you can borrow.\nBooks:\n\nThe Book of Why\n\nJudea Pearl\n\nAlgorithms of Oppression\n\nSafiya Umoja Noble\n\nData Feminism\n\nCatherine D’Ignazio and Lauren Klein\n\nWeapons of Math Destruction\n\nCathy O’Niel\n\nInvisible women : data bias in a world designed for men\n\nCaroline Criado Perez\n\nFactfulness: Ten Reasons We’re Wrong About the World… and Why Things are Better Than You Think\n\nHans Rosling\n\nArtificial Unintelligence: How Computers Misunderstand the World\n\nMerideth Broussard\n\nTechnically Wrong: Sexist Apps, Biased Algorithms, and Other Threats of Toxic Tech\n\nSara Wachter-Boettcher\n\nAutomating Inequality\n\nVirgina Eubanks\n\nCloud Ethics\n\nLouise Amoore\n\nRace after Technology\n\nRuha Benjamin\n\nDigitizing Race\n\nLisa Nakamura\n\nUnmasking AI\n\nJoy Buolamwini\n\nHumble Pi\n\nMatt Parker\n\nMore than a Glitch\n\nMeridith Broussard\n\nData Action\n\nSarah Williams\n\n\nReport guidelines\n\n5-6 Pages\nMust include a title page\nDouble Spaced\n12 point font\nProofread your work\nSubmit a pdf of your work to Canvas\nDue 12/6/2024"
  },
  {
    "objectID": "r/r_lab_3.html",
    "href": "r/r_lab_3.html",
    "title": "Lab 3: Generalized Linear Models",
    "section": "",
    "text": "library(VGAM) # Multinomial and Ordinal Regression\n\nLoading required package: stats4\n\n\nLoading required package: splines\n\nlibrary(survival) # Data sets\nlibrary(MASS) # Data Set\nlibrary(GLMsData) # Data Set\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::select() masks MASS::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(haven) # Reading Stata File"
  },
  {
    "objectID": "r/r_lab_3.html#r-packages-used",
    "href": "r/r_lab_3.html#r-packages-used",
    "title": "Lab 3: Generalized Linear Models",
    "section": "",
    "text": "library(VGAM) # Multinomial and Ordinal Regression\n\nLoading required package: stats4\n\n\nLoading required package: splines\n\nlibrary(survival) # Data sets\nlibrary(MASS) # Data Set\nlibrary(GLMsData) # Data Set\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::select() masks MASS::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(haven) # Reading Stata File"
  },
  {
    "objectID": "r/r_lab_3.html#references",
    "href": "r/r_lab_3.html#references",
    "title": "Lab 3: Generalized Linear Models",
    "section": "References",
    "text": "References\nFor more information on this topic, I recommend you reading the following books:\n\nGeneralized Linear Models with Examples in R - Dunn\nVectorized Generalized Linear and Additive Models - Yee"
  },
  {
    "objectID": "r/r_lab_3.html#generalized-linear-models",
    "href": "r/r_lab_3.html#generalized-linear-models",
    "title": "Lab 3: Generalized Linear Models",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nIn linear regression, we are fitting a model between a set of predictors and an outcome variable. The outcome variable is generally normally distributed. However, if the outcome is not normally distributed, you must use a generalized linear model to assess the association between a set of predictors and an outcome variable.\n\nLink Functions\nDue to the outcome variable being non-normally distributed, we must use link functions. The link function allows us to link the outcome variable to a linear model.\n\n\nLogistic Regression\nA logistic regression model is used when the outcome is binary, yes and no. The link function is the logit function that models the association between a set of linear predictors and the odds of observing yes:\n\\[\n\\log\\left\\{\\frac{P(Y=1)}{P(Y=0)}\\right\\} = \\boldsymbol{X^T\\beta}\n\\] Fitting a logistic regression requires you to use the glm() function and specifying family=binomial. Then use the summary function to obtain the coefficients and broom::tidy()1\n\nlogit_glm &lt;- glm(y ~ ap + hilo, data = bacteria, \n                 family = binomial)\nsummary(logit_glm)\n\n\nCall:\nglm(formula = y ~ ap + hilo, family = binomial, data = bacteria)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.3539     0.2855   4.743 2.11e-06 ***\napp           0.7933     0.3748   2.116   0.0343 *  \nhilolo       -0.4816     0.3480  -1.384   0.1664    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 217.38  on 219  degrees of freedom\nResidual deviance: 209.87  on 217  degrees of freedom\nAIC: 215.87\n\nNumber of Fisher Scoring iterations: 4\n\n## OR ##\n\nlogit_glm %&gt;% broom::tidy(exp=T)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)    3.87      0.285      4.74 0.00000211\n2 app            2.21      0.375      2.12 0.0343    \n3 hilolo         0.618     0.348     -1.38 0.166     \n\n\nA \\(\\beta\\) coefficient is interpreted as an odds ratio. The odds of success when ap=p is 2.21 times higher than when ap=a, after adjusting for hilo.\n\n\nPoisson Regression\nPoisson regression is used when the outcome is count data. The link function is the log of the mean count:\n\\[\n\\log(\\mu)=\\boldsymbol{X^T\\beta}\n\\]\nFitting a Poisson regression requires you to use the glm() function and specifying family=poisson. Then use the summary function to obtain the coefficients and broom::tidy()2\n\npois_glm &lt;- glm(recur ~ treatment + number, data = bladder1, \n                family = poisson)\nsummary(pois_glm)\n\n\nCall:\nglm(formula = recur ~ treatment + number, family = poisson, data = bladder1)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          1.00918    0.06057  16.661  &lt; 2e-16 ***\ntreatmentpyridoxine  0.25506    0.06889   3.702 0.000214 ***\ntreatmentthiotepa   -0.45167    0.08626  -5.236 1.64e-07 ***\nnumber               0.11603    0.01620   7.164 7.82e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 868.47  on 293  degrees of freedom\nResidual deviance: 772.19  on 290  degrees of freedom\nAIC: 1529.5\n\nNumber of Fisher Scoring iterations: 5\n\npois_glm %&gt;% broom::tidy(exp=T)\n\n# A tibble: 4 × 5\n  term                estimate std.error statistic  p.value\n  &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)            2.74     0.0606     16.7  2.51e-62\n2 treatmentpyridoxine    1.29     0.0689      3.70 2.14e- 4\n3 treatmentthiotepa      0.637    0.0863     -5.24 1.64e- 7\n4 number                 1.12     0.0162      7.16 7.82e-13\n\n\nThe line pois_glm %&gt;% broom::tidy(exp=T) provides the exponentiated values of the \\(\\beta\\) coefficients. Interpreting the number variable, as the initial number of tumors increases, the mean number of recurrences increases by a factor of 1.12, adjusting for treatment status.\n\n\nMultinomial Regression\nMultinomial Regression is used when the outcome is categorical variable with 3 or more categories. We fit a model using a logit link function for each category and the reference variable. For \\(k=1,\\ldots,m\\) categories and reference level \\(2\\), the logit link function is modeled as:\n\\[\n\\log\\left\\{\\frac{P(Y=j)}{P(Y=2)}\\right\\}=\\boldsymbol{X^T\\beta}\\ \\ j\\in\\{1,3,4,5,\\ldots,m\\}\n\\]\nYou can fit a multinomial regression model when using the vglm() function and setting family = multinomial(refLevel=k), where k is your reference group. For more information of the data, visit this site: https://stats.oarc.ucla.edu/r/dae/multinomial-logistic-regression/.\n\nml &lt;- read_dta(\"https://stats.idre.ucla.edu/stat/data/hsbdemo.dta\")\nmglm &lt;- vglm(factor(prog) ~ factor(ses) + write, data = ml, family = multinomial(refLevel = \"2\"))\nsummary(mglm)\n\n\nCall:\nvglm(formula = factor(prog) ~ factor(ses) + write, family = multinomial(refLevel = \"2\"), \n    data = ml)\n\nCoefficients: \n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept):1   2.85219    1.16643   2.445  0.01448 *  \n(Intercept):2   5.21820    1.16351   4.485 7.30e-06 ***\nfactor(ses)2:1 -0.53329    0.44373  -1.202  0.22943    \nfactor(ses)2:2  0.29139    0.47637   0.612  0.54074    \nfactor(ses)3:1 -1.16283    0.51422  -2.261  0.02374 *  \nfactor(ses)3:2 -0.98267    0.59553  -1.650  0.09893 .  \nwrite:1        -0.05793    0.02141  -2.706  0.00682 ** \nwrite:2        -0.11360    0.02222  -5.113 3.17e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNames of linear predictors: log(mu[,1]/mu[,2]), log(mu[,3]/mu[,2])\n\nResidual deviance: 359.9635 on 392 degrees of freedom\n\nLog-likelihood: -179.9817 on 392 degrees of freedom\n\nNumber of Fisher scoring iterations: 4 \n\nNo Hauck-Donner effect found in any of the estimates\n\n\nReference group is level  2  of the response\n\nexp(coef(mglm))\n\n (Intercept):1  (Intercept):2 factor(ses)2:1 factor(ses)2:2 factor(ses)3:1 factor(ses)3:2        write:1        write:2 \n    17.3256181    184.6016219      0.5866710      1.3382906      0.3125996      0.3743102      0.9437175      0.8926126 \n\n\nThe line exp(coef(mglm)) provides the exponentiated \\beta coefficients for categorical variable. Interpreting write for group=1 (academic): as writing score increased by 1 unit, the odds of of being in the academic group decreases by a factor of 0.94, adjusting of social economic status.\n\n\nOrdinal Regression\nOrdinal regression is a subclass of multinomial regression, but the categories have an ordinal component to it. For example, a likert scale can be considered ordinal: Strongly Disagree to Strongly Agree. An ordinal regression model uses the logit link function for model the probability of observing something greater:\n\\[\n\\log\\left\\{\\frac{P(Y\\ge j)}{P(Y&lt;j)}\\right\\}=\\boldsymbol{X^T\\beta}\\ \\ j\\in\\{1,2,\\ldots,m\\}\n\\] You can fit a ordinal regression model when using the vglm() function and setting family = propodds.\n\ndat &lt;- read_dta(\"https://stats.idre.ucla.edu/stat/data/ologit.dta\")\noglm &lt;- vglm(apply~pared+public+gpa, data = dat, family = propodds)\nsummary(oglm)\n\n\nCall:\nvglm(formula = apply ~ pared + public + gpa, family = propodds, \n    data = dat)\n\nCoefficients: \n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept):1 -2.20335    0.78440  -2.809  0.00497 ** \n(Intercept):2 -4.29879    0.80915  -5.313 1.08e-07 ***\npared          1.04766    0.26845   3.903 9.52e-05 ***\npublic        -0.05867    0.28861  -0.203  0.83891    \ngpa            0.61575    0.26258   2.345  0.01903 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNames of linear predictors: logitlink(P[Y&gt;=2]), logitlink(P[Y&gt;=3])\n\nResidual deviance: 717.0249 on 795 degrees of freedom\n\nLog-likelihood: -358.5124 on 795 degrees of freedom\n\nNumber of Fisher scoring iterations: 4 \n\nNo Hauck-Donner effect found in any of the estimates\n\n\nExponentiated coefficients:\n    pared    public       gpa \n2.8509581 0.9430165 1.8510513 \n\nexp(coef(oglm))\n\n(Intercept):1 (Intercept):2         pared        public           gpa \n   0.11043293    0.01358498    2.85095814    0.94301649    1.85105132 \n\n\nIntepreting gpa, as gpa increases by 1 point, the odds of being likely to apply increases by a factor of 1.851, adjusting for parental undergraduate education and plubic vs private college."
  },
  {
    "objectID": "r/r_lab_3.html#questions",
    "href": "r/r_lab_3.html#questions",
    "title": "Lab 3: Generalized Linear Models",
    "section": "Questions",
    "text": "Questions\n\nWhat is the link function for a gamma distribution?\nWhen will you use gamma regression?\nThe lime data set contains 385 observations on small-leaved lime trees. The Foliage variable measures the foliage biomass. Fit a regression model between Foliage and the following covariates: DBH tree diameter and Age. Make sure to print out your results.\n\nSubmit your assignment as an html file. You can use either a QMD or RMD file to create the html"
  },
  {
    "objectID": "r/r_lab_3.html#footnotes",
    "href": "r/r_lab_3.html#footnotes",
    "title": "Lab 3: Generalized Linear Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe double colon tells R to use a specific function from a package. Here, use the tidy function from the broom package.↩︎\nThe double colon tells R to use a specific function from a package. Here, use the tidy function from the broom package.↩︎"
  },
  {
    "objectID": "r/r_lab_1.html",
    "href": "r/r_lab_1.html",
    "title": "Central Limit Theorem",
    "section": "",
    "text": "Let \\(X_1, X_2, \\ldots, X_n\\) be identical and independent distributed random variables with \\(E(X_i)=\\mu\\) and \\(Var(X_i) = \\sigma²\\). We define\n\\[\nY_n = \\sqrt n \\left(\\frac{\\bar X-\\mu}{\\sigma}\\right) \\mathrm{ where }\\ \\bar X = \\frac{1}{n}\\sum^n_{i=1}X_i.\n\\]\nThen, the distribution of the function \\(Y_n\\) converges to a standard normal distribution function as \\(n\\rightarrow \\infty\\). The central limit theorem can b reexpressed as:\n\\[\n\\bar X \\sim N(\\mu,\\sigma^2/n)\n\\]"
  },
  {
    "objectID": "r/r_lab_1.html#central-limit-theorem",
    "href": "r/r_lab_1.html#central-limit-theorem",
    "title": "Central Limit Theorem",
    "section": "",
    "text": "Let \\(X_1, X_2, \\ldots, X_n\\) be identical and independent distributed random variables with \\(E(X_i)=\\mu\\) and \\(Var(X_i) = \\sigma²\\). We define\n\\[\nY_n = \\sqrt n \\left(\\frac{\\bar X-\\mu}{\\sigma}\\right) \\mathrm{ where }\\ \\bar X = \\frac{1}{n}\\sum^n_{i=1}X_i.\n\\]\nThen, the distribution of the function \\(Y_n\\) converges to a standard normal distribution function as \\(n\\rightarrow \\infty\\). The central limit theorem can b reexpressed as:\n\\[\n\\bar X \\sim N(\\mu,\\sigma^2/n)\n\\]"
  },
  {
    "objectID": "r/r_lab_1.html#normal-assumption",
    "href": "r/r_lab_1.html#normal-assumption",
    "title": "Central Limit Theorem",
    "section": "Normal Assumption",
    "text": "Normal Assumption\nTo check whether the data follows a normal distribution, we can utilize a QQ-Plot. The main idea is that the points, or quantiles must follow \\(f(x)=x\\). To create a qq-plot in R, you will use qqnorm(). This will plot the points; additionally, you can use qqline() function to add \\(f(x)=x\\) to the plot1 The example below will generate 1000 random variables from a standard normal distribution. Then it will create the qq-plot for you to evaluate. You will need to run qqnorm and qqline the same time to get the plot.\n\nz_norm &lt;- rnorm(1000)\nqqnorm(z_norm)\nqqline(z_norm)\n\n\n\n\n\n\n\n\nThe closer all the points are to the line, the more proof you have that the sample came from a normal distribution."
  },
  {
    "objectID": "r/r_lab_1.html#normal-distribution",
    "href": "r/r_lab_1.html#normal-distribution",
    "title": "Central Limit Theorem",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nThe normal distribution has 2 finite moments. While the distribution of \\(\\bar X\\) will be normal regardless of sample size, the central limit theorem will still apply.\nThe code below provides a small simulation study where we will generate nreals random variables from standard normal distribution nsim times. Then we obtain the sample mean for each simulated data set and plot the histogram and QQ-plot.\n\nnsims &lt;- 1000\nnreals &lt;- 10000\nx &lt;- matrix(nrow = nreals, ncol = nsims)\nfor(i in 1:nsims){\n  x[,i]&lt;- rnorm(nreals)\n}\nxbar &lt;- colMeans(x)\nplot(density(xbar))\n\n\n\n\n\n\n\nqqnorm(xbar)\nqqline(xbar)"
  },
  {
    "objectID": "r/r_lab_1.html#cauchy-distribution",
    "href": "r/r_lab_1.html#cauchy-distribution",
    "title": "Central Limit Theorem",
    "section": "Cauchy Distribution",
    "text": "Cauchy Distribution\nThe Cauchy Distribution has and undefined mean and variance; therefore, the central limit theorem does not apply to it.\n\nnsims &lt;- 1000\nnreals &lt;- 10000\nx &lt;- matrix(nrow = nreals, ncol = nsims)\nfor(i in 1:nsims){\n  x[,i]&lt;- rcauchy(nreals)\n}\nxbar &lt;- colMeans(x)\nplot(density(xbar))\n\n\n\n\n\n\n\nqqnorm(xbar)\nqqline(xbar)"
  },
  {
    "objectID": "r/r_lab_1.html#problems",
    "href": "r/r_lab_1.html#problems",
    "title": "Central Limit Theorem",
    "section": "Problems",
    "text": "Problems\n\nProblem 1\nComment the code below describing the what each line is doing:\n\nnsims &lt;- 1000\nnreals &lt;- 10000\nx &lt;- matrix(nrow = nreals, ncol = nsims)\nfor(i in 1:nsims){\n  x[,i]&lt;- rnorm(nreals)\n}\nxbar &lt;- colMeans(x)\nplot(density(xbar))\n\n\n\n\n\n\n\nqqnorm(xbar)\nqqline(xbar)"
  },
  {
    "objectID": "r/r_lab_1.html#problem-2",
    "href": "r/r_lab_1.html#problem-2",
    "title": "Central Limit Theorem",
    "section": "Problem 2",
    "text": "Problem 2\nLet \\(X_1,\\ldots, X_n\\overset{iid}{\\sim}Bernoulli(0.4)\\). Does \\(\\bar X\\) follow a normal distribution when the sample size gets larger.\n\nProblem 3\nLet \\(X_1,\\ldots, X_n\\overset{iid}{\\sim}Gamma(3,4)\\). Does \\(\\bar X\\) follow a normal distribution when the sample size gets larger.\n\n\nProblem 3\nLet \\(X_1,\\ldots, X_n\\overset{iid}{\\sim}Beta(1,2)\\). Does \\(\\bar X\\) follow a normal distribution when the sample size gets larger.\nComplete the assignment and submit your code as a QMD file. Submit your file to Canvas on 11/2/2022 at 11:59 PM."
  },
  {
    "objectID": "r/r_lab_1.html#footnotes",
    "href": "r/r_lab_1.html#footnotes",
    "title": "Central Limit Theorem",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou will need to run both of these lines at the same time.↩︎"
  },
  {
    "objectID": "r/r_lab_0.html",
    "href": "r/r_lab_0.html",
    "title": "Introduction to R",
    "section": "",
    "text": "Below is a basic installation guide. Depending on your operating system, you may need to install other R packages1 to have R running smoothly.\n\nGo to the R Website and and go to the “Getting Started” section. This is usually the first thing you will see on the website.\nClick on the ‘download R’ link in this section. It will take you to a page called ‘CRAN Mirrors’.\nSelect the first mirror: https://cloud.r-project.org/\nGo to the ‘Download and Install R’ and choose your operating system.\n\nIf on Mac, download and install, choose from the ‘Latest release’2.\nIf on Windows, click on ‘install R for the first time’ link and click on the ‘Download R X.X.X for Windows’ link to install.\nIf on Linux, it is more complicated and you will need to be comfortable using a terminal. Watch tutorials on how to install on Linux.\n\n\n\n\n\nR is a language that tells a computer what to do. It is not exactly an application that you can click on and start conducting statistical analysis. For example, I can write up an R script in a text application, and then run it in a terminal to get the results. Therefore, R can be used in multiple ways on your computer.\nWhen you install R, there is an R GUI (Graphical User Interface) where you can write up R Code and analyze data. The R GUI is perfectly capable of doing everything you need to do. You don’t have to run anything in a terminal. However, it is limited in the features it provides and looks archaic. RStudio is an IDE (integrated development environment) that enhances your R experience. It provides a console (terminal), a script editor (how to write your R Code), environment window (tells you what you have created), and so much more. While you can do everything on the R GUI, I highly recommend installing RStudio! To install RStudio, go to its website www.rstudio.com\n\n\nInstalling RStudio is much simpler than installing R. However, you must install R before you can install RStudio. Here are some of the basic steps to install RStudio.\n\nGo to the website: https://www.rstudio.com/products/rstudio/download/#download\nScroll down to ‘All Installers’.\nClick on your operating system, download and install."
  },
  {
    "objectID": "r/r_lab_0.html#r",
    "href": "r/r_lab_0.html#r",
    "title": "Introduction to R",
    "section": "",
    "text": "Below is a basic installation guide. Depending on your operating system, you may need to install other R packages1 to have R running smoothly.\n\nGo to the R Website and and go to the “Getting Started” section. This is usually the first thing you will see on the website.\nClick on the ‘download R’ link in this section. It will take you to a page called ‘CRAN Mirrors’.\nSelect the first mirror: https://cloud.r-project.org/\nGo to the ‘Download and Install R’ and choose your operating system.\n\nIf on Mac, download and install, choose from the ‘Latest release’2.\nIf on Windows, click on ‘install R for the first time’ link and click on the ‘Download R X.X.X for Windows’ link to install.\nIf on Linux, it is more complicated and you will need to be comfortable using a terminal. Watch tutorials on how to install on Linux."
  },
  {
    "objectID": "r/r_lab_0.html#rstudio",
    "href": "r/r_lab_0.html#rstudio",
    "title": "Introduction to R",
    "section": "",
    "text": "R is a language that tells a computer what to do. It is not exactly an application that you can click on and start conducting statistical analysis. For example, I can write up an R script in a text application, and then run it in a terminal to get the results. Therefore, R can be used in multiple ways on your computer.\nWhen you install R, there is an R GUI (Graphical User Interface) where you can write up R Code and analyze data. The R GUI is perfectly capable of doing everything you need to do. You don’t have to run anything in a terminal. However, it is limited in the features it provides and looks archaic. RStudio is an IDE (integrated development environment) that enhances your R experience. It provides a console (terminal), a script editor (how to write your R Code), environment window (tells you what you have created), and so much more. While you can do everything on the R GUI, I highly recommend installing RStudio! To install RStudio, go to its website www.rstudio.com\n\n\nInstalling RStudio is much simpler than installing R. However, you must install R before you can install RStudio. Here are some of the basic steps to install RStudio.\n\nGo to the website: https://www.rstudio.com/products/rstudio/download/#download\nScroll down to ‘All Installers’.\nClick on your operating system, download and install."
  },
  {
    "objectID": "r/r_lab_0.html#introduction",
    "href": "r/r_lab_0.html#introduction",
    "title": "Introduction to R",
    "section": "Introduction",
    "text": "Introduction\nWhile most of your statistical analysis will be done with R functions, it is important to at least have an idea of what is going on. Additionally, we will cover other topics that you may or may not need to know. The topics we will cover are:\n\nBasic calculations in R\nTypes of Data\nR Objects"
  },
  {
    "objectID": "r/r_lab_0.html#basic-calculations",
    "href": "r/r_lab_0.html#basic-calculations",
    "title": "Introduction to R",
    "section": "Basic Calculations",
    "text": "Basic Calculations\nThis section focuses the basic calculation that you can do in R. Essentially, we look at how R can be used as a calculator. This is done by using different operators in R. An operator is a symbol that tells R to do something. Some common operators are +,-, and * which corresponds to addition, subtraction, and division.\n\nCalculator\n\nAddition\nTo add numbers in R, all you need to use the + operator. For example 2+2=4. When you type it in R you have:\n\n2+2\n\n[1] 4\n\n\nWhen you ask R to perform a task, it prints out the result of the task. As we can see above, R prints out the number 4.\nTo add more than 2 numbers, you can simply just type it in.\n\n2+2+2\n\n[1] 6\n\n\nThis provides the number 6.\n\n\nSubtraction\nTo subtract numbers, you need to use the - operator. Try 4-2:\n\n4-2\n\n[1] 2\n\n\nTry 4-6-4\n\n4-6-4\n\n[1] -6\n\n\nNotice that you get a negative number.\nNow try 4+4-2+8:\n\n4+4-2+8\n\n[1] 14\n\n\n\n\nMultiplication\nTo multiply numbers, you will need to use the * operator. Try 4*4:\n\n4*4\n\n[1] 16\n\n\n\n\nDivision\nTo divide numbers, you can use the / operator. Try 9/3:\n\n9/3\n\n[1] 3\n\n\n\n\nExponents\nTo exponentiate a number to the power of another number, you can use the ^ operator. Try 2^5:\n\n2^5\n\n[1] 32\n\n\nIf you want to take e to the power 2, you will use the exp() function. Try exp(2):\n\nexp(2)\n\n[1] 7.389056\n\n\n\n\nRoots\nTo take the n-th root of a value, use the ^ operator with the / operator to take the n-th root. For example, to take the 5th-root of 32, type 32^(1/5):\n\n32^(1/5)\n\n[1] 2\n\n\n\n\nLogarithms\nTo take the natural logarithm of a value, you will use the log() function. Try log(5):\n\nlog(5)\n\n[1] 1.609438\n\n\nIf you want to take the logarithm of a different base, you will use the log() function with base argument. We will discuss this more in section 7 of this chapter.\n\n\n\nComparing Numbers\nAnother important part of R is comparing numbers. When you compare two numbers, R will tell you if that is true or false. We will talk about some of the basic comparisons and their operators.\n\nLess than/Greater than\nTo check if one number is less than or greater than another number, you will use the &gt; or &lt; operators. Try 5&gt;4:\n\n5&gt;4\n\n[1] TRUE\n\n\nNotice that R states it’s true. It evaluates the expression and tells you if it’s true or not. Try 5&lt;4:\n\n5&lt;4\n\n[1] FALSE\n\n\nNotice that R tells you it is false.\n\n\nLess than or equal to/Greater than or equal to\nTo check if one number is less than or equal to/greater than or equal to another number, you will use the &gt;= or &lt;= operators. Try 5&gt;=5:\n\n5&gt;=5\n\n[1] TRUE\n\n\nTry 5&gt;=4:\n\n5&gt;=4\n\n[1] TRUE\n\n\nTry 5&lt;=4\n\n5&lt;=4\n\n[1] FALSE\n\n\n\n\nEquals and Not Equals\nTo check if 2 numbers are equal to each other, you can use the == operator. Try 3==3:\n\n3==3\n\n[1] TRUE\n\n\nTry 4==3\n\n3==4\n\n[1] FALSE\n\n\nAnother way to see if 2 numbers are not equal to each other, you can use the !=. Try 3!=4:\n\n3!=4\n\n[1] TRUE\n\n\nTry 3!=3:\n\n3!=3\n\n[1] FALSE\n\n\nYou may be asking why use != instead of ==. They both provides similar results. Well the reason is that you may need the ‘TRUE’ output for analysis. One is only true when they are equal, while the other is true when they are not equal.\n\n\n\nHelp\nThe last operator we will discuss is the help operator ?. If you want to know more about anything we talked about you can type ? in front of a functiona and a help page will pop-up in your browser or in RStudio’s ‘Help’ tab. For example you can type ?Arithmetic or ?Comparison, to review what we talked about. For other operators we didn’t talk about use ?assignOps and ?Logic."
  },
  {
    "objectID": "r/r_lab_0.html#types-of-data",
    "href": "r/r_lab_0.html#types-of-data",
    "title": "Introduction to R",
    "section": "Types of Data",
    "text": "Types of Data\nIn R, the type of data, also known as class, that we are using dictates how the programming works. For the most part, users will use ‘numeric’,‘logical’, ‘POSIX’ and ‘character’ data types. Other types of data you may encounter are ‘integer’, ‘complex’, and ‘raw’. These types of data are rarely used. To obtain more information on them, use the ? operator.\n\nNumeric\nThe numeric class is the data that are numbers. Almost every analysis that you use will be based on the numeric class. To check if you have a numeric class, you just need to use the is.numeric() function. For example, try is.numeric(5):\n\nis.numeric(5)\n\n[1] TRUE\n\n\nNotice that when you input an number into R, it automatically changes it to a numeric class. R is changes data to the class that it most likely needs to be. Now this is great because you do not need to do anything on your end. Howerver, if you need a different class, you will need to change it.\n\n\nLogical\nA logical class are data where the only value is ‘TRUE’ or ‘FALSE’. Sometimes the data is coded as 1 for ‘TRUE’ and 0 for ‘FALSE’. The data may also be coded as ‘T’ or ‘F’. To check if data belongs in the logical class, you will need the is.logical() function. Try is.logical(3&lt;4):\n\nis.logical(3&lt;4)\n\n[1] TRUE\n\n\nRemember when we ran 3&lt;4 in the previous section. The output was ‘TRUE’. Now R is checking whether the output is of a logical class. Since it it, R returns ‘TRUE’. Now try is.logical(3&gt;4):\n\nis.logical(3&gt;4)\n\n[1] TRUE\n\n\nThe output is ‘TRUE’ as well even though the condition 3&gt;4 is ‘FALSE’. Since the output is a logical data type, it is a logical variable.\n\n\nPOSIX\nThe POSIX class are date-time data. Where the data value is a time component. The POSIX class can be very complex in how it is formatted. IF you would like to learn more try ?POSIXct or ?POSIClt. First, lets run Sys.time() to check what is today’s data and time:\n\nSys.time()\n\n[1] \"2024-09-17 16:47:44 PDT\"\n\n\nNow lets check if its of POSIX class, you can use the class() function to figure out which class is it. Try class(Sys.time()):\n\nclass(Sys.time())\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\n\n\nCharacter\nA character value is where the data values follow a string format. Examples of characters values are letters, words and even numbers. A character value is any value surrounded by quotation marks. For example, the phrase “Hello World!” is considere as one character value. Another example if you data is coded with the actual words “yes” or “no”. To check if you have character data, use the is.character() function. Try is.character(\"Hello World!\"):\n\nis.character(\"Hello World!\")\n\n[1] TRUE\n\n\nNotice that the output says ‘TRUE’. Character values can be created with single quotations. Try is.character('Hello World!'):\n\nis.character('Hello World!')\n\n[1] TRUE\n\n\n\n\nIntegers\nIntegers are just whole numbers for the most part. To create an interger, type the letter ‘L’ after a number. To check if you are using integer data, use the is.integer() function. Try is.integer(5L):\n\nis.integer(5L)\n\n[1] TRUE\n\n\n\n\nComplex Numbers\nComplex numbers are data values where there is a real component and an imaginary component. The imaginary component is a number multiplied by \\(i=\\sqrt{-1}\\). To create a complect number, use the complex() function. To check if a number is complex, use the is.complex() function. Try the following to create a complex number complex(1,4,5):\n\ncomplex(1,4,5)\n\n[1] 4+5i\n\n\nNow try is.complex(complex(1,4,5)):\n\nis.complex(complex(1,4,5))\n\n[1] TRUE\n\n\n\n\nRaw\nYou will probably never use raw data. I have never used raw data in R. To create a raw value, use the raw() or charToRaw() functions. Try charToRaw('Hello World!'):\n\ncharToRaw('Hello World!')\n\n [1] 48 65 6c 6c 6f 20 57 6f 72 6c 64 21\n\n\nTo check if you have raw data, use the is.raw() function. Try is.raw(charToRaw('Hello World!')):\n\nis.raw(charToRaw('Hello World!'))\n\n[1] TRUE\n\n\n\n\nMissing\nThe last data class in R is missing data denoted as NA. Whenever you see NA in any of the analysis you see, it means that the data is missing. To check if you have missing data, use the is.na() function. Try is.na(NA):\n\nis.na(NA)\n\n[1] TRUE"
  },
  {
    "objectID": "r/r_lab_0.html#r-objects",
    "href": "r/r_lab_0.html#r-objects",
    "title": "Introduction to R",
    "section": "R Objects",
    "text": "R Objects\nR objects are where most of the statistical analysis is conducted on. An R object can be thought of as a container of data. For the most part, you will only use a data frame (or tibble) for your data analysis. However, it is always a good idea to to have some basic understanding of the other R objects.\n\nAssigning objects\nTo create an R object, all we need to do is assign data to a variable. The variable is the name of the R object. it can be called anything, but you can only use alphanumeric values, underscore, and periods. To assign a value to a variable, use the &lt;- operator. This is known a left assignment. Kinda like an arrow pointing left. Try assigning 9 to ‘x’ (x&lt;-9)`:\n\nx&lt;-9\n\nTo see if x contains 9, type x in the console:\n\nx\n\n[1] 9\n\n\nNow x can be treated as data and we can perform data analysis on it. For example, try squaring it:\n\nx^2\n\n[1] 81\n\n\nYou can use any mathematical operation from the previous sections. Try some other operations and see what happens.\nThe output R prints out can be stored in a variable using the asign operator, &lt;-. Try storing x^3 in a variable called x_cubed:\n\nx_cubed&lt;-x^3\n\nTo see what is stored in x_cubed you can either type x_cubed in the console or use the print() function with ‘x_cubed’ inside the paranthesis.\n\nx_cubed\n\n[1] 729\n\nprint(x_cubed)\n\n[1] 729\n\n\n\n\nVectors\nA vector is a set data values of a certain leng. The R object x is considered as a numerical vector (because it contains a number) with the length 1. To check, try is.numeric(x) and is.vector(x):\n\nis.numeric(x)\n\n[1] TRUE\n\nis.vector(x)\n\n[1] TRUE\n\n\nNow let’s create a logical vector that contains 4 elements (have it follow this sequence: T,F,T,F) and assign it to y. To create a vector use the c() function and type all the values and seperating it with columns. Type y&lt;-c(T,F,T,F):\n\ny&lt;-c(T,F,T,F)\n\nNow, lets see how y looks like. Type y:\n\ny\n\n[1]  TRUE FALSE  TRUE FALSE\n\n\nNow lets see if it’s a logical vector:\n\nis.logical(y)\n\n[1] TRUE\n\nis.vector(y)\n\n[1] TRUE\n\n\nFortunately, this vector is really small to count how many elements it has, but what if the vector is really large? To find out how many elements a vector has, use the length() function. Try length(y):\n\nlength(y)\n\n[1] 4\n\n\nThe c() function allows you to put any data type and as many values as you wish. The only condition of a vector is that it must be the same data type.\n\n\nMatrices\nA matrix can be thought as a square or rectangular grid of data values. This grid can be constructed in any shape. Similar to vectors they must contain the same data type. The size of a matrix is usually denoted as \\(n\\times k\\), where \\(n\\) represents the number of rows and \\(k\\) represents the number of columns. To get a rough idea of how a matrix may look like, type matrix(rep(1,12),nrow=4,ncol=3)3:\n\nmatrix(rep(1,12),nrow=4,ncol=3)\n\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    1    1    1\n[3,]    1    1    1\n[4,]    1    1    1\n\n\nNotice that this is a \\(4\\times 3\\) matrix. Each element in the matrix has the value 1. Now try this matrix(rbinom(12,1.5),nrow=4,ncol=3)4:\n\nmatrix(rbinom(12,1,.5),nrow=4,ncol=3)\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    1    0    0\n[3,]    1    1    1\n[4,]    1    0    1\n\n\nYour matrix may look different, but that is to be expected. Notice that some elements in a matrix are 0’s and some are 1’s. Each element in a matrix can hold any value.\nConstructing a matrix can be a bit difficult to do because the data values may need to be arranged in a certain way. Notice that I used the matrix() function to create the matrix. The examples above contain other components in the function that we will discuss later.\n\n\nArrays\nMatrices can be considered as a 2-dimensional block of numbers. An array is an n-dimensional block of numbers. While you may never need to use an array for data analysis. It may come in handy when programming by hand. To create an array, use the array() function. Below is an example of a \\(3 \\times 3 \\times 3\\) with the numbers 1, 2, and 3 representing the 3rd dimension stored in an R object called first_array5.\n\n(first_array &lt;- array(c(rep(1,9),rep(2,9),rep(3,9)),dim=c(3,3,3)))\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    1    1    1\n[3,]    1    1    1\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    2    2    2\n[2,]    2    2    2\n[3,]    2    2    2\n\n, , 3\n\n     [,1] [,2] [,3]\n[1,]    3    3    3\n[2,]    3    3    3\n[3,]    3    3    3\n\n\n\n\nData Frames\nData frames can be thought as the data sets that we normally see in other softwares. You can think about it as an excel spreadsheet. However, you cannot not change the values easily other than coding the changes. In a much general sense, a data frame is just a collection of labeled vectors. To get an idea of what a data frame looks like, try head(iris):\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nThe head() function just tells R to only print the top few components of the data frame.\nNow try tail(iris):\n\ntail(iris)\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n145          6.7         3.3          5.7         2.5 virginica\n146          6.7         3.0          5.2         2.3 virginica\n147          6.3         2.5          5.0         1.9 virginica\n148          6.5         3.0          5.2         2.0 virginica\n149          6.2         3.4          5.4         2.3 virginica\n150          5.9         3.0          5.1         1.8 virginica\n\n\nThe tail() function provides the last 6 rows of the data frame.\n\n\nLists\nTo me a list is just a container that you can store practically anything. It is compiled of elements, where each element contains an R object. For example, the first element of a list may contain a data frame, the second element may contain a vector, and the third element may contain another list. It is just a way to store things.\nTo create a list, use the list() function. Create a list compiled of first element with the mtcars data set, second element with a vector of zeros of size 4, and a matrix \\(3 \\times 3\\) identity matrix6. Store the list in an object called list_one:\n\nlist_one&lt;-list(mtcars,rep(0,4),diag(rep(1,3)))\n\nType list_one to see what pops out:\n\nlist_one\n\n[[1]]\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n[[2]]\n[1] 0 0 0 0\n\n[[3]]\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n\nEach element in the list is labeled as a number. It is more useful to have the elements named. An element is named by typing the name in quotes followed by the = symbol before your object in the list() function (mtcars=mtcars).\n\nlist_one&lt;-list(mtcars=mtcars,vector=rep(0,4),identity=diag(rep(1,3)))\n\nHere I am creating an object called list_one, where the first element is mtcars labeled mtcars, the second element is a vector of zeros labeled vector and the last element is the identity matrix labeled identity.’\nNow create a new list called list_two and store list_one labeled as list_one and first_array labeled as array.\n\n(list_two&lt;-list(list_one=list_one,array=first_array))\n\n$list_one\n$list_one$mtcars\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n$list_one$vector\n[1] 0 0 0 0\n\n$list_one$identity\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n\n$array\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    1    1    1\n[3,]    1    1    1\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    2    2    2\n[2,]    2    2    2\n[3,]    2    2    2\n\n, , 3\n\n     [,1] [,2] [,3]\n[1,]    3    3    3\n[2,]    3    3    3\n[3,]    3    3    3"
  },
  {
    "objectID": "r/r_lab_0.html#footnotes",
    "href": "r/r_lab_0.html#footnotes",
    "title": "Introduction to R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nR packages can be thought as more software that increases the capabilities of R↩︎\nThere may be challenges using R and RStudio on a Mac. Follow an online tutorial to properly install these packages. In the past, many of my students had trouble installing R packages.↩︎\nThe function rep() creates a vector by repeating a value for a certain length. rep(1,12) creates a vector of length 12 with each element being 1↩︎\nThe rbinom() function generates binomial random variables and stores them in a vector. rbinom(12,1.5) This creates 12 random binomial numbers with parameter \\(n=1\\) and \\(p=0.5\\).↩︎\nNotice the code is surrounded by parenthesis. This tells R to store the array and print out the results. You can surround code with parenthesis evertime you create an object to also print what is stored.↩︎\nAn identity matrix is a matrix where the diagonal elements are 1 and the non-diagonal elements are 0↩︎"
  },
  {
    "objectID": "r/r_lab_2.html",
    "href": "r/r_lab_2.html",
    "title": "Lab 2: Optimization",
    "section": "",
    "text": "In class, we learned how to find estimates for the parameters of interest of commonly used distribution functions. We find these parameters by maximizing the Likelihood function with respect to parameter. However, when we are working with distributions where closed-form solutions do not exist, then we need to rely with numerical techniques to maximize the likelihood function. This leads to optimization techniques.\nOptimization is a set of algorithms that can be used to identify a set of parameters that maximize a function. In R, we will use the optim function to minimize a function. Therefore, we will minimize the negative log-likelihood function."
  },
  {
    "objectID": "r/r_lab_2.html#optimization-and-maximum-likelihood-estimators",
    "href": "r/r_lab_2.html#optimization-and-maximum-likelihood-estimators",
    "title": "Lab 2: Optimization",
    "section": "",
    "text": "In class, we learned how to find estimates for the parameters of interest of commonly used distribution functions. We find these parameters by maximizing the Likelihood function with respect to parameter. However, when we are working with distributions where closed-form solutions do not exist, then we need to rely with numerical techniques to maximize the likelihood function. This leads to optimization techniques.\nOptimization is a set of algorithms that can be used to identify a set of parameters that maximize a function. In R, we will use the optim function to minimize a function. Therefore, we will minimize the negative log-likelihood function."
  },
  {
    "objectID": "r/r_lab_2.html#normal-distribution",
    "href": "r/r_lab_2.html#normal-distribution",
    "title": "Lab 2: Optimization",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nTo begin we will simulate 250 random variables from \\(N(2,1.5)\\).\n\nx &lt;- rnorm(250, 2, sqrt(1.5))\n\nThen, we construct a function that will provide the negative log-likelihood function. We will create a function using function and set two arguments: data to specify a vector data, and params as a vector for \\(\\mu\\) and \\(\\sigma²\\). To obtain the log-likelihood values, we will use the dnorm function and set log=T. Then, we sill add up all the values and return the negative values. The code below is used to construct the function:\n\nll_norm &lt;- function(data, params){\n  ll &lt;- sum(dnorm(data, params[1], sqrt(params[2]), log = T))\n  return(-ll)\n}\n\nTo minimize the function, we will optim and provide the initial values for params [c(0,1)], then the function ll_norm, and data=x. R will minimize the function and provide output to interpret.\n\noptim(c(0,1), ll_norm, data = x)\n\n$par\n[1] 2.038788 1.434028\n\n$value\n[1] 399.8515\n\n$counts\nfunction gradient \n      85       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nThe par element provides the values for the parameters. The first value is for \\(\\mu\\) and the second value is for \\(\\sigma²\\). Notice how close it is from the true values. Notice that it is slightly off to \\(\\bar x\\) and \\((n-1)s²/n\\)."
  },
  {
    "objectID": "r/r_lab_2.html#uniform-distribution",
    "href": "r/r_lab_2.html#uniform-distribution",
    "title": "Lab 2: Optimization",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nSimulate 250 random variables from \\(U(0,3)\\).\n\nx &lt;- runif(250, max = 3)\n\nLet’s assume that we know the lower bound 0 and was interested in estimating the upper bound from the data. We will construct an R function that provides the negative log-likelihood function. We will follow similar steps to the normal distribution:\n\nll_unif &lt;- function(data, params){\n  ll &lt;- sum(dunif(data, 0, params, log = T))\n  return(-ll)\n}\n\nNow we will use optim to minimize the function. We will use the maximum of x for the initial value of the upper bound:\n\noptim(max(x), ll_unif, data = x)\n\nWarning in optim(max(x), ll_unif, data = x): one-dimensional optimization by Nelder-Mead is unreliable:\nuse \"Brent\" or optimize() directly\n\n\n$par\n[1] 2.999513\n\n$value\n[1] 274.6125\n\n$counts\nfunction gradient \n      48       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nIs this value close the maximum of the data?"
  },
  {
    "objectID": "r/r_lab_2.html#questions",
    "href": "r/r_lab_2.html#questions",
    "title": "Lab 2: Optimization",
    "section": "Questions",
    "text": "Questions\nConstruct the R code to find the maximum likelihood estimators.\n\nFind the MLE for the following data:\n\nx &lt;- rpois(250, 4)\n\nFind the MLE for the following data:\n\nx &lt;- rexp(250, 1.5)\n\nFind the MLE for the following data:\n\nx &lt;- rgamma(250, shape = 2, rate = 2)\n\nFind the MLE for the following data:\n\nx &lt;- rbeta(250, shape1 = 2, shape2 = 3)\n\n\nComplete the assignment and submit your code as a QMD file. Submit your file to Canvas on 11/3/2022 at 11:59 PM."
  },
  {
    "objectID": "exc/exc1.html",
    "href": "exc/exc1.html",
    "title": "Extra Credit 1",
    "section": "",
    "text": "Write a 1-2 page report related to your favorite activity. Provide details of the activity and why you like the activity.\nReport guidelines\n\n1-2 Pages\nDouble Spaced\n12 point font\nDue 9/14/2024"
  },
  {
    "objectID": "exc/exc4.html",
    "href": "exc/exc4.html",
    "title": "Extra Credit 4",
    "section": "",
    "text": "Write a report on one of the topics below. Provide a brief history and their purpose. Lastly, provide a toy example demonstrating the use of the topic in R.\n\nMonte Carlo Methods\nMonte Carlo Methods are a class of computational algorithms that rely on random sampling to obtain numerical results. These methods use statistical sampling techniques to approximate complex mathematical problems, particularly those with deterministic or probabilistic aspects. The name “Monte Carlo” is derived from the Monte Carlo Casino in Monaco, known for its games of chance and randomness.\nIn Monte Carlo Methods, random samples are generated to simulate the behavior of a system or process, and the results are analyzed to estimate desired quantities or solve problems. These methods find applications in various fields, such as physics, finance, engineering, and statistics. Monte Carlo simulations are particularly useful for solving problems with a large number of variables or complex interactions, where analytical solutions may be challenging or impossible to obtain.\n\nResources\n\nhttps://towardsdatascience.com/an-overview-of-monte-carlo-methods-675384eb1694\nhttps://towardsdatascience.com/monte-carlo-simulation-a-practical-guide-85da45597f0e\n\n\n\n\nSurvival Analysis\nSurvival analysis is a statistical method used to analyze the time until an event of interest occurs. This type of analysis is commonly employed in medical research, epidemiology, and other fields to study the duration until a specific event, often referred to as a “failure” or “survival” event. The event could be anything from the onset of a disease, death, relapse, or any other occurrence of interest.\nSurvival analysis is conducted using various statistical models, with the Cox proportional hazards model being one of the most widely used. These analyses help researchers understand factors influencing the time to an event, identify risk factors, and estimate survival probabilities over time.\nMake sure to provide a brief history of survival analysis and prominent methods such as Kaplan-Meier curves and Cox proportional hazard models.\n\nResources\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC2394262/\nhttps://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_survival/BS704_Survival_print.html\n\n\n\n\nBayesian Analysis\nBayesian analysis is a statistical approach that involves updating probabilities for hypotheses based on new evidence or data. It is rooted in Bayes’ theorem, which describes how beliefs about the probability of a hypothesis should change in light of new information. In Bayesian analysis, the prior probability (initial belief) is combined with the likelihood of observing the data given the hypothesis and results in the posterior probability (updated belief).\nBayesian methods are particularly valuable in situations with limited data or when incorporating prior knowledge is crucial. These methods provide a flexible framework for modeling uncertainty and updating beliefs as more information becomes available. Bayesian analysis is applied across various fields, including statistics, machine learning, physics, biology, and finance. Markov Chain Monte Carlo (MCMC) methods are often used to simulate samples from the posterior distribution in complex Bayesian models.\n\nResources\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6406060/\nhttps://www.statlect.com/fundamentals-of-statistics/Bayesian-inference\n\n\n\n\nCausal Inference\nCausal inference is a field within statistics and epidemiology that focuses on understanding and estimating the causal relationships between variables or events. The goal is to determine whether a particular factor or intervention has a causal impact on an outcome. Causal inference involves identifying and controlling for confounding factors, which are variables that may influence both the cause and the effect, leading to potential bias in estimating causal relationships.\nCausal inference is essential for making informed decisions in fields such as medicine, public health, economics, and social sciences. It helps researchers draw valid conclusions about the effectiveness of interventions, policies, or treatments by accounting for potential sources of bias and confounding. Advances in causal inference methodologies contribute to a more rigorous understanding of cause-and-effect relationships in complex systems.\n\nResources\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC2836213/\nhttps://web.stanford.edu/~swager/stats361.pdf\n\n\n\n\nReport Guideline\n\n5-6 Pages\nMust include a title page\nDouble Spaced\n12 point font\nProofread your work\nSubmit a pdf of your work to Canvas\nDue 12/6/2024"
  },
  {
    "objectID": "ec.html",
    "href": "ec.html",
    "title": "Extra Credit",
    "section": "",
    "text": "Extra Credit is designed to expand on different topics that related to Probability and Statistics, but are not necessarily required for the course. Additionally, these opportunities provide students relief when unexpected situations occur during the semester. While it is not required, I encourage everyone to attempt each opportunity.\nBelow is more information on each assignment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtra Credit 1\n\n\n\n\n\nInstructions for extra credit one.\n\n\n\n\n\nAug 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nExtra Credit 2\n\n\n\n\n\nInstructions for extra credit two.\n\n\n\n\n\nAug 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nExtra Credit 3\n\n\n\n\n\nInstructions for extra credit three.\n\n\n\n\n\nAug 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nExtra Credit 4\n\n\n\n\n\nInstructions for extra credit four.\n\n\n\n\n\nAug 20, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/week_4.html",
    "href": "posts/week_4.html",
    "title": "Week 4",
    "section": "",
    "text": "Lecture\nSlides\nVideos\n\n\n\n\nMonday\nSlides\nVideo\n\n\nWednesday\nSlides"
  },
  {
    "objectID": "posts/week_4.html#learning-outcomes",
    "href": "posts/week_4.html#learning-outcomes",
    "title": "Week 4",
    "section": "",
    "text": "Lecture\nSlides\nVideos\n\n\n\n\nMonday\nSlides\nVideo\n\n\nWednesday\nSlides"
  },
  {
    "objectID": "posts/week_2.html",
    "href": "posts/week_2.html",
    "title": "Week 2",
    "section": "",
    "text": "Define Moment Generating Functions\nDiscuss Properties"
  },
  {
    "objectID": "posts/week_2.html#learning-outcomes",
    "href": "posts/week_2.html#learning-outcomes",
    "title": "Week 2",
    "section": "",
    "text": "Define Moment Generating Functions\nDiscuss Properties"
  },
  {
    "objectID": "posts/week_2.html#important-concepts",
    "href": "posts/week_2.html#important-concepts",
    "title": "Week 2",
    "section": "Important Concepts",
    "text": "Important Concepts\n\nMonday\n\nDiscrete Variables\nA random variable is considered to be discrete if it can only map to a finite or countably infinite number of distinct values.\n\nPMF\nThe probability mass function of discrete variable can be represented by a formula, table, or a graph. The Probability of a random variable Y can be expressed as \\(P(Y=y)\\) for all values of \\(y\\).\n\n\nCDF\nThe cumulative distribution function provides the \\(P(Y\\leq y)\\) for a random variable \\(Y\\).\n\n\nExpected Value\nThe expected value is the value we expect when we randomly sample from population that follows a specific distribution. The expected value of Y is\n\\[\nE(Y)=\\sum_y yP(y)\n\\]\n\n\n\nContinuous Variables\nA random variable \\(X\\) is considered continuous if the \\(P(X=x)\\) does not exist.\n\nCDF\nThe cumulative distribution function of \\(X\\) provides the \\(P(X\\leq x)\\), denoted by \\(F(x)\\), for the domain of \\(X\\).\nProperties of the CDF of \\(X\\):\n\n\\(F(-\\infty)\\equiv \\lim_{y\\rightarrow -\\infty}F(y)=0\\)\n\\(F(\\infty)\\equiv \\lim_{y\\rightarrow \\infty}F(y)=1\\)\n\\(F(x)\\) is a nondecreaseing function\n\n\n\nPDF\nThe probability density function of the random variable \\(X\\) is given by\n\\[\nf(x)=\\frac{dF(x)}{d(x)}=F^\\prime(x)\n\\]\nwherever the derivative exists.\nProperties of pdfs:\n\n\\(f(x)\\geq 0\\)\n\\(\\int^\\infty_{-\\infty}f(x)dx=1\\)\n\\(P(a\\leq X\\leq b) = P(a&lt;X&lt;b)=\\int^b_af(x)dx\\)\n\n\n\nExpected Value\nThe expected value for a continuous distribution is defined as\n\\[\nE(X)=\\int x f(x)dx\n\\]\nThe expectation of a function \\(g(X)\\) is defined as\n\\[\nE\\{g(X)\\}=\\int g(x)f(x)dx\n\\] Special properties of the expected value:\n\n\\(E(c)=c\\), where \\(c\\) is constant\n\\(E\\{cg(X)\\}=cE\\{g(X)\\}\\)\n\\(E\\{g_1(X)+g_2(X)+\\cdots+g_n(X)\\}=E\\{g_1(X)\\}+E\\{g_2(X)\\}+\\cdots+E\\{g_n(X)\\}\\)\n\n\n\n\n\nWednesday\n\nMoments\nThe \\(k\\)th moment is defined as the expectation of the random variable, raised to the \\(k\\)th power, defined as \\(E(X^k)\\).\n\n\nMoment Generating Functions\nThe moment generating functions is used to obtain the \\(k\\)th moment. The mgf is defined as\n\\[\nm(c) = E(e^{tX})\n\\]\nThe \\(k\\)th moment can be obtained by taking the \\(k\\)th derivative of the mgf, with respect to \\(c\\), and setting \\(c\\) equal to 0:\n\\[\nE(X^k)=\\frac{d^km(c)}{dc}\\Bigg|_{c=0}\n\\]"
  },
  {
    "objectID": "posts/week_2.html#resources",
    "href": "posts/week_2.html#resources",
    "title": "Week 2",
    "section": "Resources",
    "text": "Resources\nYou must log on to your CI Google account to access the video.\n\n\n\nLecture\nSlides\nVideos\n\n\n\n\nMonday\n\n\n\n\nWednesday\nSlides\n2023 Math 352 Website\nM352 Continuous MGF\nM352 Categorical MGF"
  },
  {
    "objectID": "posts/week_1.html#resources",
    "href": "posts/week_1.html#resources",
    "title": "Week 1",
    "section": "Resources",
    "text": "Resources\n\n\n\nLecture\nSlides\nVideo\n\n\n\n\nMonday\nSlides\nVideo\n\n\nWednesday\nSlides\nVideo"
  },
  {
    "objectID": "hws/hw1.html",
    "href": "hws/hw1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Homework 1 is due 9/7/2024 at 11:59 PM. Submit your homework on Canvas as one PDF document.\n\nX\\sim Bin(n,p), show that the variance of Binomial distribution is np(1-p).\nX\\sim Pois(\\lambda) , show that the expected value of a Poisson distribution is \\lambda.\nX\\sim Pois(\\lambda) , show that the variance of a Poisson distribution is \\lambda.\nX\\sim N(\\mu,\\sigma^2), show that the expected value is \\mu."
  },
  {
    "objectID": "r.html",
    "href": "r.html",
    "title": "R Labs",
    "section": "",
    "text": "Below are the R Labs for the course. Make sure you complete and submit your QMD file by the deadline on Canvas.\n\n\n\n\n\n\n\n\n\nLab 2: Optimization\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\n\n\n\n\n\nLab 3: Generalized Linear Models\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2022\n\n\n\n\n\n\n\nIntroduction to R\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lectures/3a.html#function-of-random-variables-1",
    "href": "lectures/3a.html#function-of-random-variables-1",
    "title": "Functions of Random Variables",
    "section": "Function of Random Variables",
    "text": "Function of Random Variables"
  },
  {
    "objectID": "lectures/3a.html#using-the-distribution-function",
    "href": "lectures/3a.html#using-the-distribution-function",
    "title": "Functions of Random Variables",
    "section": "Using the Distribution Function",
    "text": "Using the Distribution Function\nLet there be a random variable \\(X\\) with a known distribution function \\(F_X(x)\\), the density function for the random variable \\(Y=g(X)\\) can be found with the following steps\n\n\n\nFind the region of \\(Y\\) in the space of \\(X\\), find \\(g^{-1}(y)\\)\nFind the region of \\(Y\\le y\\)\nFind \\(F_Y(y)=P(Y\\le y)\\) using the probability density function of \\(X\\) over region \\(Y\\le y\\)\nFind \\(f_Y(y)\\) by differentiating \\(F_Y(y)\\)"
  },
  {
    "objectID": "lectures/3a.html#example-1",
    "href": "lectures/3a.html#example-1",
    "title": "Functions of Random Variables",
    "section": "Example 1",
    "text": "Example 1\nLet \\(X\\) have the following probability density function:\n\\[\nf_X(x)=\\left\\{\\begin{array}{cc}\n2x & 0\\le x \\le 1 \\\\\n0 & \\mathrm{otherwise}\n\\end{array}\n\\right.\n\\]\nFind the probability density function of \\(Y=3X-1\\)?"
  },
  {
    "objectID": "lectures/3a.html#using-the-pdf",
    "href": "lectures/3a.html#using-the-pdf",
    "title": "Functions of Random Variables",
    "section": "Using the PDF",
    "text": "Using the PDF\nLet there be a random variable \\(X\\) with a known distribution function \\(F_X(x)\\), if the random variable \\(Y=g(X)\\) is either increasing or decreasing, than the probability density function can be found as\n\\[\nf_Y(y) = f_X\\{g^{-1}(y)\\}\\left|\\frac{dg^{-1}(y)}{dy}\\right|\n\\]"
  },
  {
    "objectID": "lectures/3a.html#example-2",
    "href": "lectures/3a.html#example-2",
    "title": "Functions of Random Variables",
    "section": "Example 2",
    "text": "Example 2\nLet \\(X\\) have the following probability density function:\n\\[\nf_X(x)=\\left\\{\\begin{array}{cc}\n\\frac{3}{2}x^2 + x & 0\\le y \\le 1 \\\\\n0 & \\mathrm{otherwise}\n\\end{array}\n\\right.\n\\]\nFind the probability density function of \\(Y=5-(X/2)\\)?"
  },
  {
    "objectID": "lectures/3a.html#using-the-mgf",
    "href": "lectures/3a.html#using-the-mgf",
    "title": "Functions of Random Variables",
    "section": "Using the MGF",
    "text": "Using the MGF\nUsing the uniqueness property of Moment Generating Functions, for a random variable \\(X\\) with a known distribution function \\(F_X(x)\\) and random variable \\(Y=g(X)\\), the distribution of \\(Y\\) can be found by:\n\n\nFind the moment generating function of \\(Y\\), \\(M_Y(t)\\).\nCompare \\(M_Y(t)\\), with known moment generating functions. If \\(M_Y(t)=M_V(t)\\), for all values \\(t\\), them \\(Y\\) and \\(V\\) have identical distributions."
  },
  {
    "objectID": "lectures/3a.html#example-3",
    "href": "lectures/3a.html#example-3",
    "title": "Functions of Random Variables",
    "section": "Example 3",
    "text": "Example 3\nLet \\(X\\) follow a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Find the distribution of \\(Z=\\frac{X-\\mu}{\\sigma}\\)."
  },
  {
    "objectID": "lectures/3a.html#example-4",
    "href": "lectures/3a.html#example-4",
    "title": "Functions of Random Variables",
    "section": "Example 4",
    "text": "Example 4\nLet \\(Z\\) follow a standard normal distribution with mean \\(0\\) and variance \\(1\\). Find the distribution of \\(Y=Z^2\\)"
  },
  {
    "objectID": "lectures/4a.html#independent-random-variables",
    "href": "lectures/4a.html#independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Independent Random Variables",
    "text": "Independent Random Variables\nRandom variables are considered independent of each other if the probability of one variable does not affect the probability of another variable."
  },
  {
    "objectID": "lectures/4a.html#discrete-independent-random-variables",
    "href": "lectures/4a.html#discrete-independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Discrete Independent Random Variables",
    "text": "Discrete Independent Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint density function of \\(p_{X_1,X_2}(x_1,x_2)\\). \\(X_1\\) is independent of \\(X_2\\) if and only if\n\\[\np_{X_1,X_2}(x_1,x_2) = p_{X_1}(x_1)p_{X_2}(x_2)\n\\]"
  },
  {
    "objectID": "lectures/4a.html#continuous-independent-random-variables",
    "href": "lectures/4a.html#continuous-independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Continuous Independent Random Variables",
    "text": "Continuous Independent Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). \\(X_1\\) is independent of \\(X_2\\) if and only if\n\\[\nf_{X_1,X_2}(x_1,x_2) = f_{X_1}(x_1)f_{X_2}(x_2)\n\\]"
  },
  {
    "objectID": "lectures/4a.html#matrix-algebra",
    "href": "lectures/4a.html#matrix-algebra",
    "title": "Joint Distribution Functions",
    "section": "Matrix Algebra",
    "text": "Matrix Algebra\n\\[\nA = \\left(\\begin{array}{cc}\na_1 & 0\\\\\n0 & a_2\n\\end{array}\\right)\n\\]\n\\[\n\\det(A) = a_1a_2\n\\]\n\\[\nA^{-1}=\\left(\\begin{array}{cc}\n1/a_1 & 0 \\\\\n0 & 1/a_2\n\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "lectures/4a.html#example",
    "href": "lectures/4a.html#example",
    "title": "Joint Distribution Functions",
    "section": "Example",
    "text": "Example\n\\[\n\\left(\\begin{array}{c}\nX\\\\\nY\n\\end{array}\\right)\\sim N \\left\\{\n\\left(\\begin{array}{c}\n\\mu_x\\\\\n\\mu_y\n\\end{array}\\right),\\left(\\begin{array}{cc}\n\\sigma_x^2 & 0\\\\\n0 & \\sigma_y^2\n\\end{array}\\right)\n\\right\\}\n\\]\nShow that \\(X\\perp Y\\).\n\\[\nf_{X,Y}(x,y)=\\det(2\\pi\\Sigma)^{-1/2}\\exp\\left\\{-\\frac{1}{2}(\\boldsymbol{w}-\\boldsymbol\\mu)^T\\Sigma^{-1}(\\boldsymbol w-\\boldsymbol\\mu)\\right\\}\n\\]\nwhere \\(\\Sigma=\\left(\\begin{array}{cc}\\sigma_y^2 & 0\\\\0 & \\sigma_y^2\\end{array}\\right)\\), \\(\\boldsymbol \\mu = \\left(\\begin{array}{cc}\\mu_x\\\\ \\mu_y \\end{array}\\right)\\), and \\(\\boldsymbol w = \\left(\\begin{array}{cc} x\\\\ y \\end{array}\\right)\\)"
  },
  {
    "objectID": "lectures/4a.html#expectations-1",
    "href": "lectures/4a.html#expectations-1",
    "title": "Joint Distribution Functions",
    "section": "Expectations",
    "text": "Expectations\nLet \\(X_1, X_2, \\ldots,X_n\\) be a set of random variables, the expectation of a function \\(g(X_1,\\ldots, X_n)\\) is defined as\n\\[\nE\\{g(X_1,\\ldots, X_n)\\} = \\sum_{x_1\\in X_1}\\cdots\\sum_{x_n\\in X_n}g(X_1,\\ldots, X_n)p(x_1,\\ldots,x_n)\n\\]\nor\n\\[\nE\\{g(\\boldsymbol X)\\} = \\int_{x_1\\in X_1}\\cdots\\int_{x_n\\in X_n}g(\\boldsymbol X)f(\\boldsymbol X)dx_n \\cdots dx_1\n\\]\n\n\\(\\boldsymbol X = (X_1,\\cdots, X_n)\\)"
  },
  {
    "objectID": "lectures/4a.html#expected-value-and-variance-of-linear-functions",
    "href": "lectures/4a.html#expected-value-and-variance-of-linear-functions",
    "title": "Joint Distribution Functions",
    "section": "Expected Value and Variance of Linear Functions",
    "text": "Expected Value and Variance of Linear Functions\nLet \\(X_1,\\ldots,X_n\\) and \\(Y_1,\\ldots,Y_m\\) be random variables with \\(E(X_i)=\\mu_i\\) and \\(E(Y_j)=\\tau_j\\). Furthermore, let \\(U = \\sum^n_{i=1}a_iX_i\\) and \\(V=\\sum^m_{j=1}b_jY_j\\) where \\(\\{a_i\\}^n_{i=1}\\) and \\(\\{b_j\\}_{j=1}^m\\) are constants. We have the following properties:\n\n\\(E(U)=\\sum_{i=1}^na_i\\mu_i\\)\n\\(Var(U)=\\sum^n_{i=1}a_i^2Var(X_i)+2\\underset{i&lt;j}{\\sum\\sum}a_ia_jCov(X_i,X_j)\\)\n\\(Cov(U,V)=\\sum^n_{i=1}\\sum^m_{j=1}Cov(X_i,Y_j)\\)"
  },
  {
    "objectID": "lectures/4a.html#expectation-of-product",
    "href": "lectures/4a.html#expectation-of-product",
    "title": "Joint Distribution Functions",
    "section": "Expectation of Product",
    "text": "Expectation of Product\nLet \\(X\\) and \\(Y\\) be independent random variables with Joint Function \\(f_{XY}(x,y)\\), then\n\\[\nE(XY) = E(X)E(Y)\n\\]\n\nProve it!"
  },
  {
    "objectID": "lectures/4a.html#conditional-expectations",
    "href": "lectures/4a.html#conditional-expectations",
    "title": "Joint Distribution Functions",
    "section": "Conditional Expectations",
    "text": "Conditional Expectations\nLet \\(X_1\\) and \\(X_2\\) be two random variables, the conditional expectation of \\(g(X_1)\\), given \\(X_2=x_2\\), is defined as\n\\[\nE\\{g(X_1)|X_2=x_2\\}=\\sum_{x_1}g(x_1)p(x_1|x_2)\n\\]\nor\n\\[\nE\\{g(X_1)|X_2=x_2\\}=\\int_{x_1}g(x_1)f(x_1|x_2)dx_1.\n\\]"
  },
  {
    "objectID": "lectures/4a.html#conditional-expectations-1",
    "href": "lectures/4a.html#conditional-expectations-1",
    "title": "Joint Distribution Functions",
    "section": "Conditional Expectations",
    "text": "Conditional Expectations\nFurthermore,\n\\[\nE(X_1)=E_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n\\]\nand\n\\[\nVar(X_1) = E_{X_2}\\{Var_{X_1|X_2}(X_1|X_2)\\} + Var_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n\\]"
  },
  {
    "objectID": "lectures/4a.html#covariance-1",
    "href": "lectures/4a.html#covariance-1",
    "title": "Joint Distribution Functions",
    "section": "Covariance",
    "text": "Covariance\nLet \\(X_1\\) and \\(X_2\\) be 2 random variables with mean \\(E(X_1)=\\mu_1\\) and \\(E(X_2)=\\mu_2\\), respectively. The covariance of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\begin{eqnarray*}\nCov(X_1,X_2) & = & E\\{(X_1-\\mu_1)(X_2-\\mu_2)\\}\\\\\n& =& E(X_1X_2)-\\mu_1\\mu_2\n\\end{eqnarray*}\n\\]\nIf \\(X_1\\) and \\(X_2\\) are independent random variables, then\n\\[\nCov(X_1,X_2)=0\n\\]"
  },
  {
    "objectID": "lectures/4a.html#correlation",
    "href": "lectures/4a.html#correlation",
    "title": "Joint Distribution Functions",
    "section": "Correlation",
    "text": "Correlation\nThe correlation of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\rho = Cor(X_1,X_2) = \\frac{Cov(X_1,X_2)}{\\sqrt{Var(X_1)Var(X_2)}}\n\\]"
  },
  {
    "objectID": "lectures/4a.html#mgf-property-independence",
    "href": "lectures/4a.html#mgf-property-independence",
    "title": "Joint Distribution Functions",
    "section": "MGF Property: Independence",
    "text": "MGF Property: Independence\nLet \\(X\\) and \\(Y\\) be independent random variables. Let \\(Z = X+Y\\), the MGF of Z is\n\\[\nM_Z(t) = M_X(t)M_Y(t)\n\\]"
  },
  {
    "objectID": "lectures/4a.html#examples-1",
    "href": "lectures/4a.html#examples-1",
    "title": "Joint Distribution Functions",
    "section": "Examples",
    "text": "Examples\nLet \\(X_1\\sim Bin(n_1,p)\\) and \\(X_2\\sim Bin(n_2, p)\\). Find the distribution function of \\(Y=X_1 + X_2\\). Assume \\(X_1\\perp X_2\\)."
  },
  {
    "objectID": "lectures/4a.html#examples-2",
    "href": "lectures/4a.html#examples-2",
    "title": "Joint Distribution Functions",
    "section": "Examples",
    "text": "Examples\nLet \\(X_1\\sim N(\\mu_1,\\sigma_1^2)\\) and \\(X_2\\sim N(\\mu_2,\\sigma_2^2)\\). Find the distribution function of \\(Y=X_1 + X_2\\). Assume \\(X_1\\perp X_2\\)."
  },
  {
    "objectID": "lectures/4b.html#sample",
    "href": "lectures/4b.html#sample",
    "title": "Sampling Distributions",
    "section": "Sample",
    "text": "Sample\nWhen collecting data to construct a sample, the sample is a collection of random variables.\n\nTherefore, the sample can be subjected to probability properties."
  },
  {
    "objectID": "lectures/4b.html#iid-random-variables",
    "href": "lectures/4b.html#iid-random-variables",
    "title": "Sampling Distributions",
    "section": "iid Random Variables",
    "text": "iid Random Variables\nA sample of random variables are said to be iid if they are identical and independentally distributed.\nFor example, \\(X\\) and \\(Y\\) are iid, if \\(X\\) and \\(Y\\) has the same distribution \\(f(\\theta)\\) and \\(X \\perp  Y\\)"
  },
  {
    "objectID": "lectures/4b.html#statistics-1",
    "href": "lectures/4b.html#statistics-1",
    "title": "Sampling Distributions",
    "section": "Statistics",
    "text": "Statistics\nA statistic is a transformation of the the sample data.\n\nBefore data is calculated, a statistic from a sample can take any value.\n\n\nTherefore, a statistic must be a random variable."
  },
  {
    "objectID": "lectures/4b.html#sampling-distributions-1",
    "href": "lectures/4b.html#sampling-distributions-1",
    "title": "Sampling Distributions",
    "section": "Sampling Distributions",
    "text": "Sampling Distributions\nA sampling distribution is the distribution of a statistic. Many known statistics have a known distribution."
  },
  {
    "objectID": "lectures/4b.html#bar-x",
    "href": "lectures/4b.html#bar-x",
    "title": "Sampling Distributions",
    "section": "\\(\\bar X\\)",
    "text": "\\(\\bar X\\)\nLet \\(X_1, X_2, \\ldots, X_n\\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\) , show that \\(\\bar X \\sim N(\\mu,\\sigma^2/n)\\). Note: the MGF of \\(X_i\\) is \\(e^{\\mu t + \\frac{t^2\\sigma^2}{2}}\\)."
  },
  {
    "objectID": "lectures/4b.html#sum-of-chi2_1",
    "href": "lectures/4b.html#sum-of-chi2_1",
    "title": "Sampling Distributions",
    "section": "Sum of \\(\\chi^2_1\\)",
    "text": "Sum of \\(\\chi^2_1\\)\nLet \\(Z_1^2, \\ldots, Z_n^2\\) be a iid \\(\\chi^2_1\\). Find \\(Y = \\sum^n_{i=1} Z_i^2\\)"
  },
  {
    "objectID": "lectures/4b.html#s2",
    "href": "lectures/4b.html#s2",
    "title": "Sampling Distributions",
    "section": "\\(s^2\\)",
    "text": "\\(s^2\\)"
  },
  {
    "objectID": "lectures/4b.html#t-distribution",
    "href": "lectures/4b.html#t-distribution",
    "title": "Sampling Distributions",
    "section": "t-distribution",
    "text": "t-distribution\nLet \\(Z\\sim N(0,1)\\), \\(W\\sim \\chi^2_\\nu\\), \\(Z\\perp W\\); therefore:\n\\[\nT=\\frac{Z}{\\sqrt{W/\\nu}} \\sim t_\\nu\n\\]"
  },
  {
    "objectID": "lectures/4b.html#f-distribution",
    "href": "lectures/4b.html#f-distribution",
    "title": "Sampling Distributions",
    "section": "F-distribution",
    "text": "F-distribution\nLet \\(W_1\\sim\\chi^2_{\\nu_1}\\) \\(W_2\\sim\\chi^2_{\\nu_2}\\), and \\(W_1\\perp W_2\\); therefore:\n\\[\nF = \\frac{W_1/\\nu_1}{W_2/\\nu_2}\\sim F_{\\nu_1,\\nu_2}\n\\]"
  },
  {
    "objectID": "lectures/4b.html#central-limit-theorem-1",
    "href": "lectures/4b.html#central-limit-theorem-1",
    "title": "Sampling Distributions",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nLet \\(X_1, X_2, \\ldots, X_n\\) be identical and independent distributed random variables with \\(E(X_i)=\\mu\\) and \\(Var(X_i) = \\sigma²\\). We define\n\\[\nY_n = \\sqrt n \\left(\\frac{\\bar X-\\mu}{\\sigma}\\right) \\mathrm{ where }\\ \\bar X = x\\frac{1}{n}\\sum^n_{i=1}X_i.\n\\]\nThen, the distribution of the function \\(Y_n\\) converges to a standard normal distribution function as \\(n\\rightarrow \\infty\\)."
  },
  {
    "objectID": "lectures/4b.html#central-limit-theorem-2",
    "href": "lectures/4b.html#central-limit-theorem-2",
    "title": "Sampling Distributions",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\\[\n\\bar X \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n\\]"
  },
  {
    "objectID": "lectures/4b.html#example",
    "href": "lectures/4b.html#example",
    "title": "Sampling Distributions",
    "section": "Example",
    "text": "Example\nLet \\(X_1, \\ldots, X_n \\overset{iid}{\\sim} \\chi^2_p\\), the MGF is \\(M(t)=(1-2t)^{-p/2}\\). Find the distribution of \\(\\bar X\\) as \\(n \\rightarrow \\infty\\)."
  },
  {
    "objectID": "lectures/1a.html#introductions",
    "href": "lectures/1a.html#introductions",
    "title": "Welcome!",
    "section": "Introductions",
    "text": "Introductions\n\nSan Bernardino, CA\nCSU Monterey Bay\n\nBS Biology\n\nSan Diego State University\n\nMaster’s in Public Health\n\nUC Riverside\n\nPhD in Applied Statistics"
  },
  {
    "objectID": "lectures/1a.html#introductions-1",
    "href": "lectures/1a.html#introductions-1",
    "title": "Welcome!",
    "section": "Introductions",
    "text": "Introductions\n\n\nName\nYear\nMajor\nFun Fact\nCareer Goal"
  },
  {
    "objectID": "lectures/1a.html#learning-objectives",
    "href": "lectures/1a.html#learning-objectives",
    "title": "Welcome!",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nDefine and identify population, sample, parameter, and statistics\nDefine and identify different variable types (quantitative and categorical)\nDefine concept of random samples and sampling methods"
  },
  {
    "objectID": "lectures/1a.html#website-and-syllabus",
    "href": "lectures/1a.html#website-and-syllabus",
    "title": "Welcome!",
    "section": "Website and Syllabus",
    "text": "Website and Syllabus"
  },
  {
    "objectID": "lectures/1a.html#fundamental-of-statistics-1",
    "href": "lectures/1a.html#fundamental-of-statistics-1",
    "title": "Welcome!",
    "section": "Fundamental of Statistics",
    "text": "Fundamental of Statistics\n\nObservational Unit\nVariable\n\nTypes of Variables\n\nQuantitative\nCategorical\n\nRoles of Variables\n\nPredictors\nOutcome"
  },
  {
    "objectID": "lectures/1a.html#observational-unit",
    "href": "lectures/1a.html#observational-unit",
    "title": "Welcome!",
    "section": "Observational Unit",
    "text": "Observational Unit"
  },
  {
    "objectID": "lectures/1a.html#type-of-variable---quantitative",
    "href": "lectures/1a.html#type-of-variable---quantitative",
    "title": "Welcome!",
    "section": "Type of Variable - Quantitative",
    "text": "Type of Variable - Quantitative"
  },
  {
    "objectID": "lectures/1a.html#type-of-variable---qualitative",
    "href": "lectures/1a.html#type-of-variable---qualitative",
    "title": "Welcome!",
    "section": "Type of Variable - Qualitative",
    "text": "Type of Variable - Qualitative"
  },
  {
    "objectID": "lectures/1a.html#predictor-variables",
    "href": "lectures/1a.html#predictor-variables",
    "title": "Welcome!",
    "section": "Predictor Variables",
    "text": "Predictor Variables"
  },
  {
    "objectID": "lectures/1a.html#outcomes",
    "href": "lectures/1a.html#outcomes",
    "title": "Welcome!",
    "section": "Outcomes",
    "text": "Outcomes"
  },
  {
    "objectID": "lectures/1a.html#activity",
    "href": "lectures/1a.html#activity",
    "title": "Welcome!",
    "section": "Activity",
    "text": "Activity\n\nHow many Harry Potter books have you read?\nWith which hand do you write?\nHow many hours have you slept in the past 24 hours?\nHave you have slept for at least 7 hours in the past 24 hours?\nWhich college is your major in?\nHow many states have you visited?"
  },
  {
    "objectID": "lectures/1a.html#activity-1",
    "href": "lectures/1a.html#activity-1",
    "title": "Welcome!",
    "section": "Activity",
    "text": "Activity\n\nWhat is the average number of Harry Potter books read by a student in this class?\nWhat percentage of students in this class are left-handed?"
  },
  {
    "objectID": "lectures/1a.html#fundamental-of-statistical-inference-1",
    "href": "lectures/1a.html#fundamental-of-statistical-inference-1",
    "title": "Welcome!",
    "section": "Fundamental of Statistical Inference",
    "text": "Fundamental of Statistical Inference\n\n\n\nCategorical Variables\n\nProportions\n\np or \\(\\pi\\)\n\\(\\hat p\\)\n\n\n\n\n\nContinuous Variables\n\nMeans or Averages\n\n\\(\\mu\\)\n\\(\\hat \\mu\\) or \\(\\bar X\\)\n\nVariances\n\n\\(\\sigma^2\\)\n\\(\\hat \\sigma^2\\) or \\(s^2\\)"
  },
  {
    "objectID": "lectures/1a.html#sampling-techniques-1",
    "href": "lectures/1a.html#sampling-techniques-1",
    "title": "Welcome!",
    "section": "Sampling Techniques",
    "text": "Sampling Techniques\n\nSimple Random Sampling\nStratified Sampling\nCluster Sampling\nMultistage Sampling"
  },
  {
    "objectID": "files/Lecture_1/index.html#introductions",
    "href": "files/Lecture_1/index.html#introductions",
    "title": "Introduction to Statistics",
    "section": "Introductions",
    "text": "Introductions\n\nSan Bernardino, CA\nCSU Monterey Bay\n\nBS Biology\n\nSan Diego State University\n\nMaster’s in Public Health\n\nUC Riverside\n\nPhD in Applied Statistics"
  },
  {
    "objectID": "files/Lecture_1/index.html#introductions-1",
    "href": "files/Lecture_1/index.html#introductions-1",
    "title": "Introduction to Statistics",
    "section": "Introductions",
    "text": "Introductions\n\n\nName\nYear\nMajor\nFun Fact\nCareer Goal"
  },
  {
    "objectID": "files/Lecture_1/index.html#learning-objectives",
    "href": "files/Lecture_1/index.html#learning-objectives",
    "title": "Introduction to Statistics",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nDefine and identify population, sample, parameter, and statistics\nDefine and identify different variable types (quantitative and categorical)\nDefine concept of random samples and sampling methods"
  },
  {
    "objectID": "files/Lecture_1/index.html#fundamental-of-statistics-1",
    "href": "files/Lecture_1/index.html#fundamental-of-statistics-1",
    "title": "Introduction to Statistics",
    "section": "Fundamental of Statistics",
    "text": "Fundamental of Statistics\n\nObservational Unit\nVariable\n\nTypes of Variables\n\nQuantitative\nCategorical\n\nRoles of Variables\n\nPredictors\nOutcome"
  },
  {
    "objectID": "files/Lecture_1/index.html#observational-unit",
    "href": "files/Lecture_1/index.html#observational-unit",
    "title": "Introduction to Statistics",
    "section": "Observational Unit",
    "text": "Observational Unit"
  },
  {
    "objectID": "files/Lecture_1/index.html#type-of-variable---quantitative",
    "href": "files/Lecture_1/index.html#type-of-variable---quantitative",
    "title": "Introduction to Statistics",
    "section": "Type of Variable - Quantitative",
    "text": "Type of Variable - Quantitative"
  },
  {
    "objectID": "files/Lecture_1/index.html#type-of-variable---qualitative",
    "href": "files/Lecture_1/index.html#type-of-variable---qualitative",
    "title": "Introduction to Statistics",
    "section": "Type of Variable - Qualitative",
    "text": "Type of Variable - Qualitative"
  },
  {
    "objectID": "files/Lecture_1/index.html#predictor-variables",
    "href": "files/Lecture_1/index.html#predictor-variables",
    "title": "Introduction to Statistics",
    "section": "Predictor Variables",
    "text": "Predictor Variables"
  },
  {
    "objectID": "files/Lecture_1/index.html#outcomes",
    "href": "files/Lecture_1/index.html#outcomes",
    "title": "Introduction to Statistics",
    "section": "Outcomes",
    "text": "Outcomes"
  },
  {
    "objectID": "files/Lecture_1/index.html#activity",
    "href": "files/Lecture_1/index.html#activity",
    "title": "Introduction to Statistics",
    "section": "Activity",
    "text": "Activity\n\nHow many Harry Potter books have you read?\nWith which hand do you write?\nHow many hours have you slept in the past 24 hours?\nHave you have slept for at least 7 hours in the past 24 hours?\nWhich college is your major in?\nHow many states have you visited?"
  },
  {
    "objectID": "files/Lecture_1/index.html#activity-1",
    "href": "files/Lecture_1/index.html#activity-1",
    "title": "Introduction to Statistics",
    "section": "Activity",
    "text": "Activity\n\nWhat is the average number of Harry Potter books read by a student in this class?\nWhat percentage of students in this class are left-handed?"
  },
  {
    "objectID": "files/Lecture_1/index.html#fundamental-of-statistical-inference-1",
    "href": "files/Lecture_1/index.html#fundamental-of-statistical-inference-1",
    "title": "Introduction to Statistics",
    "section": "Fundamental of Statistical Inference",
    "text": "Fundamental of Statistical Inference\n\n\n\nCategorical Variables\n\nProportions\n\np or \\(\\pi\\)\n\\(\\hat p\\)\n\n\n\n\n\nContinuous Variables\n\nMeans or Averages\n\n\\(\\mu\\)\n\\(\\hat \\mu\\) or \\(\\bar X\\)\n\nVariances\n\n\\(\\sigma^2\\)\n\\(\\hat \\sigma^2\\) or \\(s^2\\)"
  },
  {
    "objectID": "files/Lecture_1/index.html#sampling-techniques-1",
    "href": "files/Lecture_1/index.html#sampling-techniques-1",
    "title": "Introduction to Statistics",
    "section": "Sampling Techniques",
    "text": "Sampling Techniques\n\nSimple Random Sampling\nStratified Sampling\nCluster Sampling\nMultistage Sampling"
  },
  {
    "objectID": "files/Lecture_1/index.html#simple-random-sampling",
    "href": "files/Lecture_1/index.html#simple-random-sampling",
    "title": "Introduction to Statistics",
    "section": "Simple Random Sampling",
    "text": "Simple Random Sampling"
  },
  {
    "objectID": "files/Lecture_1/index.html#stratified-sampling",
    "href": "files/Lecture_1/index.html#stratified-sampling",
    "title": "Introduction to Statistics",
    "section": "Stratified Sampling",
    "text": "Stratified Sampling"
  },
  {
    "objectID": "files/Lecture_1/index.html#cluster-sampling",
    "href": "files/Lecture_1/index.html#cluster-sampling",
    "title": "Introduction to Statistics",
    "section": "Cluster Sampling",
    "text": "Cluster Sampling"
  },
  {
    "objectID": "files/Lecture_1/index.html#multistage-sampling",
    "href": "files/Lecture_1/index.html#multistage-sampling",
    "title": "Introduction to Statistics",
    "section": "Multistage Sampling",
    "text": "Multistage Sampling"
  },
  {
    "objectID": "files/Lecture_17_class/index.html#learning-outcomes",
    "href": "files/Lecture_17_class/index.html#learning-outcomes",
    "title": "Simple Linear Regression",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nLinear Regression\nOrdinary Least Squares\nR Code"
  },
  {
    "objectID": "files/Lecture_17_class/index.html#r-packages",
    "href": "files/Lecture_17_class/index.html#r-packages",
    "title": "Simple Linear Regression",
    "section": "R Packages",
    "text": "R Packages\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)"
  },
  {
    "objectID": "files/Lecture_17_class/index.html#linear-regression-1",
    "href": "files/Lecture_17_class/index.html#linear-regression-1",
    "title": "Simple Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nLinear regression is used to model the association between a set of predictor variables (x’s) and an outcome variable (y). Linear regression will fit a line that best describes the data points."
  },
  {
    "objectID": "files/Lecture_17_class/index.html#simple-linear-regression",
    "href": "files/Lecture_17_class/index.html#simple-linear-regression",
    "title": "Simple Linear Regression",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\nSimple linear regression will model the association between one predictor variable and an outcome:\n\\[\nY = \\beta_0 + \\beta_1 X + \\epsilon\n\\]\n\n\\(\\beta_0\\): Intercept term\n\\(\\beta_1\\): Slope term\n\\(\\epsilon\\sim N(0,\\sigma^2)\\)"
  },
  {
    "objectID": "files/Lecture_17_class/index.html#palmerpenguins",
    "href": "files/Lecture_17_class/index.html#palmerpenguins",
    "title": "Simple Linear Regression",
    "section": "palmerpenguins",
    "text": "palmerpenguins\nThe palmerpenguins data set contains 344 observations of 7 penguin characteristics. We will be looking at different association of the penguins"
  },
  {
    "objectID": "files/Lecture_17_class/index.html#scatter-plot",
    "href": "files/Lecture_17_class/index.html#scatter-plot",
    "title": "Simple Linear Regression",
    "section": "Scatter Plot",
    "text": "Scatter Plot\n\nggplot(penguins, aes(y = flipper_length_mm, x = body_mass_g)) +\n  geom_point() + theme_bw()"
  },
  {
    "objectID": "files/Lecture_17_class/index.html#scatter-plot-1",
    "href": "files/Lecture_17_class/index.html#scatter-plot-1",
    "title": "Simple Linear Regression",
    "section": "Scatter Plot",
    "text": "Scatter Plot\n\nggplot(sample_n(penguins,10), aes(y = flipper_length_mm, x = body_mass_g)) +\n  geom_point() + theme_bw()"
  },
  {
    "objectID": "files/Lecture_17_class/index.html#fitting-a-line",
    "href": "files/Lecture_17_class/index.html#fitting-a-line",
    "title": "Simple Linear Regression",
    "section": "Fitting a Line",
    "text": "Fitting a Line\n\nggplot(penguins, aes(y = flipper_length_mm, x = body_mass_g)) + geom_point() + geom_smooth(method = \"lm\") +\n  theme_bw() + annotate(\"text\", label = \"y=136.7+0.015x\", x=3250, y=230, size = 10)"
  },
  {
    "objectID": "files/Lecture_17_class/index.html#interpretation",
    "href": "files/Lecture_17_class/index.html#interpretation",
    "title": "Simple Linear Regression",
    "section": "Interpretation",
    "text": "Interpretation\n\\[\n\\hat y = 136.73 + 0.015 x\n\\]"
  },
  {
    "objectID": "files/Lecture_17_class/index.html#ordinary-least-squares-1",
    "href": "files/Lecture_17_class/index.html#ordinary-least-squares-1",
    "title": "Simple Linear Regression",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\nFor a data pair \\((X_i,Y_i)_{i=1}^n\\), the ordinary least squares estimator will find the estimates of \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) that minimize the following function:\n\\[\n\\sum^n_{i=1}\\{y_i-(\\beta_0+\\beta_1x_i)\\}^2\n\\]"
  },
  {
    "objectID": "files/Lecture_17_class/index.html#estimates",
    "href": "files/Lecture_17_class/index.html#estimates",
    "title": "Simple Linear Regression",
    "section": "Estimates",
    "text": "Estimates\n\\[\n\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\bar x\n\\] \\[\n\\hat\\beta_1 = \\frac{\\sum^n_{i=1}(y_i-\\bar y)(x_i-\\bar x)}{\\sum^n_{i=1}(x_i-\\bar x)^2}\n\\] \\[\n\\hat\\sigma^2 = \\frac{1}{n-2}\\sum^n_{i=1}(y_i-\\hat y_i)^2\n\\]"
  },
  {
    "objectID": "files/Lecture_17_class/index.html#standard-errors-of-betas-1",
    "href": "files/Lecture_17_class/index.html#standard-errors-of-betas-1",
    "title": "Simple Linear Regression",
    "section": "Standard Errors of \\(\\beta\\)’s",
    "text": "Standard Errors of \\(\\beta\\)’s\n\\[\nSE(\\hat\\beta_0)=\\sqrt{\\frac{\\sum^n_{i=1}x_i^2\\hat\\sigma^2}{n\\sum^n_{i=1}(x_i-\\bar x)^2}}\n\\]\n\\[\nSE(\\hat\\beta_1)=\\sqrt\\frac{\\hat\\sigma^2}{\\sum^n_{i=1}(x_i-\\bar x)^2}\n\\]"
  },
  {
    "objectID": "files/Lecture_17_class/index.html#standard-error-of-beta_0",
    "href": "files/Lecture_17_class/index.html#standard-error-of-beta_0",
    "title": "Simple Linear Regression",
    "section": "Standard Error of \\(\\beta_0\\)",
    "text": "Standard Error of \\(\\beta_0\\)\nFile"
  },
  {
    "objectID": "files/Lecture_17_class/index.html#unbiasedness-of-betas-1",
    "href": "files/Lecture_17_class/index.html#unbiasedness-of-betas-1",
    "title": "Simple Linear Regression",
    "section": "Unbiasedness of \\(\\beta\\)’s",
    "text": "Unbiasedness of \\(\\beta\\)’s\nBoth \\(\\beta_0\\) and \\(\\beta_1\\) are unbiased estimators."
  },
  {
    "objectID": "files/Lecture_17_class/index.html#ebeta_1",
    "href": "files/Lecture_17_class/index.html#ebeta_1",
    "title": "Simple Linear Regression",
    "section": "\\(E(\\beta_1)\\)",
    "text": "\\(E(\\beta_1)\\)"
  },
  {
    "objectID": "files/Lecture_17_class/index.html#r-code",
    "href": "files/Lecture_17_class/index.html#r-code",
    "title": "Simple Linear Regression",
    "section": "R Code",
    "text": "R Code\nUse the lm() and summary() functions to fit a model and obtain the relevant statistics."
  },
  {
    "objectID": "files/Lecture_17_class/index.html#fitting-a-line-1",
    "href": "files/Lecture_17_class/index.html#fitting-a-line-1",
    "title": "Simple Linear Regression",
    "section": "Fitting a line",
    "text": "Fitting a line\n\np_lm &lt;- lm(bill_depth_mm ~ bill_length_mm, data = penguins)\nsummary(p_lm)\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1381 -1.4263  0.0164  1.3841  4.5255 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    20.88547    0.84388  24.749  &lt; 2e-16 ***\nbill_length_mm -0.08502    0.01907  -4.459 1.12e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.922 on 340 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.05525,   Adjusted R-squared:  0.05247 \nF-statistic: 19.88 on 1 and 340 DF,  p-value: 1.12e-05"
  },
  {
    "objectID": "files/Lecture_4_class/index.html#learning-outcomes",
    "href": "files/Lecture_4_class/index.html#learning-outcomes",
    "title": "Moment Generating Functions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nDefine Moment Generating Functions\nDiscuss Properties"
  },
  {
    "objectID": "files/Lecture_4_class/index.html#moments",
    "href": "files/Lecture_4_class/index.html#moments",
    "title": "Moment Generating Functions",
    "section": "Moments",
    "text": "Moments\nThe \\(k\\)th moment is defined as the expectation of the random variable, raised to the \\(k\\)th power, defined as \\(E(X^k)\\)."
  },
  {
    "objectID": "files/Lecture_4_class/index.html#moment-generating-functions-1",
    "href": "files/Lecture_4_class/index.html#moment-generating-functions-1",
    "title": "Moment Generating Functions",
    "section": "Moment Generating Functions",
    "text": "Moment Generating Functions\nThe moment generating functions is used to obtain the \\(k\\)th moment. The mgf is defined as\n\\[\nm(t) = E(e^{tX})\n\\]\nThe \\(k\\)th moment can be obtained by taking the \\(k\\)th derivative of the mgf, with respect to \\(t\\), and setting \\(t\\) equal to 0:\n\\[\nE(X^k)=\\frac{d^km(t)}{dt}\\Bigg|_{t=0}\n\\]"
  },
  {
    "objectID": "files/Lecture_4_class/index.html#mgf",
    "href": "files/Lecture_4_class/index.html#mgf",
    "title": "Moment Generating Functions",
    "section": "MGF",
    "text": "MGF"
  },
  {
    "objectID": "files/Lecture_4_class/index.html#expected-value",
    "href": "files/Lecture_4_class/index.html#expected-value",
    "title": "Moment Generating Functions",
    "section": "Expected Value",
    "text": "Expected Value"
  },
  {
    "objectID": "files/Lecture_4_class/index.html#variance",
    "href": "files/Lecture_4_class/index.html#variance",
    "title": "Moment Generating Functions",
    "section": "Variance",
    "text": "Variance"
  },
  {
    "objectID": "files/Lecture_4_class/index.html#variance-1",
    "href": "files/Lecture_4_class/index.html#variance-1",
    "title": "Moment Generating Functions",
    "section": "Variance",
    "text": "Variance"
  },
  {
    "objectID": "files/Lecture_4_class/index.html#mgf-1",
    "href": "files/Lecture_4_class/index.html#mgf-1",
    "title": "Moment Generating Functions",
    "section": "MGF",
    "text": "MGF"
  },
  {
    "objectID": "files/Lecture_4_class/index.html#mgf-2",
    "href": "files/Lecture_4_class/index.html#mgf-2",
    "title": "Moment Generating Functions",
    "section": "MGF",
    "text": "MGF"
  },
  {
    "objectID": "files/Lecture_4_class/index.html#mgf-3",
    "href": "files/Lecture_4_class/index.html#mgf-3",
    "title": "Moment Generating Functions",
    "section": "MGF",
    "text": "MGF"
  },
  {
    "objectID": "files/Lecture_4_class/index.html#linearity",
    "href": "files/Lecture_4_class/index.html#linearity",
    "title": "Moment Generating Functions",
    "section": "Linearity",
    "text": "Linearity\nLet \\(X\\) follow a distribution \\(f\\), with the an MGF \\(M_X(t)\\), the MGF of \\(Y=aX+b\\) is given as\n\\[\nM_Y(t) = e^{tb}M_X(at)\n\\]"
  },
  {
    "objectID": "files/Lecture_4_class/index.html#derivation",
    "href": "files/Lecture_4_class/index.html#derivation",
    "title": "Moment Generating Functions",
    "section": "Derivation",
    "text": "Derivation"
  },
  {
    "objectID": "files/Lecture_4_class/index.html#linearity-1",
    "href": "files/Lecture_4_class/index.html#linearity-1",
    "title": "Moment Generating Functions",
    "section": "Linearity",
    "text": "Linearity\nLet \\(X\\) and \\(Y\\) be two random variables with MGFs \\(M_X(t)\\) and \\(M_Y(t)\\), respectively, and are independent. The MGF of \\(U=X-Y\\)\n\\[\nM_U(t) = M_X(t)M_Y(-t)\n\\]"
  },
  {
    "objectID": "files/Lecture_4_class/index.html#derivation-1",
    "href": "files/Lecture_4_class/index.html#derivation-1",
    "title": "Moment Generating Functions",
    "section": "Derivation",
    "text": "Derivation"
  },
  {
    "objectID": "files/Lecture_4_class/index.html#uniqueness",
    "href": "files/Lecture_4_class/index.html#uniqueness",
    "title": "Moment Generating Functions",
    "section": "Uniqueness",
    "text": "Uniqueness\nLet \\(X\\) and \\(Y\\) have the following distributions \\(F_X(x)\\) and \\(F_Y(y)\\) and MGFs \\(M_X(t)\\) and \\(M_Y(t)\\), respectively. \\(X\\) and \\(Y\\) have the same distribution \\(F_X(x)=F_Y(y)\\) if and only if \\(M_X(t)=M_Y(t)\\)."
  },
  {
    "objectID": "files/Lecture_4_class/index.html#uniqueness-1",
    "href": "files/Lecture_4_class/index.html#uniqueness-1",
    "title": "Moment Generating Functions",
    "section": "Uniqueness",
    "text": "Uniqueness\nLet \\(X_1,\\cdots, X_n\\) be independent random variables, where \\(X_i\\sim N(\\mu_i, \\sigma^2_i)\\), with \\(M_{X_i}(t)=\\exp\\{\\mu_i t+\\sigma^2_it^2/2\\}\\) for \\(i=1,\\cdots, n\\). Find the MGF of \\(Y=a_1X_1+\\cdots+a_nX_n\\), where \\(a_1, \\cdots, a_n\\) are constants."
  },
  {
    "objectID": "files/Lecture_15_class/index.html#learning-outcomes",
    "href": "files/Lecture_15_class/index.html#learning-outcomes",
    "title": "Type I and II Error, Power, and Neyman-Pearson Lemma",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nType I Error\nType II Error\nPower\nTypes of Hypothesis\nNeyman-Pearson Lemma"
  },
  {
    "objectID": "files/Lecture_15_class/index.html#types-of-error-1",
    "href": "files/Lecture_15_class/index.html#types-of-error-1",
    "title": "Type I and II Error, Power, and Neyman-Pearson Lemma",
    "section": "Types of Error",
    "text": "Types of Error"
  },
  {
    "objectID": "files/Lecture_15_class/index.html#type-i-error",
    "href": "files/Lecture_15_class/index.html#type-i-error",
    "title": "Type I and II Error, Power, and Neyman-Pearson Lemma",
    "section": "Type I Error",
    "text": "Type I Error\nA type I error occurs when \\(H_0\\) is rejected when \\(H_0\\) is true. The probability of a type I error is denoted as \\(\\alpha\\)."
  },
  {
    "objectID": "files/Lecture_15_class/index.html#type-ii-error",
    "href": "files/Lecture_15_class/index.html#type-ii-error",
    "title": "Type I and II Error, Power, and Neyman-Pearson Lemma",
    "section": "Type II Error",
    "text": "Type II Error\nA type I error occurs when \\(H_0\\) fails to be rejected when \\(H_0\\) is false. The probability of a type II error is denoted as \\(\\beta\\)."
  },
  {
    "objectID": "files/Lecture_15_class/index.html#relationship-between-alpha-and-beta",
    "href": "files/Lecture_15_class/index.html#relationship-between-alpha-and-beta",
    "title": "Type I and II Error, Power, and Neyman-Pearson Lemma",
    "section": "Relationship Between \\(\\alpha\\) and \\(\\beta\\)",
    "text": "Relationship Between \\(\\alpha\\) and \\(\\beta\\)"
  },
  {
    "objectID": "files/Lecture_15_class/index.html#type-ii-error-for-z-tests",
    "href": "files/Lecture_15_class/index.html#type-ii-error-for-z-tests",
    "title": "Type I and II Error, Power, and Neyman-Pearson Lemma",
    "section": "Type II Error for Z Tests",
    "text": "Type II Error for Z Tests"
  },
  {
    "objectID": "files/Lecture_15_class/index.html#type-ii-error-for-2-sample-z-tests",
    "href": "files/Lecture_15_class/index.html#type-ii-error-for-2-sample-z-tests",
    "title": "Type I and II Error, Power, and Neyman-Pearson Lemma",
    "section": "Type II Error for 2-sample Z Tests",
    "text": "Type II Error for 2-sample Z Tests"
  },
  {
    "objectID": "files/Lecture_15_class/index.html#power-1",
    "href": "files/Lecture_15_class/index.html#power-1",
    "title": "Type I and II Error, Power, and Neyman-Pearson Lemma",
    "section": "Power",
    "text": "Power\nThe power of a test is the probability of rejecting \\(H_0\\) when the true parameter is \\(\\theta\\).\n\\[\n\\mathrm{power}(\\theta) = \\mathrm P(\\mathrm{Reject}\\ H_0\\ \\mathrm{when}\\ \\theta\\ \\mathrm{is true})\n\\]"
  },
  {
    "objectID": "files/Lecture_15_class/index.html#power-and-type-ii-error",
    "href": "files/Lecture_15_class/index.html#power-and-type-ii-error",
    "title": "Type I and II Error, Power, and Neyman-Pearson Lemma",
    "section": "Power and Type II Error",
    "text": "Power and Type II Error\n\\[\n\\beta = 1 - \\mathrm{power}(\\theta_a)\n\\]"
  },
  {
    "objectID": "files/Lecture_15_class/index.html#simple-hypothesis",
    "href": "files/Lecture_15_class/index.html#simple-hypothesis",
    "title": "Type I and II Error, Power, and Neyman-Pearson Lemma",
    "section": "Simple Hypothesis",
    "text": "Simple Hypothesis"
  },
  {
    "objectID": "files/Lecture_15_class/index.html#composite-hypothesis",
    "href": "files/Lecture_15_class/index.html#composite-hypothesis",
    "title": "Type I and II Error, Power, and Neyman-Pearson Lemma",
    "section": "Composite Hypothesis",
    "text": "Composite Hypothesis"
  },
  {
    "objectID": "files/Lecture_15_class/index.html#neyman-pearson-lemma-1",
    "href": "files/Lecture_15_class/index.html#neyman-pearson-lemma-1",
    "title": "Type I and II Error, Power, and Neyman-Pearson Lemma",
    "section": "Neyman-Pearson Lemma",
    "text": "Neyman-Pearson Lemma\nSuppose you test the simple null hypothesis (\\(H_0: \\theta=\\theta_0\\)) vs a simple alternative hypothesis (\\(H_a: \\theta=\\theta_1\\)), based on a random sample with parameter \\(\\theta\\). Let \\(L(\\theta)\\) denote the likelihood function of the sample with parameter \\(\\theta\\). Then for a given \\(\\alpha\\), the test that maximizes the power at \\(\\theta_1\\) has a rejection region determined by \\[\n\\frac{L(\\theta_0)}{L(\\theta_1)}&lt;k\n\\]The value of \\(k\\) is chosen so that the test has the desired value for \\(\\alpha\\). such a test is most powerful \\(\\alpha\\)-level test for \\(H_0\\) vs \\(H_a\\)"
  },
  {
    "objectID": "files/Lecture_10_class/index.html#learning-outcomes",
    "href": "files/Lecture_10_class/index.html#learning-outcomes",
    "title": "Method of Moments Estimator",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nMethod of Moments"
  },
  {
    "objectID": "files/Lecture_10_class/index.html#estimators",
    "href": "files/Lecture_10_class/index.html#estimators",
    "title": "Method of Moments Estimator",
    "section": "Estimators",
    "text": "Estimators\nAn estimator is an operation computing the value of an estimate, that targets the parameter, using measurements from a sample."
  },
  {
    "objectID": "files/Lecture_10_class/index.html#data",
    "href": "files/Lecture_10_class/index.html#data",
    "title": "Method of Moments Estimator",
    "section": "Data",
    "text": "Data\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}F(\\boldsymbol \\theta)\\) where \\(F(\\cdot)\\) is a known distribution function and \\(\\boldsymbol\\theta\\) is a vector of parameters. Let \\(\\boldsymbol X = (X_1,\\ldots, X_n)^\\mathrm{T}\\), be the sample collected."
  },
  {
    "objectID": "files/Lecture_10_class/index.html#method-of-moments",
    "href": "files/Lecture_10_class/index.html#method-of-moments",
    "title": "Method of Moments Estimator",
    "section": "Method of Moments",
    "text": "Method of Moments\nLet the \\(k\\)th moment be defined as \\(\\mu_k\\) and the corresponding \\(k\\)th moment average \\(\\frac{1}{n}\\sum^n_{i=1}X_i^{k}\\):\n\\[\n\\mu_k = \\frac{1}{n}\\sum^n_{i=1}X_i^k.\n\\]\nThe parameter estimates are for \\(t\\) parameters are the solutions for \\(\\mu_k\\) for \\(k=1,\\ldots,t\\)."
  },
  {
    "objectID": "files/Lecture_10_class/index.html#bernoulli-distribution",
    "href": "files/Lecture_10_class/index.html#bernoulli-distribution",
    "title": "Method of Moments Estimator",
    "section": "Bernoulli Distribution",
    "text": "Bernoulli Distribution\nLet \\(X_1, \\ldots,X_n\\overset{iid}{\\sim}\\mathrm{Bin}(n,p)\\), find the method of moments estimator for \\(p\\)."
  },
  {
    "objectID": "files/Lecture_10_class/index.html#poisson-distribution",
    "href": "files/Lecture_10_class/index.html#poisson-distribution",
    "title": "Method of Moments Estimator",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nLet \\(X_1, \\ldots,X_n\\overset{iid}{\\sim}\\mathrm{Pois}(\\lambda)\\), find the method of moments estimator for \\(\\lambda\\)."
  },
  {
    "objectID": "files/Lecture_10_class/index.html#uniform-distribution",
    "href": "files/Lecture_10_class/index.html#uniform-distribution",
    "title": "Method of Moments Estimator",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nLet \\(X_1, \\ldots,X_n\\overset{iid}{\\sim}U(1,\\theta)\\), find the method of moments estimator for \\(\\theta\\)."
  },
  {
    "objectID": "files/Lecture_10_class/index.html#gamma-distribution",
    "href": "files/Lecture_10_class/index.html#gamma-distribution",
    "title": "Method of Moments Estimator",
    "section": "Gamma Distribution",
    "text": "Gamma Distribution\nLet \\(X_1, \\ldots,X_n\\overset{iid}{\\sim}\\mathrm{Gamma}(\\alpha,\\beta)\\), find the method of moments estimator for \\(\\alpha\\) and \\(\\beta\\)."
  },
  {
    "objectID": "files/Lecture_10_class/index.html#nomal-distribution",
    "href": "files/Lecture_10_class/index.html#nomal-distribution",
    "title": "Method of Moments Estimator",
    "section": "Nomal Distribution",
    "text": "Nomal Distribution\nLet \\(X_1, \\ldots,X_n\\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\), find the method of moments estimator for \\(\\mu\\) and \\(\\sigma^2\\)."
  },
  {
    "objectID": "files/Lecture_4/index.html#learning-outcomes",
    "href": "files/Lecture_4/index.html#learning-outcomes",
    "title": "Moment Generating Functions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nDefine Moment Generating Functions\nDiscuss Properties"
  },
  {
    "objectID": "files/Lecture_4/index.html#moments",
    "href": "files/Lecture_4/index.html#moments",
    "title": "Moment Generating Functions",
    "section": "Moments",
    "text": "Moments\nThe \\(k\\)th moment is defined as the expectation of the random variable, raised to the \\(k\\)th power, defined as \\(E(X^k)\\)."
  },
  {
    "objectID": "files/Lecture_4/index.html#moment-generating-functions-1",
    "href": "files/Lecture_4/index.html#moment-generating-functions-1",
    "title": "Moment Generating Functions",
    "section": "Moment Generating Functions",
    "text": "Moment Generating Functions\nThe moment generating functions is used to obtain the \\(k\\)th moment. The mgf is defined as\n\\[\nm(t) = E(e^{tX})\n\\]\nThe \\(k\\)th moment can be obtained by taking the \\(k\\)th derivative of the mgf, with respect to \\(t\\), and setting \\(t\\) equal to 0:\n\\[\nE(X^k)=\\frac{d^km(t)}{dt}\\Bigg|_{t=0}\n\\]"
  },
  {
    "objectID": "files/Lecture_4/index.html#mgf",
    "href": "files/Lecture_4/index.html#mgf",
    "title": "Moment Generating Functions",
    "section": "MGF",
    "text": "MGF"
  },
  {
    "objectID": "files/Lecture_4/index.html#expected-value",
    "href": "files/Lecture_4/index.html#expected-value",
    "title": "Moment Generating Functions",
    "section": "Expected Value",
    "text": "Expected Value"
  },
  {
    "objectID": "files/Lecture_4/index.html#variance",
    "href": "files/Lecture_4/index.html#variance",
    "title": "Moment Generating Functions",
    "section": "Variance",
    "text": "Variance"
  },
  {
    "objectID": "files/Lecture_4/index.html#variance-1",
    "href": "files/Lecture_4/index.html#variance-1",
    "title": "Moment Generating Functions",
    "section": "Variance",
    "text": "Variance"
  },
  {
    "objectID": "files/Lecture_4/index.html#mgf-1",
    "href": "files/Lecture_4/index.html#mgf-1",
    "title": "Moment Generating Functions",
    "section": "MGF",
    "text": "MGF"
  },
  {
    "objectID": "files/Lecture_4/index.html#mgf-2",
    "href": "files/Lecture_4/index.html#mgf-2",
    "title": "Moment Generating Functions",
    "section": "MGF",
    "text": "MGF"
  },
  {
    "objectID": "files/Lecture_4/index.html#mgf-3",
    "href": "files/Lecture_4/index.html#mgf-3",
    "title": "Moment Generating Functions",
    "section": "MGF",
    "text": "MGF"
  },
  {
    "objectID": "files/Lecture_4/index.html#linearity",
    "href": "files/Lecture_4/index.html#linearity",
    "title": "Moment Generating Functions",
    "section": "Linearity",
    "text": "Linearity\nLet \\(X\\) follow a distribution \\(f\\), with the an MGF \\(M_X(t)\\), the MGF of \\(Y=aX+b\\) is given as\n\\[\nM_Y(t) = e^{tb}M_X(at)\n\\]"
  },
  {
    "objectID": "files/Lecture_4/index.html#derivation",
    "href": "files/Lecture_4/index.html#derivation",
    "title": "Moment Generating Functions",
    "section": "Derivation",
    "text": "Derivation"
  },
  {
    "objectID": "files/Lecture_4/index.html#linearity-1",
    "href": "files/Lecture_4/index.html#linearity-1",
    "title": "Moment Generating Functions",
    "section": "Linearity",
    "text": "Linearity\nLet \\(X\\) and \\(Y\\) be two random variables with MGFs \\(M_X(t)\\) and \\(M_Y(t)\\), respectively, and are independent. The MGF of \\(U=X-Y\\)\n\\[\nM_U(t) = M_X(t)M_Y(-t)\n\\]"
  },
  {
    "objectID": "files/Lecture_4/index.html#derivation-1",
    "href": "files/Lecture_4/index.html#derivation-1",
    "title": "Moment Generating Functions",
    "section": "Derivation",
    "text": "Derivation"
  },
  {
    "objectID": "files/Lecture_4/index.html#uniqueness",
    "href": "files/Lecture_4/index.html#uniqueness",
    "title": "Moment Generating Functions",
    "section": "Uniqueness",
    "text": "Uniqueness\nLet \\(X\\) and \\(Y\\) have the following distributions \\(F_X(x)\\) and \\(F_Y(y)\\) and MGFs \\(M_X(t)\\) and \\(M_Y(t)\\), respectively. \\(X\\) and \\(Y\\) have the same distribution \\(F_X(x)=F_Y(y)\\) if and only if \\(M_X(t)=M_Y(t)\\)."
  },
  {
    "objectID": "files/Lecture_4/index.html#uniqueness-1",
    "href": "files/Lecture_4/index.html#uniqueness-1",
    "title": "Moment Generating Functions",
    "section": "Uniqueness",
    "text": "Uniqueness\nLet \\(X_1,\\cdots, X_n\\) be independent random variables, where \\(X_i\\sim N(\\mu_i, \\sigma^2_i)\\), with \\(M_{X_i}(t)=\\exp\\{\\mu_i t+\\sigma^2_it^2/2\\}\\) for \\(i=1,\\cdots, n\\). Find the MGF of \\(Y=a_1X_1+\\cdots+a_nX_n\\), where \\(a_1, \\cdots, a_n\\) are constants."
  },
  {
    "objectID": "files/Lecture_7_class/index.html#learning-outcomes",
    "href": "files/Lecture_7_class/index.html#learning-outcomes",
    "title": "Central Limit Theorem and other Sampling Distributions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nCentral Limit Theorem\nNormal Approximation of Binomial Distribution\nOther Sampling Distributions"
  },
  {
    "objectID": "files/Lecture_7_class/index.html#central-limit-theorem",
    "href": "files/Lecture_7_class/index.html#central-limit-theorem",
    "title": "Central Limit Theorem and other Sampling Distributions",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nLet \\(X_1, X_2, \\ldots, X_n\\) be identical and independent distributed random variables with \\(E(X_i)=\\mu\\) and \\(Var(X_i) = \\sigma²\\). We define\n\\[\nY_n = \\sqrt n \\left(\\frac{\\bar X-\\mu}{\\sigma}\\right) \\mathrm{ where }\\ \\bar X = \\frac{1}{n}\\sum^n_{i=1}X_i.\n\\]\nThen, the distribution of the function \\(Y_n\\) converges to a standard normal distribution function as \\(n\\rightarrow \\infty\\)."
  },
  {
    "objectID": "files/Lecture_7_class/index.html#normal-approximation-of-binomial-distribution",
    "href": "files/Lecture_7_class/index.html#normal-approximation-of-binomial-distribution",
    "title": "Central Limit Theorem and other Sampling Distributions",
    "section": "Normal Approximation of Binomial Distribution",
    "text": "Normal Approximation of Binomial Distribution\nSuppose \\(X\\sim Bin(n,p)\\), furthermore, let \\(\\bar X = X/n\\). If \\(n\\) is large enough, then\n\\[\n\\bar X \\overset{\\circ}{\\sim}N\\left\\{p,p(1-p)/n\\right\\}\\]\\[\n\\]"
  },
  {
    "objectID": "files/Lecture_7_class/index.html#other-sampling-distributions",
    "href": "files/Lecture_7_class/index.html#other-sampling-distributions",
    "title": "Central Limit Theorem and other Sampling Distributions",
    "section": "Other Sampling Distributions",
    "text": "Other Sampling Distributions\n\n\\(\\chi^2\\)-distribution\nt-distribution\nF-distribution"
  },
  {
    "objectID": "files/Lecture_7_class/index.html#chi2-distribution",
    "href": "files/Lecture_7_class/index.html#chi2-distribution",
    "title": "Central Limit Theorem and other Sampling Distributions",
    "section": "\\(\\chi^2\\)-distribution",
    "text": "\\(\\chi^2\\)-distribution\nLet \\(Z_1, Z_2,\\ldots,Z_n \\overset{iid}{\\sim}N(0,1)\\),\n\\[\n\\sum_{i=1}^nZ_i^2\\sim\\chi^2_n.\n\\]"
  },
  {
    "objectID": "files/Lecture_7_class/index.html#independence-of-bar-x-and-s2",
    "href": "files/Lecture_7_class/index.html#independence-of-bar-x-and-s2",
    "title": "Central Limit Theorem and other Sampling Distributions",
    "section": "Independence of \\(\\bar X\\) and \\(S^2\\)",
    "text": "Independence of \\(\\bar X\\) and \\(S^2\\)\n\\(X_1, X_2,\\ldots,X_n\\) form a random sample from a normal distribution, then \\(\\bar X\\) and \\(S^2\\) are independent of each other."
  },
  {
    "objectID": "files/Lecture_7_class/index.html#distribution-of-s2",
    "href": "files/Lecture_7_class/index.html#distribution-of-s2",
    "title": "Central Limit Theorem and other Sampling Distributions",
    "section": "Distribution of \\(S^2\\)",
    "text": "Distribution of \\(S^2\\)\nLet \\(X_1, X_2,\\ldots,X_n \\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\), \\(S^2 = \\frac{1}{n-1}\\sum^n_{i=1}(X_i-\\bar X)^2\\), and \\(\\bar X \\perp S^2\\); therefore:\n\\[\n\\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1}.\n\\]"
  },
  {
    "objectID": "files/Lecture_7_class/index.html#t-distribution",
    "href": "files/Lecture_7_class/index.html#t-distribution",
    "title": "Central Limit Theorem and other Sampling Distributions",
    "section": "t-distribution",
    "text": "t-distribution\nLet \\(Z\\sim N(0,1)\\), \\(W\\sim \\chi^2_\\nu\\), \\(Z\\perp W\\); therefore:\n\\[\nT=\\frac{Z}{\\sqrt{W/\\nu}} \\sim t_\\nu\n\\]"
  },
  {
    "objectID": "files/Lecture_7_class/index.html#f-distribution",
    "href": "files/Lecture_7_class/index.html#f-distribution",
    "title": "Central Limit Theorem and other Sampling Distributions",
    "section": "F-distribution",
    "text": "F-distribution\nLet \\(W_1\\sim\\chi^2_{\\nu_1}\\) \\(W_2\\sim\\chi^2_{\\nu_2}\\), and \\(W_1\\perp W_2\\); therefore:\n\\[\nF = \\frac{W_1/\\nu_1}{W_2/\\nu_2}\\sim F_{\\nu_1,\\nu_2}\n\\]"
  },
  {
    "objectID": "files/Lecture_13_class/index.html#learning-outcomes",
    "href": "files/Lecture_13_class/index.html#learning-outcomes",
    "title": "Sufficiency",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nSufficiency"
  },
  {
    "objectID": "files/Lecture_13_class/index.html#sufficiency-1",
    "href": "files/Lecture_13_class/index.html#sufficiency-1",
    "title": "Sufficiency",
    "section": "Sufficiency",
    "text": "Sufficiency\nSufficiency evaluates whether a statistic (or estimator) contains enough information of a parameter \\(\\theta\\). In essence a statistic is considered sufficient to infer \\(\\theta\\) if it provides enough information about \\(\\theta\\)."
  },
  {
    "objectID": "files/Lecture_13_class/index.html#sufficiency-2",
    "href": "files/Lecture_13_class/index.html#sufficiency-2",
    "title": "Sufficiency",
    "section": "Sufficiency",
    "text": "Sufficiency\nLet \\(X_1,\\ldots,X_n\\) be a random sample from a distribution with parameter \\(\\theta\\). A statistic \\(T=t(X_1,\\ldots,X_n)\\) is said to be sufficient for making inferences of a parameter \\(\\theta\\) if condition joint distribution of \\(X_1,\\ldots,X_n\\) given \\(T=t\\) does not depend on \\(\\theta\\)."
  },
  {
    "objectID": "files/Lecture_13_class/index.html#joint-sufficient-statistics",
    "href": "files/Lecture_13_class/index.html#joint-sufficient-statistics",
    "title": "Sufficiency",
    "section": "Joint Sufficient Statistics",
    "text": "Joint Sufficient Statistics\nLet \\(X_1,\\ldots,X_n\\) be a random sample from a distribution with parameters \\(\\boldsymbol\\theta=(\\theta_1,\\ldots,\\theta_m)^\\mathrm{T}\\). A joint statistic \\(\\boldsymbol T=\\left\\{t_1(X_1,\\ldots,X_n),\\ldots ,t_k(X_1,\\ldots,X_n)\\right\\}^\\mathrm{T}\\) is said to be sufficient for making inferences on paramters \\(\\boldsymbol \\theta\\) if condition joint distribution of \\(X_1,\\ldots,X_n\\) given \\(\\boldsymbol T=\\boldsymbol t\\) does not depend on \\(\\boldsymbol \\theta\\)."
  },
  {
    "objectID": "files/Lecture_13_class/index.html#factorization-theorem",
    "href": "files/Lecture_13_class/index.html#factorization-theorem",
    "title": "Sufficiency",
    "section": "Factorization Theorem",
    "text": "Factorization Theorem\nLet \\(f(x_1, \\ldots, x_n;\\theta)\\) be the joint PMF of PDF of \\(X_1, \\ldots,X_n\\). Then \\(T=t(X_1,\\ldots,X_n)\\) is a sufficient statistic for \\(\\theta\\) if and only if there exist 2 nonnegative functions, \\(g\\) and \\(h\\), such that\n\\[\nf(x_1,\\ldots,x_n) = g\\{t(x_1,\\ldots,x_n);\\theta\\}h(x_1,\\ldots,x_n).\n\\]"
  },
  {
    "objectID": "files/Lecture_13_class/index.html#minimum-sufficient-statistics",
    "href": "files/Lecture_13_class/index.html#minimum-sufficient-statistics",
    "title": "Sufficiency",
    "section": "Minimum Sufficient Statistics",
    "text": "Minimum Sufficient Statistics\nA minimum sufficient statistic is a sufficient statistic that has the smallest dimensionality, which represents the greatest possible reduction of the data without any information loss."
  },
  {
    "objectID": "files/Lecture_13_class/index.html#example-1",
    "href": "files/Lecture_13_class/index.html#example-1",
    "title": "Sufficiency",
    "section": "Example 1",
    "text": "Example 1\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}Pois(\\lambda)\\), show that \\(\\sum^n_{i=1} X_i\\) is a sufficient statistic for \\(\\lambda\\)."
  },
  {
    "objectID": "files/Lecture_13_class/index.html#example-2",
    "href": "files/Lecture_13_class/index.html#example-2",
    "title": "Sufficiency",
    "section": "Example 2",
    "text": "Example 2\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\), show that \\((\\sum^n_{i=1} X_i,\\sum^n_{i=1} X^2_i)^\\mathrm{T}\\) is a sufficient statistic for \\(\\mu\\) and \\(\\sigma²\\)."
  },
  {
    "objectID": "files/Lecture_13_class/index.html#example-3",
    "href": "files/Lecture_13_class/index.html#example-3",
    "title": "Sufficiency",
    "section": "Example 3",
    "text": "Example 3\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}logN(\\mu,\\sigma^2)\\), find the joint sufficient statistics for \\(\\mu\\) and \\(\\sigma^2\\),\n\\[\nf(x)=\\frac{1}{x\\sqrt{2\\pi\\sigma^2}}\\exp\\left[\n-\\frac{\\{\\log(x)-\\mu)^2}{2\\sigma^2}\\right]\\]"
  },
  {
    "objectID": "files/Lecture_12_class/index.html#learning-outcomes",
    "href": "files/Lecture_12_class/index.html#learning-outcomes",
    "title": "Relative Efficiency and Consistency",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nCramér-Rao Inequality\nRelative Efficiency\nConsistency"
  },
  {
    "objectID": "files/Lecture_12_class/index.html#cramér-rao-inequality",
    "href": "files/Lecture_12_class/index.html#cramér-rao-inequality",
    "title": "Relative Efficiency and Consistency",
    "section": "Cramér-Rao Inequality",
    "text": "Cramér-Rao Inequality\nLet \\(f(x_1, \\ldots, x_n;\\theta)\\) be the joint PMF of PDF of \\(X_1, \\ldots,X_n\\) and \\(T=t(X_1,\\ldots,X_n)\\) be an unbiased estimator of \\(\\theta\\). Then\n\\[\nVar(T) \\ge \\frac{1}{nI(\\theta)}\n\\] If \\(Var(T)=\\frac{1}{nI(\\theta)}\\), then \\(T\\) is considered an efficient estimator of \\(\\theta\\)."
  },
  {
    "objectID": "files/Lecture_12_class/index.html#example",
    "href": "files/Lecture_12_class/index.html#example",
    "title": "Relative Efficiency and Consistency",
    "section": "Example",
    "text": "Example\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}Pois(\\lambda)\\), show that \\(\\bar X\\) is an efficient estimator of \\(\\lambda\\)."
  },
  {
    "objectID": "files/Lecture_12_class/index.html#example-1",
    "href": "files/Lecture_12_class/index.html#example-1",
    "title": "Relative Efficiency and Consistency",
    "section": "Example",
    "text": "Example\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\), show that \\(\\bar X\\) is an efficient estimator of \\(\\mu\\)."
  },
  {
    "objectID": "files/Lecture_12_class/index.html#relative-efficiency",
    "href": "files/Lecture_12_class/index.html#relative-efficiency",
    "title": "Relative Efficiency and Consistency",
    "section": "Relative Efficiency",
    "text": "Relative Efficiency\nGiven 2 unbiased estimators \\(\\hat\\theta_1\\) and \\(\\hat\\theta_2\\) of a parameter \\(\\theta\\) , with variances \\(V(\\hat\\theta_1)\\) and \\(V(\\hat\\theta_2)\\), respectively, then the efficiency of \\(\\hat\\theta_1\\) relative to \\(\\hat\\theta_2\\) is defined as\n\\[\nreleff(\\hat\\theta_1,\\hat\\theta_2)=\\frac{\\hat\\theta_1}{\\hat\\theta_2}\n\\]"
  },
  {
    "objectID": "files/Lecture_12_class/index.html#example-2",
    "href": "files/Lecture_12_class/index.html#example-2",
    "title": "Relative Efficiency and Consistency",
    "section": "Example",
    "text": "Example\nLet \\(X_1,\\ldots,X_n\\) be a random sample from a population with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\n\n\n\\(\\hat\\mu_1=(X_1+X_2)/2\\)\n\\(\\hat\\mu_2=X_1/4+\\frac{\\sum^{n-1}_{i=2}X_i}{2(n-2)}+X_n/4\\)\n\\(\\hat\\mu_3=\\bar X\\)\n\n\nFind the relative efficiency of \\(\\hat\\mu_3\\) with respect to \\(\\hat\\mu_1\\) and \\(\\hat\\mu_2\\)."
  },
  {
    "objectID": "files/Lecture_12_class/index.html#consistency",
    "href": "files/Lecture_12_class/index.html#consistency",
    "title": "Relative Efficiency and Consistency",
    "section": "Consistency",
    "text": "Consistency\nLet \\(X_1,\\ldots,X_n\\) be a random sample from a distribution with parameter \\(\\theta\\). The estimator \\(\\hat \\theta\\) is a consistent estimator of the \\(\\theta\\) if\n\n\\(E\\{(\\hat\\theta-\\theta)^2\\}\\rightarrow0\\) as \\(n\\rightarrow \\infty\\)\n\\(P(|\\hat\\theta-\\theta|\\ge \\epsilon)\\rightarrow0\\) as \\(n\\rightarrow \\infty\\) for every \\(\\epsilon&gt;0\\)"
  },
  {
    "objectID": "files/Lecture_12_class/index.html#weak-law-of-large-numbers",
    "href": "files/Lecture_12_class/index.html#weak-law-of-large-numbers",
    "title": "Relative Efficiency and Consistency",
    "section": "Weak Law of Large Numbers",
    "text": "Weak Law of Large Numbers\nLet \\(X_1,\\ldots,X_n\\) be iid random variables with \\(E(X_i)=\\mu\\) and \\(Var(X_i)=\\sigma^2&lt;\\infty\\). Let \\(\\bar X_n=\\frac{1}{n}\\sum^n_{i=1}X_i\\), for every, \\(\\epsilon&gt;0\\),\n\\[\n\\lim_{n\\rightarrow\\infty} P(|\\bar X-\\mu|&lt;\\epsilon) = 1\n\\]\nthat is, \\(\\bar X_n\\) converges in probability to \\(\\mu\\)."
  },
  {
    "objectID": "files/Lecture_12_class/index.html#strong-law-of-large-numbers",
    "href": "files/Lecture_12_class/index.html#strong-law-of-large-numbers",
    "title": "Relative Efficiency and Consistency",
    "section": "Strong Law of Large Numbers",
    "text": "Strong Law of Large Numbers\nLet \\(X_1,\\ldots,X_n\\) be iid random variables with \\(E(X_i)=\\mu\\) and \\(Var(X_i)=\\sigma^2&lt;\\infty\\). Let \\(\\bar X_n=\\frac{1}{n}\\sum^n_{i=1}X_i\\), for every, \\(\\epsilon&gt;0\\),\n\\[\nP(\\lim_{n\\rightarrow\\infty} |\\bar X-\\mu|&lt;\\epsilon) = 1\n\\]\nthat is, \\(\\bar X_n\\) converges almost surely to \\(\\mu\\)."
  }
]